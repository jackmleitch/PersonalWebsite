<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.21">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Jack Leitch RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Jack Leitch Atom Feed">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">The ins and outs of Gradient Descent | Jack Leitch</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://jackmleitch.com/blog/Gradient-Descent"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="The ins and outs of Gradient Descent | Jack Leitch"><meta data-rh="true" name="description" content="Optimizing your choice of optimizer"><meta data-rh="true" property="og:description" content="Optimizing your choice of optimizer"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2020-10-28T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/jackmleitch"><meta data-rh="true" property="article:tag" content="Optimization,Mathematics,Python"><link data-rh="true" rel="icon" href="/img/favicon.png"><link data-rh="true" rel="canonical" href="https://jackmleitch.com/blog/Gradient-Descent"><link data-rh="true" rel="alternate" href="https://jackmleitch.com/blog/Gradient-Descent" hreflang="en"><link data-rh="true" rel="alternate" href="https://jackmleitch.com/blog/Gradient-Descent" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.bb947118.css">
<link rel="preload" href="/assets/js/runtime~main.97da6740.js" as="script">
<link rel="preload" href="/assets/js/main.237c04f2.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="Machine Learning Engineer" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/favicon.ico" alt="Machine Learning Engineer" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Jack Leitch</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/docs/experience">Experience</a><a class="navbar__item navbar__link" href="/docs/projects">Projects</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/jackmleitch" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Strava-Data-Pipeline">Building a ELT Strava Data Pipline</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Essay-Companion">Building NLP Powered Applications with Hugging Face Transformers</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Churn-Prediction">Building Interpretable Models on Imbalanced Data</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Strava-Kudos">Predicting Strava Kudos</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Recipe-Recommendation">Building a Recipe Recommendation System</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Strava-AP">Automating Mundane Web-Based Tasks With Selenium and Heroku</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Project-Auto">Automating the Setup of my Data Science Projects</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Recipe-Recommendation2">Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/OrganizingML">Organizing Machine Learning Projects</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/Gradient-Descent">The ins and outs of Gradient Descent</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Recipe-Recommendation1">Using BeautifulSoup to Help Make Beautiful Soups</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_Ikge" itemprop="headline">The ins and outs of Gradient Descent</h1><div class="blogPostData_SAv4 margin-vert--md"><time datetime="2020-10-28T00:00:00.000Z" itemprop="datePublished">October 28, 2020</time> · <!-- -->12 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_sTYa"><div class="avatar margin-bottom--sm"><a href="https://github.com/jackmleitch" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/jackmleitch.png" alt="Jack Leitch"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/jackmleitch" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jack Leitch</span></a></div><small class="avatar__subtitle" itemprop="description">Machine Learning Engineer</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p><strong>Optimizing your choice of optimizer</strong></p><p><strong>Gradient descent</strong> is an optimization algorithm used to minimize some cost function by iteratively moving in the direction of <strong>steepest descent</strong>. That is, moving in the direction which has the most negative gradient. In machine learning, we use gradient descent to continually tweak the parameters in our model in order to minimize a cost function. We start with some set of values for our model parameters (weights and biases in a neural network) and improve them slowly. In this blog post, we will start by exploring some basic optimizers commonly used in classical machine learning and then move on to some of the more popular algorithms used in Neural Networks and Deep Learning.</p><p>Imagine you&#x27;re out for a run (or walk, whatever floats your boat) in the mountains and suddenly a thick mist comes in impairing your vision. A good strategy to get down the mountain is to feel the ground in every direction and take a step in the direction in which the ground is descending the fastest. Repeating this you should end up at the bottom of the mountain (although you might also end up at something that looks like the bottom that isn&#x27;t — more on this later). This is exactly what gradient descent does: it measures the local gradient of the cost function <strong>J(ω)</strong> (parametrized by model parameters ω), and moves in the direction of descending gradient. Once we reach a gradient of zero, we have reached our minimum.</p><p>Suppose now that the steepness of the hill isn&#x27;t immediately obvious to you (maybe you&#x27;re too cold to feel your feet, use your imagination!), so you have to use your phone&#x27;s accelerometer to measure the gradient. It&#x27;s also really cold out so to check your phone you have to stop running and take your gloves off. Therefore, you need to minimize the use of your phone if you hope to make it down anytime soon! So you need to choose the right frequency at which you should measure the steepness of the hill so as not to go off track and at the same time reach home before the sunset. This amount of time between checks is what is known as the <strong>learning rate</strong> i.e. the size of steps we take downhill. If our learning rate is too small, then the algorithm will take a long time to converge. But, if our learning rate is too high the algorithm can diverge and just past the minimum.</p><p><img loading="lazy" alt="img1" src="/assets/images/image1-bd8112b90c6c03a9430fff11530c9ccd.png" width="1400" height="1321" class="img_ev3q"></p><p>An important point to note is that not all cost functions are convex (the MSE cost function for Linear Regression is convex, however). A function is convex if the line segment between any two points on the graph of the function lies above the graph between the two points. Convexity is nice as it implies that there is a single global minimum. Problems start to occur when the cost functions aren’t convex: there may be ridges, plateaus, holes, etc. This makes convergence to the minimum difficult. Fortunately, there are techniques available to us to help combat some of these issues.</p><p><img loading="lazy" alt="img2" src="/assets/images/image2-b2afbbf17cd27f3ca4bcd376f4eaa6f0.png" width="1400" height="1364" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="batch-gradient-descent">Batch Gradient Descent<a class="hash-link" href="#batch-gradient-descent" title="Direct link to heading">​</a></h2><p>The vanilla of the gradient descent world. In batch gradient descent we use the full training set X at each gradient step. Because of this, when using large training sets it can be agonizingly slow.
We start by finding the gradient vector of the cost function, we denote this by</p><p><img loading="lazy" alt="img3" src="data:image/png;base64,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" width="176" height="62" class="img_ev3q"></p><p>Here, <strong>J(ω)</strong> is the cost function parametrized by the model parameters <strong>ω</strong>) and <strong>∇</strong> denotes the vector differential operator. Now we have the gradient vector, we just need it to go in the opposite direction (downhill). Then, multiplying by the learning rate <strong>η</strong> we get our one-step gradient descent update equation:</p><p><img loading="lazy" alt="img3" src="data:image/png;base64,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" width="358" height="56" class="img_ev3q"></p><p>Implementation of batch gradient descent in python couldn&#x27;t be simpler!</p><div class="language-py codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_Ktv7">/src/batch_gradient_descent.py</div><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-py codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> numpy </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> np</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">eta </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.01</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic"># our chosen learning rate</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">iterations </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">10</span><span class="token operator" style="color:#393A34">^</span><span class="token number" style="color:#36acaa">3</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">n </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">100</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">theta </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">random</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">randn</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic"># random initialization, with d being</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># the dimension of theta</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> i </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">iterations</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    gradients </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">n </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> X</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">T</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dot</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">X</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dot</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">theta</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    theta </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> theta </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> eta </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> gradients</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>We can use a grid search to find a good learning rate if we so desire. To find the correct number of iterations it normally suffices to set the number of iterations to a very large number but then interrupt the algorithm when the gradient vector (gradients) becomes smaller than some pre-set tolerance (normally denoted <strong>ϵ</strong>). This works because when the gradient vector becomes tiny, we are extremely close to our minimum.</p><p>One other important note is that when using Gradient Descent you should ensure that all of your features have a similar scale as otherwise, it will take much longer for the algorithm to converge — Sckikit-Learn’s StandardScaler usually does the trick. What can we do to speed up our optimizer though? Enter Stochastic Gradient Descent.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="stochastic-gradient-descent">Stochastic Gradient Descent<a class="hash-link" href="#stochastic-gradient-descent" title="Direct link to heading">​</a></h2><p>Stochastic GD takes the opposite extreme to batch GD, rather than using the whole training set at each step, it takes a random instance of the training set at each step and uses this to work out the gradient. This is obviously going to be much faster than batch GD, especially when using huge training sets. The downside, however, is that due to its random behavior the algorithm is less regular: it only decreases on average as opposed to decreasing gradually at each step. Our final parameter values are therefore going to be good but not optimal (unlike batch gradient descent) as once we end up close to the minimum, the algorithm with continue to bounce around.</p><p>As unideal as this sounds, it can actually help! If for example, the cost function is very irregular (and not convex) the stochastic-ness can help in jumping out of local minima. So actually, stochastic GD actually has a better chance at finding the global minimum than plain old batch GD.</p><p>We can actually also do something to help deal with the lack of convergence when we get near the minimum: gradually reduce the learning rate. We can start with the steps being (relatively) large, this helps converge quicker to the minimum and also helps jump out of local minima. Gradually we can then reduce the learning rate, which allows the algorithm to settle down at the global minimum. This process is called a <strong>learning schedule</strong>. Learning rate scheduling is a pretty sweet technique, maybe I’ll give it its own blog post in the future.</p><p>The following code implements Stochastic Gradient Descent along with a (simple) learning schedule:</p><div class="language-py codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_Ktv7">/src/stochastic_gradient_descent.py</div><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-py codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">epochs </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">50</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">t0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> t1 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">10</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic"># Learning schedule hyperparams</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">n </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">100</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">learning_schedule</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">t</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> t0</span><span class="token operator" style="color:#393A34">/</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">t </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> t1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">theta </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">random</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">randn</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic"># random initialization, with d being</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># the dimension of theta</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> epoch </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">epochs</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> i </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">n</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        rand_idx </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">random</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">randint</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">n</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        xi </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> X</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">rand_idx</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">rand_idx</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">d</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        yi </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">rand_idx</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">rand_idx</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">d</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        gradients </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> xi</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">T</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dot</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">xi</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">dot</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">theta</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> yi</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        eta </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> learning_schedule</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">epoch </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> n </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> i</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        theta </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> theta </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> eta </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> gradients</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Here we iterate over rounds of n iterations, and each round is called an epoch.</p><p>It is also worth mentioning another method, <strong>Mini-batch Gradient Descent</strong>. To put it simply, it is just a combination of both Batch and Stochastic GD methods: at each step, Mini-batch GD computes the gradients on a small random subset of the training instances (a mini-batch). This has the advantage of being less erratic than Stochastic GD; however, it is more likely to get stuck in local minimums.</p><p>So far we have discussed the more classical GD algorithms which perform to a satisfactory level when training more classic ML models, for example, SVMs, Random Forests, etc. But, using these methods to train large deep natural networks can be mind-numbingly slow. Faster optimizers can provide us with an extra speed boost in training. A few of the most popular algorithms will be discussed below.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="momentum-optimisation">Momentum Optimisation<a class="hash-link" href="#momentum-optimisation" title="Direct link to heading">​</a></h2><p>Instead of using only the gradient of the loss function at the current step to guide the search, momentum optimization uses the concept of momentum and accumulates the previous gradients to help determine the direction to go. Thinking back to our analogy earlier: imagine you are now on a bike and you were riding down the mountain, you will start slow but you will gradually start to pick up more and more momentum until you reach a terminal velocity. This is exactly what the momentum algorithm does. Another way to think of it is that the gradient vector is now used for acceleration as opposed to speed. An update equation for our model parameters is hence given by:</p><p><img loading="lazy" alt="img3" src="data:image/png;base64,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" width="380" height="100" class="img_ev3q"></p><p>At each iteration, we subtract the local gradient (multiplied by <strong>η</strong>) from the momentum vector m and then update the weights <strong>ω</strong> by adding the momentum vector. As well as converging faster in general, the momentum helps the algorithm escape from plateaus faster and also roll past local minima. The hyperparameter <strong>β</strong> is the amount of friction in the system, the higher the β the slower the terminal velocity.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="nesterov-accelerated-gradient">Nesterov Accelerated Gradient<a class="hash-link" href="#nesterov-accelerated-gradient" title="Direct link to heading">​</a></h2><p>With one slight adjustment to the momentum algorithm, we can almost always get faster convergence. The NAG method measures the gradient of the cost function slightly ahead of the local position <strong>ω</strong>. The one-step update is given by</p><p><img loading="lazy" alt="img3" src="data:image/png;base64,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" width="518" height="100" class="img_ev3q"></p><p>The reason this works is simple: in general, the momentum vector is pointing towards the minimum, so measuring the gradient a little bit further in that direction will help. These small speed improvements then add up and ultimately the NAG algorithm ends up being significantly faster. Implementing this in Keras is incredibly easy, we can just put:</p><div class="language-py codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-py codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">optimizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> keras</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">optimizers</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">SGD</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">lr</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.001</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> momentum</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.9</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> nesterov</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="rmsprop">RMSProp<a class="hash-link" href="#rmsprop" title="Direct link to heading">​</a></h2><p>The RMSProp algorithm is an altered version of the <strong>AdaGrad</strong> algorithm, introduced by Geoffrey Hinton and Tijmen Tieleman in 2012 (so pretty recently in mathematics terms!). Imagine a scenario where the cost function looks like an elongated bowl, the GD algorithm will start going down the steepest slope but this slope doesn&#x27;t point directly towards the global minimum. The AdaGrad algorithm provides us with a correction so that the algorithm does indeed actually move towards the global minimum. It does this by scaling down the gradient vector along the steepest dimensions. Another way to think of this is that we decaying the learning rate faster in steeper dimensions and slower in dimensions with a more gentle slope. The easiest way to understand how this works is to get hands-on with the one-step update equations:</p><p><img loading="lazy" alt="img3" src="/assets/images/math5-827b0a72cba756630452998f5c3bbaff.png" width="452" height="98" class="img_ev3q"></p><p>Here the <strong>𝇇</strong> represents element-wise multiplication. Therefore each element of s is given by</p><p><img loading="lazy" alt="img3" src="data:image/png;base64,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" width="326" height="84" class="img_ev3q"></p><p>So if the cost function <strong>J(ω)</strong> is steep in the i-th dimension, then s is going to accumulate and get larger and larger as we iterate through. The second step here is practically the same as the usual gradient descent update except we scale down the gradient vector in relation to <strong>s</strong>. The <strong>⊘</strong> represent element-wise division, thus the bigger each element of the gradient vector, the smaller the step we take in that direction. The ε term is a very small smoothing term (e.g. <strong>ε = 1e-10</strong>) and it is just there to avoid division by zero (I once divided by zero and had a nose bleed, its the devils work).</p><p>The problem with AdaGrad however, is that it often stops too early when training neural networks. This is because the learning rate gets scaled down too much. RMSProp makes one simple adjustment. Instead, we exponentially decay the gradients accumulated in the most recent iterations.</p><p><img loading="lazy" alt="img3" src="/assets/images/math7-c9f76e5e39b65f6b8c16ce2b6bb9568d.png" width="566" height="90" class="img_ev3q"></p><p>Although we have introduced a new hyperparameter to tune (<strong>ρ</strong>), the value <strong>ρ = 0.9</strong> normally does the trick. As you can probably guess by now, implementing RMSProp in Keras is very easy:</p><div class="language-py codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-py codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">optimizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> keras</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">optimizers</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">RMSprop</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">lr</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.001</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> rho</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.9</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>I guess now it&#x27;s time to talk about the big daddy of optimizers.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="adam-and-nadam-optimization">Adam and Nadam Optimization<a class="hash-link" href="#adam-and-nadam-optimization" title="Direct link to heading">​</a></h2><p>Adaptive moment estimation (Adam) combines two of the methods we have already looked at: momentum and RMSProp optimization. It steals the idea of keeping track of an exponentially decaying average of previous gradients from momentum optimization and the idea of keeping track of exponentially decaying averages of past squared gradients from RMSProp.</p><p><img loading="lazy" alt="img3" src="/assets/images/math8-2c1a138c227a2f70fb095144f8ee6c88.png" width="466" height="184" class="img_ev3q"></p><p>Here, <strong>t</strong> is the iteration number (starting at 1). The first, second, and fifth equations are almost identical to the momentum and RMSProp optimization algorithms, with the difference coming from hyperparameters. The third and fourth equations are simply there to boost <strong>m</strong> and <strong>s</strong> at the start of the algorithm as initially, they are biased towards 0. As with all things in Keras, Adam is extremely easy to implement (the hyperparameters default values are given below, also).</p><div class="language-py codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-py codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">optimizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> keras</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">optimizers</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Adam</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">lr</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.001</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> beta_1</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.9</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> beta_2</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.999</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The <strong>Nadam</strong> algorithm is the same as the Adam algorithm but it also includes the Nesterov trick, meaning it often converges slightly faster than standard Adam.</p><p>Another, not immediately obvious, advantage of Adam (and with Nadam and RMSProp) is that it requires less tuning of <strong>η</strong> — so, in some ways, it&#x27;s even easier to implement than the standard vanilla GD method!</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h2><p>We went from classic vanilla gradient descent and gradually got more and more complex with our methodology. When working with the less computing power-intensive models I have found usually the simple SGD/mini-batch models work really well. When training large neural networks the likes of Adam/Nadam and RMSProp tend to work great. As with everything in the ML world, having a little play around experimenting with different methods often leads to the best results.</p><p>One closing remark: it has been <a href="https://arxiv.org/abs/1705.08292" target="_blank" rel="noopener noreferrer">found</a> that sometimes adaptive optimization methodologies (Adam, Nadam, RMSProp) can sometimes lead to solutions that generalize poorly so if you find this problem, maybe give the NAG method a try!</p><p>If you found this article interesting, I would highly recommend picking up a copy of Hands-On Machine Learning with Scikit-Learn and TensorFlow by Aurélien Géron. It&#x27;s an absolutely incredible book and it’s where I first read about a lot of the content in this post!</p></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_u0Nl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/optimization">Optimization</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mathematics">Mathematics</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/python">Python</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/OrganizingML"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Organizing Machine Learning Projects</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/Recipe-Recommendation1"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Using BeautifulSoup to Help Make Beautiful Soups</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#batch-gradient-descent" class="table-of-contents__link toc-highlight">Batch Gradient Descent</a></li><li><a href="#stochastic-gradient-descent" class="table-of-contents__link toc-highlight">Stochastic Gradient Descent</a></li><li><a href="#momentum-optimisation" class="table-of-contents__link toc-highlight">Momentum Optimisation</a></li><li><a href="#nesterov-accelerated-gradient" class="table-of-contents__link toc-highlight">Nesterov Accelerated Gradient</a></li><li><a href="#rmsprop" class="table-of-contents__link toc-highlight">RMSProp</a></li><li><a href="#adam-and-nadam-optimization" class="table-of-contents__link toc-highlight">Adam and Nadam Optimization</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://linkedin.com/in/jackmleitch" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn</a></li><li class="footer__item"><a href="https://twitter.com/jackmleitch" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li><li class="footer__item"><a href="https://medium.com/@jackmleitch" target="_blank" rel="noopener noreferrer" class="footer__link-item">Medium</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/jackmleitch" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Jack Leitch. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.97da6740.js"></script>
<script src="/assets/js/main.237c04f2.js"></script>
</body>
</html>