"use strict";(self.webpackChunkjackmleitch_com_np=self.webpackChunkjackmleitch_com_np||[]).push([[9211],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return u}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),m=c(a),u=r,h=m["".concat(l,".").concat(u)]||m[u]||p[u]||i;return a?n.createElement(h,o(o({ref:t},d),{},{components:a})):n.createElement(h,o({ref:t},d))}));function u(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var c=2;c<i;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},3015:function(e,t,a){a.r(t),a.d(t,{assets:function(){return d},contentTitle:function(){return l},default:function(){return u},frontMatter:function(){return s},metadata:function(){return c},toc:function(){return p}});var n=a(7462),r=a(3366),i=(a(7294),a(3905)),o=["components"],s={slug:"Churn-Prediction",title:"Building Interpretable Models on Imbalanced Data",tags:["Python","Classification","Imbalanced Classification","Scikit-Learn","Optuna","MLFlow"],authors:"jack"},l=void 0,c={permalink:"/PersonalWebsite/blog/Churn-Prediction",source:"@site/blog/2022-01-04-ChurnPrediction.md",title:"Building Interpretable Models on Imbalanced Data",description:"Predicting customer churn from a telecom provider",date:"2022-01-04T00:00:00.000Z",formattedDate:"January 4, 2022",tags:[{label:"Python",permalink:"/PersonalWebsite/blog/tags/python"},{label:"Classification",permalink:"/PersonalWebsite/blog/tags/classification"},{label:"Imbalanced Classification",permalink:"/PersonalWebsite/blog/tags/imbalanced-classification"},{label:"Scikit-Learn",permalink:"/PersonalWebsite/blog/tags/scikit-learn"},{label:"Optuna",permalink:"/PersonalWebsite/blog/tags/optuna"},{label:"MLFlow",permalink:"/PersonalWebsite/blog/tags/ml-flow"}],readingTime:24.86,truncated:!0,authors:[{name:"Jack Leitch",title:"Machine Learning Engineer",url:"https://github.com/jackmleitch",imageURL:"https://github.com/jackmleitch.png",key:"jack"}],frontMatter:{slug:"Churn-Prediction",title:"Building Interpretable Models on Imbalanced Data",tags:["Python","Classification","Imbalanced Classification","Scikit-Learn","Optuna","MLFlow"],authors:"jack"},prevItem:{title:"Building NLP Powered Applications with Hugging Face Transformers",permalink:"/PersonalWebsite/blog/Essay-Companion"},nextItem:{title:"Predicting Strava Kudos",permalink:"/PersonalWebsite/blog/Strava-Kudos"}},d={authorsImageUrls:[void 0]},p=[{value:"Performance vs Interpretability",id:"performance-vs-interpretability",level:2},{value:"Model Evaluation",id:"model-evaluation",level:2},{value:"Model Specifications",id:"model-specifications",level:2},{value:"Data",id:"data",level:2},{value:"Feature Engineering",id:"feature-engineering",level:2},{value:"Modeling",id:"modeling",level:2},{value:"Baseline Model",id:"baseline-model",level:3},{value:"SMOTE: To Over-Sample the Minority Class",id:"smote-to-over-sample-the-minority-class",level:3},{value:"Feature Selection",id:"feature-selection",level:3},{value:"But can we do better than our logistic regression?",id:"but-can-we-do-better-than-our-logistic-regression",level:3},{value:"Hyperparameter Tuning",id:"hyperparameter-tuning",level:3},{value:"Pruning the Decision Tree",id:"pruning-the-decision-tree",level:3},{value:"How Does It Perform on Test Data",id:"how-does-it-perform-on-test-data",level:3},{value:"Picking the Optimal Probability Threshold",id:"picking-the-optimal-probability-threshold",level:3},{value:"MLFLOW Experiment Session",id:"mlflow-experiment-session",level:2},{value:"Model Interpreting",id:"model-interpreting",level:2},{value:"Conclusion",id:"conclusion",level:2}],m={toc:p};function u(e){var t=e.components,s=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,n.Z)({},m,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Predicting customer churn from a telecom provider")),(0,i.kt)("p",null,"I\u2019ve always believed that to truly learn data science you need to practice data science and I wanted to do this project to practice working with imbalanced classes in classification problems. This was also a perfect opportunity to start working with ",(0,i.kt)("a",{parentName:"p",href:"https://mlflow.org/"},"mlflow")," to help track my machine learning experiments: it allows me to track the different models I have used, the parameters I\u2019ve trained with, and the metrics I\u2019ve recorded."),(0,i.kt)("p",null,"This project was aimed at predicting customer churn using the telecommunications data found on ",(0,i.kt)("a",{parentName:"p",href:"https://www.kaggle.com/c/customer-churn-prediction-2020/overview"},"Kaggle")," (which is a publicly available synthetic dataset). That is, we want to be able to predict if a given customer is going the leave the telecom provider based on the information we have on that customer. Now, why is this useful? Well, if we can predict which customers we think are going to leave ",(0,i.kt)("strong",{parentName:"p"},"before")," they leave then we can try to do something about it! For example, we could target them with specific offers, and maybe we could even use the model to provide us insight into what to offer them because we will know, or at least have an idea, as to why they are leaving."),(0,i.kt)("h2",{id:"performance-vs-interpretability"},"Performance vs Interpretability"),(0,i.kt)("p",null,"It\u2019s very important to know and understand the problem/task at hand before we start to even think about writing any code. Would it be useful in this case to build a really powerful model like XGBOOST? No, of course not. Our goal isn\u2019t to squeeze every drop of performance out of our model. Our goal is to ",(0,i.kt)("strong",{parentName:"p"},"understand")," why people are leaving so we can do something about it and try to get them to stay. In an ideal world, we would build a very interpretable model, but in reality, we may have to find a happy medium between performance and interpretability. As always, logistic regression would be a good start."),(0,i.kt)("h2",{id:"model-evaluation"},"Model Evaluation"),(0,i.kt)("p",null,"We now need to decide how we are going to evaluate our models and what we are going to be happy with. I think it\u2019s important to decide an end goal beforehand as otherwise, it\u2019s going to be hard to decide when to stop, squeezing out those extra 1%s is not worth it a lot of the time."),(0,i.kt)("p",null,"Due to the nature of our data, our classes are likely going to be highly imbalanced, with the case we are interested in (customers leaving) being the minority class. This makes selecting the right metric super important."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"alt text",src:a(8285).Z,width:"1070",height:"1380"})),(0,i.kt)("p",null,"The metrics we are going to be interested in are ",(0,i.kt)("strong",{parentName:"p"},"precision"),", ",(0,i.kt)("strong",{parentName:"p"},"recall"),", and other metrics associated with these. Precision is the ratio of correct positive predictions to the overall number of positive predictions. Recall is the ratio of correct positive predictions to the overall number of positive predictions in the dataset. In our case we are looking at trying to retain customers by predicting which customers are going to leave: so we aren\u2019t too fussed if we miss-classify some customers as \u2018churn\u2019 when they are not (false positives). If anything, these miss-classifications might be customers that would soon become \u2018churn\u2019 if nothing changes as they may lie on the edge of the decision boundary. So, we are looking to ",(0,i.kt)("strong",{parentName:"p"},"maximize recall")," as it will minimize the number of false negatives."),(0,i.kt)("p",null,"We are also going to look at the ",(0,i.kt)("strong",{parentName:"p"},"F-measure")," as it provides a way to express both concerns of precision and recall with a single score \u2014 we don\u2019t just want to forfeit precision to get 100% recall!"),(0,i.kt)("h2",{id:"model-specifications"},"Model Specifications"),(0,i.kt)("p",null,"Once we have built our final model, we can then use a precision-recall curve to optimize our performance on the positive (minority class). In this case, we are going to assume that stakeholders in our imaginary telecoms business want to ",(0,i.kt)("strong",{parentName:"p"},"achieve a recall of 0.80")," (i.e. we identify 80% of the positive samples correctly) while maximizing precision."),(0,i.kt)("h2",{id:"data"},"Data"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"train.csv")," \u2014 the training set. Contains 4250 rows with 20 columns. 3652 samples (85.93%) belong to class churn=no and 598 samples (14.07%) belong to class churn=yes.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"test.csv")," \u2014 the test set. Contains 850 rows with 18 columns."))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/read_data.py"',title:'"/src/read_data.py"'},'import pandas as pd\nfrom src.data.load_data import read_params\n# load in training data\nconfig = read_params("params.yaml")\nexternal_data_path = config["external_data_config"]["external_data_csv"]\ndf = pd.read_csv(external_data_path, sep=",", encoding="utf-8")\n# check we have our 20 cols\nassert len(df.columns) == 20\n')),(0,i.kt)("p",null,"EDA (along with all the modeling etc.) was done in different python scripts and I have chosen to not include it here as it is irrelevant to the topic I am writing about. It is nonetheless very important and you can find this whole project on my ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/jackmleitch"},"Github page"),"."),(0,i.kt)("p",null,"Due to the high cardinality of the state columns, we need to be careful when encoding them otherwise we will end up with 50 different features!"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"df.head()"),"\n",(0,i.kt)("img",{alt:"alt text",src:a(3630).Z,width:"5070",height:"355"})),(0,i.kt)("p",null,"We can look at a correlation matrix to see any initial promising features."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/plot_correlation_matrix.py"',title:'"/src/plot_correlation_matrix.py"'},'import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style="white")\n# Compute the correlation matrix\ncorr = df.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={"shrink": .5})\n')),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"alt text",src:a(5067).Z,width:"745",height:"671"})),(0,i.kt)("p",null,"Ok, this doesn\u2019t look great but we can see that churn is somewhat correlated with total_day_minutes, total_day_charge, and number_customer_service_calls. Let\u2019s build a simple ",(0,i.kt)("strong",{parentName:"p"},"baseline model")," with total_day_minutes and number_customer_service_calls (we omit total_day_charge because it\u2019s strongly correlated with total_day_minutes)."),(0,i.kt)("h2",{id:"feature-engineering"},"Feature Engineering"),(0,i.kt)("p",null,"We can generate a few features to encapsulate daily totals. We also map the state feature to the regions the state belongs to as it massively reduces the feature dimensionality. Finally, we can map churn target value to binary as this is required for a lot of models."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/preprocess_data.py"',title:'"/src/preprocess_data.py"'},"def preprocess(df):\n    # add new features\n    df['total_minutes'] = df['total_day_minutes'] + df['total_eve_minutes'] + df['total_night_minutes']\n    df['total_calls'] = df['total_day_calls'] + df['total_eve_calls'] + df['total_night_calls']\n    df['total_charge'] = df['total_day_charge'] + df['total_eve_charge'] + df['total_night_charge']\n    # target mapping\n    target_mapping = {\"no\": 0, \"yes\": 1}\n    df.loc[:, 'churn'] = df['churn'].map(target_mapping)\n    # map state\n    state_mapping = {\n        'AK': 'O', 'AL': 'S', 'AR': 'S', 'AS': 'O', 'AZ': 'W', 'CA': 'W', 'CO': 'W', 'CT': 'N', 'DC': 'N', 'DE': 'N', 'FL': 'S', 'GA': 'S',\n        'GU': 'O', 'HI': 'O', 'IA': 'M', 'ID': 'W', 'IL': 'M', 'IN': 'M', 'KS': 'M', 'KY': 'S', 'LA': 'S', 'MA': 'N', 'MD': 'N', 'ME': 'N',\n        'MI': 'W', 'MN': 'M', 'MO': 'M', 'MP': 'O', 'MS': 'S', 'MT': 'W', 'NA': 'O',  'NC': 'S', 'ND': 'M', 'NE': 'W', 'NH': 'N', 'NJ': 'N',\n        'NM': 'W', 'NV': 'W', 'NY': 'N', 'OH': 'M', 'OK': 'S', 'OR': 'W', 'PA': 'N', 'PR': 'O', 'RI': 'N', 'SC': 'S', 'SD': 'M', 'TN': 'S',\n        'TX': 'S', 'UT': 'W', 'VA': 'S', 'VI': 'O', 'VT': 'N', 'WA': 'W', 'WI': 'M', 'WV': 'S', 'WY': 'W'\n    }\n    df.loc[:, 'state'] = df['state'].map(state_mapping)\n    return df\n# preprocess dataframe and add features\ndf = preprocess(df)\ndf.head()\n")),(0,i.kt)("h2",{id:"modeling"},"Modeling"),(0,i.kt)("p",null,"The first thing that always needs to be done is to split the data into train and validation sets as it is a vital step to avoid overfitting, improve generalizability, and help us compare potential models. In this case, we use stratified K-fold cross-validation as our dataset is highly imbalanced and we want to ensure the class distribution is consistent across folds."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/kfold.py"',title:'"/src/kfold.py"'},'from sklearn.model_selection import StratifiedKFold\n\ndef stratKFold(df, n_splits=5):\n    """\n    Perform stratified K fold cross validation on training set\n    :param df: pd dataframe to split\n    :param n_splits: number of folds\n    :return: df with kfold column\n    """\n    # create new column \'kfold\' with val -1\n    df["kfold"] = -1\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n    # target values\n    y = df[\'churn\'].values\n    # initialise kfold class\n    kf = StratifiedKFold(n_splits=n_splits)\n    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n        df.loc[v_, "kfold"] = f\n    return df\n\ndf = stratKFold(df)\n')),(0,i.kt)("h3",{id:"baseline-model"},"Baseline Model"),(0,i.kt)("p",null,"We start by building a simple baseline model so that we have something to compare our later models to. In a regression scenario we could simply use the average of the target variable at every prediction, in our classification case however we are going to use a logistic regression model trained on our two most correlated features."),(0,i.kt)("p",null,"Before we start let's initialize Mlflow and write a general scoring function to evaluate our model."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/model_scoring.py"',title:'"/src/model_scoring.py"'},'import mlflow\nfrom sklearn.metrics import f1_score, recall_score, precision_score\n\n# initialize mlflow\nmlflow.set_experiment("mlflow/customer_churn_model")\n\n# scoring function\ndef score(y, preds):\n    """\n    Returns corresponding metric scores\n    :param y: true y values\n    :param preds: predicted y values\n    :return: f1_score, recall, and precision scores\n    """\n    f1 = f1_score(y, preds)\n    recall = recall_score(y, preds)\n    precision = precision_score(y, preds)\n    return [f1, recall, precision]\n')),(0,i.kt)("p",null,"Now let\u2019s build our baseline model and see how it does on each validation set!"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/evaluate_baseline.py"',title:'"/src/evaluate_baseline.py"'},'from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n# baseline model\nf1_scores, recall_scores, precision_scores = [], [], []\nfor fold in range(5):\n    # define train and validation set\n    features = ["total_day_minutes", "number_customer_service_calls"]\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    # target and features\n    y_train = df_train[\'churn\'].values\n    y_valid = df_valid[\'churn\'].values\n    # init and fit scaler\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(df_train[features])\n    x_valid = scaler.transform(df_valid[features])\n    # create and train model\n    clf = LogisticRegression()\n    clf.fit(x_train, y_train)\n    preds = clf.predict(x_valid)\n    # score model\n    scores = score(y_valid, preds)\n    f1_scores.append(scores[0])\n    recall_scores.append(scores[1])\n    precision_scores.append(scores[2])\n# average scores over each fold\nf1_avg = np.average(f1_scores)\nrecall_avg = np.average(recall_scores)\nprecision_avg = np.average(precision_scores)\nprint(f"Average F1 = {f1_avg}, Recall = {recall_avg}, Precision = {precision_avg}")\n\n# log metrics on mlflow\nwith mlflow.start_run(run_name="lr_baseline") as mlops_run:\n        mlflow.log_metric("F1", f1_avg)\n        mlflow.log_metric("Recall", recall_avg)\n        mlflow.log_metric("Preision", precision_avg)\n')),(0,i.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Average F1 = 0.08725, Recall = 0.04847, Precision = 0.44093"))),(0,i.kt)("p",null,"So yea, the results aren\u2019t great (actually they are terrible) but that only means we are going to get better. This was only a baseline! We can try a few things to improve our model:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"We can balance out our classes by over and under-sampling as the imbalance is causing bias towards the majority class in our model.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"We can train on more features."))),(0,i.kt)("h3",{id:"smote-to-over-sample-the-minority-class"},"SMOTE: To Over-Sample the Minority Class"),(0,i.kt)("p",null,"One problem we have with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary. One way to solve this problem would be to over-sample the examples in the minority class. This could be achieved by simply duplicating examples from the minority class in the training dataset, although this does not provide any additional information to the model. An improvement in duplicating examples from the minority class is to synthesize new examples from the minority class. A common technique for this, introduced in ",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1106.1813"},"this paper"),", is SMOTE. It\u2019s worth noting that oversampling isn\u2019t our only option, we could for example under-sample (or combine a mix of the two) by using a technique such as ",(0,i.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis#Tomek_links"},"TOMEK-links")," (SMOTE-TOMEK helps do both under and over-sampling in one go). In our case, however, the best performance boost came from SMOTE alone."),(0,i.kt)("p",null,"Before we do this, however, let's write a general feature processing pipeline to get our data ready for modeling. Our function returns a Sklearn pipeline object that we can use to fit and transform our data. It first splits the data into numeric, categorical features, and binary features as we process each of these differently. The categorical features are encoded using one-hot encoding, while the binary features are left alone. Finally, the numeric features have their missing values imputed. Scaling the numeric features was also tried but it didn\u2019t lead to a performance increase. It\u2019s also in our best interest to not scale these as it makes interpreting the results harder."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/feature_pipeline.py"',title:'"/src/feature_pipeline.py"'},'from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom mlxtend.feature_selection import ColumnSelector\nfrom category_encoders import HashingEncoder\n\ndef feature_pipeline(config_path="params.yaml"):\n    """\n    :param config_path: path to params.yaml file\n    :return: preprocessing feature pipeline\n    """\n    # load in config information\n    config = read_params(config_path)\n    num_features = config["raw_data_config"]["model_features"]["numeric"]\n    cat_features = config["raw_data_config"]["model_features"]["categorical"]\n    binary_features = config["raw_data_config"]["model_features"]["binary"]\n\n    # transformers\n    transforms = []\n    # categorical pipeline\n    transforms.append(\n        (\n            "categorical",\n            Pipeline(\n                [\n                    ("select", ColumnSelector(cols=cat_features)),\n                    ("encode", OneHotEncoder()),\n                ]\n            ),\n        )\n    )\n    transforms.append(\n        (\n            "binary",\n            Pipeline(\n                [\n                    ("select", ColumnSelector(cols=binary_features)),\n                    ("encode", OrdinalEncoder()),\n                ]\n            ),\n        )\n    )\n    # numeric pipeline\n    transforms.append(\n        (\n            "numeric",\n            Pipeline(\n                [\n                    ("select", ColumnSelector(cols=num_features)),\n                    ("impute", SimpleImputer(missing_values=np.nan, strategy="median")),\n                ]\n            ),\n        )\n    )\n\n    # combine features\n    features = FeatureUnion(transforms)\n    return features\n')),(0,i.kt)("p",null,"A general training function is written below, notice we can choose whether we want to use SMOTE or not. The sampling strategy in SMOTE controls how much we resample the minority class and it\u2019s something we can tune later."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/train.py"',title:'"/src/train.py"'},"from imblearn.over_sampling import SMOTE, SMOTENC\ndef train(fold, df, model=LogisticRegression(solver='newton-cg'), smote=False):\n    \"\"\"\n    :param fold: fold to train model on\n    :param df: pandas dataframe containing our data\n    :param model: model to train data on\n    :param smote: float, if named it is the sampling strategy for SMOTE\n    :return: f1, recall, precision validation score for fold, as well as y_valid and preds\n    \"\"\"\n    # feature pipeline\n    features = feature_pipeline()\n\n    # define train and validation set\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    # target and features\n    y_train = df_train['churn'].values\n    y_valid = df_valid['churn'].values\n\n    # create training and validation features\n    x_train = features.fit_transform(df_train)\n    x_valid = features.transform(df_valid)\n\n    # smote\n    if smote:\n        smt = SMOTE(random_state=42, sampling_strategy=smote)\n        x_train, y_train = smt.fit_resample(x_train, y_train)\n\n    # create and train model\n    clf = model\n    clf.fit(x_train, y_train)\n    preds = clf.predict(x_valid)\n    # score model\n    scores = score(y_valid, preds)\n    return scores, [y_valid, preds]\n")),(0,i.kt)("p",null,"Before we use SMOTE, let\u2019s train a logistic regression model on all of the features to try and get a new baseline."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/logistic_regression.py"',title:'"/src/logistic_regression.py"'},'f1_scores, recall_scores, precision_scores = [], [], []\nfor fold in range(5):\n    scores, _ = train(fold,df, smote=False)\n    f1, recall, precision = scores\n    f1_scores.append(f1)\n    recall_scores.append(recall)\n    precision_scores.append(precision)\n# average scores over each fold\nf1_avg = np.average(f1_scores)\nrecall_avg = np.average(recall_scores)\nprecision_avg = np.average(precision_scores)\nprint(f"Average F1 = {f1_avg}, Recall = {recall_avg}, Precision = {precision_avg}")\n# log metrics on mlflow\nwith mlflow.start_run(run_name="lr_all_features") as mlops_run:\n        mlflow.log_metric("F1", f1_avg)\n        mlflow.log_metric("Recall", recall_avg)\n        mlflow.log_metric("Preision", precision_avg)\n')),(0,i.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Average F1 = 0.30667, Recall = 0.21075, Precision = 0.57206"))),(0,i.kt)("p",null,"The results are definitely better than before but still not great. We\u2019ve waited long enough, let\u2019s try using SMOTE! We are going to use SMOTE to over-sample our churn data points so that we end up with equal class distributions."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/train_and_eval.py"',title:'"/src/train_and_eval.py"'},'def train_and_eval(df, model=LogisticRegression(solver=\'newton-cg\'), smote=0.75, model_name="", params = {}, log_mlflow=True):\n    \'\'\'\n    train model and evaluate it on each fold\n    :param df: pandas dataframe containing our data\n    :param model: model to train data on\n    :param model_name: string, for tracking on mlflow\n    :param params: dict, for tracking on mlflow\n    :param log_mlflow: boolean, if true then log results using mlflow\n    :return: average score for each metric\n    \'\'\'\n    f1_scores, recall_scores, precision_scores = [], [], []\n    for fold in range(5):\n        scores, _ = train(fold, df, model=model, smote=smote)\n        f1, recall, precision = scores\n        f1_scores.append(f1)\n        recall_scores.append(recall)\n        precision_scores.append(precision)\n    # average scores over each fold\n    f1_avg = np.average(f1_scores)\n    recall_avg = np.average(recall_scores)\n    precision_avg = np.average(precision_scores)\n    print(f"Average F1 = {f1_avg}, Recall = {recall_avg}, Precision = {precision_avg}")\n    # log metrics on mlflow\n    if log_mlflow:\n        with mlflow.start_run(run_name=model_name) as mlops_run:\n                mlflow.log_metric("F1", f1_avg)\n                mlflow.log_metric("Recall", recall_avg)\n                mlflow.log_metric("Preision", precision_avg)\n                if params:\n                    mlflow.log_params(params)\n    return f1_avg\n')),(0,i.kt)("p",null,"An important side note: when using SMOTE we need to evaluate performance on a validation set that has ",(0,i.kt)("strong",{parentName:"p"},"not")," been over-sampled. Otherwise, we will not be getting a true performance measure."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},'train_and_eval(df, model_name="lr_all_features_smote")')),(0,i.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Average F1 = 0.49789, Recall = 0.69068, Precision = 0.38947"))),(0,i.kt)("p",null,"Wow! We have boosted the F1 score from 0.31 to 0.50, and the recall has gone from 0.21 to 0.69! An important note is that the precision has practically stayed the same. Why is that? Well, what is precision measuring? Mathematically, precision is the number of true positives divided by the number of true positives plus the number of false positives. It tells us that our model is correct 47% of the time when trying to predict positive samples. So by over-sampling, we have decreased the number of false negatives but we have also increased the number of false positives. This is OK as we decided we will favor false positives over false negatives. An intuitive way to see this change is by looking at a ",(0,i.kt)("strong",{parentName:"p"},"confusion matrix"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/confusion_matrix.py"',title:'"/src/confusion_matrix.py"'},'from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# get preds for non-smote and smote models\n_, evals = train(0, df, smote=False)\n_, evals_smote = train(0, df, smote=True)\n# set axis and plot\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,10))\nax1.set_title("Model without SMOTE")\nConfusionMatrixDisplay.from_predictions(*evals, ax=ax1)\nax2.set_title("Model with SMOTE")\nConfusionMatrixDisplay.from_predictions(*evals_smote, ax=ax2)\nplt.tight_layout()\nplt.show()\n')),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"confusion_matrix",src:a(3318).Z,width:"1057",height:"708"})),(0,i.kt)("p",null,"We can see that the TP number goes from 33 to 90 and the FN number goes from 86 to 29, great! However, as a consequence of this, we see the FP number goes from 21 to 158. But, as mentioned earlier, we are ok with that as we care more about finding out which customers are going to leave. A little side note: the FP and FN rates can be tuned using the probability threshold and the easiest way to compare the two models is to compare F1 scores."),(0,i.kt)("h3",{id:"feature-selection"},"Feature Selection"),(0,i.kt)("p",null,"We can train a more complicated model and then use this to select features. Specifically, we train a random forest classifier and then use SHAP values to select the most promising features. Narrowing down the feature space helps reduce dimensionality and generalizability while also making interpreting results easier. It\u2019s a win-win!"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/feature_selection.py"',title:'"/src/feature_selection.py"'},"from sklearn.ensemble import RandomForestClassifier\nfeatures = feature_pipeline()\nX = np.asarray(features.fit_transform(df).todense())\ny = df['churn'].values\n\nclf = RandomForestClassifier()\nmodel = clf.fit(X,y)\n\nimport shap\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\nshap.summary_plot(shap_values, features=X, feature_names=feature_names, plot_type='bar)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"imp",src:a(1512).Z,width:"657",height:"566"})),(0,i.kt)("p",null,"We can now define a new pipeline to preprocess and only select our needed features. With this, let\u2019s see how our logistic regression model does with our narrowed-down features!"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},'train_and_eval(df, model_name="lr_selected_features_smote")')),(0,i.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Average F1 = 0.50255, Recall = 0.69403, Precision = 0.39407"))),(0,i.kt)("p",null,"Awesome, we\u2019ve removed features and our performance increases slightly. This confirms to us that the other features weren\u2019t important."),(0,i.kt)("h3",{id:"but-can-we-do-better-than-our-logistic-regression"},"But can we do better than our logistic regression?"),(0,i.kt)("p",null,"It would be easy here to go all guns blazing and train an XGBOOST model, but remember that is ",(0,i.kt)("strong",{parentName:"p"},"not")," our goal. Our goal is to build an interpretable model that we can use to try and keep customers from leaving. As well as logistic regression, decision tree classifiers are very interpretable. Let\u2019s see how it gets on."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/random_forest.py"',title:'"/src/random_forest.py"'},'from sklearn.tree import DecisionTreeClassifier\ntrain_and_eval(df, model=DecisionTreeClassifier(), model_name="dt_selected_features_smote")\n')),(0,i.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Average F1 = 0.81529, Recall = 0.84614, Precision = 0.78694"))),(0,i.kt)("p",null,"It does well! We need to be ",(0,i.kt)("strong",{parentName:"p"},"very")," careful though as decision trees overfit like there is no tomorrow. Let\u2019s go ahead and tune hyperparameters on both models to see if we can optimize things a little more. We will use ",(0,i.kt)("a",{parentName:"p",href:"https://optuna.org/"},"Optuna")," as I just love how easy and fast it is."),(0,i.kt)("h3",{id:"hyperparameter-tuning"},"Hyperparameter Tuning"),(0,i.kt)("p",null,"We need a be super careful here, decision trees are very prone to overfitting and this is why random forest models are usually preferred. The random forest can generalize over the data in a better way as the randomized feature selection acts as a form of regularization. As discussed earlier though, in our case we care more about interpretability than performance. Now, although cross-validation is great for seeing how the model is generalizing, it doesn\u2019t necessarily prevent overfitting as we will just end up overfitting the validation sets."),(0,i.kt)("p",null,"One measure of overfitting is when the training score is much higher than the testing score. I initially tried setting the objective function in the Optuna trial to the cross-validated validation scores but this still lead to overfitting as DTs don\u2019t have much regularization."),(0,i.kt)("p",null,"Another possibility, that is this case worked superbly, is weighting the difference between cross-validated training scores and validation scores vs the validation score itself. For example, for F1 scores, a possible objective function is"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"alt",src:a(949).Z,width:"1040",height:"170"})),(0,i.kt)("p",null,"In this case, the RMSE of the difference between validation and training is weighted four times less than the validation F1 score. Optimizing this function forces the train and valid score to stay close, while also maximizing the validation score."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/optuna.py"',title:'"/src/optuna.py"'},"from optuna import Trial, create_study\nfrom optuna.samplers import TPESampler\nfrom scipy.stats import loguniform\n\ndef objective(trial, n_jobs=-1, random_state=42):\n    '''\n    Objective function to optimize our custom metric using Optuna's TPE sampler\n    '''\n    # smote param space\n    smote_space = {'sampling_strategy': trial.suggest_uniform('sampling_strategy', 0.5, 1)}\n    # define search spaces\n    if model == LogisticRegression:\n        params = {\n            'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n            'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n            'C': trial.suggest_float(\"C\", 1.0, 10.0, log=True),\n            'tol': trial.suggest_float(\"tol\", 0.0001, 0.01, log=True),\n            'max_iter': trial.suggest_int('max_iter', 100, 1000)\n\n        }\n    else:\n        params = {\n            'max_depth': trial.suggest_int('max_depth',2,10),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf',1,30),\n            'min_samples_split': trial.suggest_int('min_samples_split',2,10),\n            'criterion': trial.suggest_categorical('criterion', [\"gini\", \"entropy\"])\n        }\n\n    # feature pipeline\n    features = feature_pipeline()\n    # create training and validation features\n    X = features.fit_transform(df)\n    y = df['churn'].values\n\n    train_f1, valid_f1 = [], []\n    # Create StratifiedKFold object.\n    strat = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n    for train_index, test_index in strat.split(X, y):\n        # split data\n        x_train, x_valid = X[train_index], X[test_index]\n        y_train, y_valid = y[train_index], y[test_index]\n        # feature transformations and smote\n        smt = SMOTE(random_state=42, **smote_space)\n        x_train_smt, y_train_smt = smt.fit_resample(x_train, y_train)\n        # train model\n        clf = model(**params)\n        clf.fit(x_train_smt, y_train_smt)\n        # compute f1 score on valid and training data (without SMOTE!)\n        preds_train = clf.predict(x_train)\n        preds_valid = clf.predict(x_valid)\n        train_f1.append(f1_score(y_train, preds_train))\n        valid_f1.append(f1_score(y_valid, preds_valid))\n    # compute mean of f1 train/valid scores\n    train_f1_mean, valid_f1_mean = np.array(train_f1).mean(), np.array(valid_f1).mean()\n    # train/test cross score\n    return train_valid_f1_score(train_f1_mean, valid_f1_mean)\n")),(0,i.kt)("p",null,"This gives us the following results:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Logistic Regression Best Hyperparameters: {\u2018solver\u2019 : \u2018liblinear\u2019, \u2018penalty\u2019 : \u2018l2\u2019, \u2018C\u2019 : 1.14, \u2018tol\u2019 : 0.0002, max_iter : 150}"),(0,i.kt)("li",{parentName:"ul"},"Decision Tree Best Hyperparameters: {\u2018max_depth\u2019: 7, \u2018min_samples_leaf\u2019: 4, \u2018min_samples_split\u2019: 10, \u2018criterion\u2019: \u2018gini\u2019},"),(0,i.kt)("li",{parentName:"ul"},"SMOTE Sampling Strategy 0.70 for LR and 0.56 for DT")),(0,i.kt)("p",null,"Let\u2019s train some models with these parameters and see what we get!"),(0,i.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"LOGISTIC REGRESSION:\nAverage training F1 score: 0.50095, Average validation F1 score: 0.50340, Overfit score: 1.98883"),(0,i.kt)("p",{parentName:"div"},"DECISION TREE\nAverage training F1 score: 0.91534, Average validation F1 score: 0.89280, Overfit score: 0.45129"))),(0,i.kt)("p",null,"Although this model looks great and doesn\u2019t appear to be overfitting we are going to go with the model below that has been tuned with a lower maximum depth. Our goal is interpretability and a depth of 7 doesn\u2019t really give us that. So we are sacrificing a little bit of accuracy for interpretability."),(0,i.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Average training F1 score: 0.82174, Average validation F1 score: 0.81610, Overfit score: 0.74122"))),(0,i.kt)("p",null,"Great, so we now have a few potential models. We are going to move forward with the decision tree model as the logistic regression model isn\u2019t quite up to scratch."),(0,i.kt)("h3",{id:"pruning-the-decision-tree"},"Pruning the Decision Tree"),(0,i.kt)("p",null,"Although our model doesn\u2019t appear to be overfitting, we are still going to prune the decision tree as it helps us get rid of sub-nodes that don\u2019t have much predictive power. We do this with the hope that this helps our model generalize better. An added bonus, that tends to come with most regularization, is that it also helps improve the interpretability of the model."),(0,i.kt)("p",null,"We can prune our tree by picking the right cost complexity parameter. We will start by training a model on the whole dataset, with our chosen hyperparams, to find our space of ",(0,i.kt)("strong",{parentName:"p"},"\u03b1"),"\u2019s \u2014 the cost complexity parameter. We can then score each of these ",(0,i.kt)("strong",{parentName:"p"},"\u03b1"),"\u2019s in a cross-validated way to find the best complexity to choose."),(0,i.kt)("p",null,"We found that the value ",(0,i.kt)("strong",{parentName:"p"},"\u03b1")," = 0.00811411 is the best complexity to choose. In general, as ",(0,i.kt)("strong",{parentName:"p"},"\u03b1")," increases the number of nodes and depth decreases. So we pick the highest \u03b1 value that still has a good average F1 score."),(0,i.kt)("p",null,"We can now train our final model!"),(0,i.kt)("h3",{id:"how-does-it-perform-on-test-data"},"How Does It Perform on Test Data"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/predict.py"',title:'"/src/predict.py"'},"def predict(X, clf, feature_pipeline, thresh=0.5):\n    '''\n    Predict customer churn on new data\n    :param X: data containing features\n    :param clf: trained model\n    :param feature_pipeline: trained feature processing pipeline\n    :param thresh: prediction threshold\n    :return: predictions\n    '''\n    X = feature_pipeline.transform(X)\n    preds = (clf.predict_proba(X)[:,1] >= thresh).astype(int)\n    return preds\n\ndf_test = pd.read_csv(\"data/external/test.csv\")\ndf_test = preprocess(df_test, target='churn')\ny_test = df_test['churn'].values\npreds = predict(df_test, clf, features)\nf1, recall, precision = score(y_test, preds)\nprint(f\"Average F1 = {f1}, Recall = {recall}, Precision = {precision}\")\n")),(0,i.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Average F1 = 0.77729, Recall = 0.65925, Precision = 0.94680"))),(0,i.kt)("p",null,"The model does well on the test data! We can see that the precision is a lot higher than the recall however but this is something that can be tuned by changing the prediction probability threshold. In our case, we are trying to get to 80% recall while maximizing precision."),(0,i.kt)("h3",{id:"picking-the-optimal-probability-threshold"},"Picking the Optimal Probability Threshold"),(0,i.kt)("p",null,"We can now tune the probability threshold to try and optimize our precision-recall trade-off. Let\u2019s plot a precision-recall curve and find the optimal threshold to achieve 80% recall."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/precision_recall.py"',title:'"/src/precision_recall.py"'},'from sklearn.metrics import PrecisionRecallDisplay\n# training data\nx_train = features.transform(df)\ny_train = df[\'churn\'].values\n# precision recall curve\ndisplay = PrecisionRecallDisplay.from_estimator(\n    clf, x_train, y_train, name="Decision tree classifier"\n)\n_ = display.ax_.set_title("Precision-Recall curve")\n')),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"recall",src:a(8691).Z,width:"388",height:"279"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/optimize_threshold.py"',title:'"/src/optimize_threshold.py"'},"from sklearn.metrics import precision_recall_curve\ndef optimize_threshold(clf, df, recall = 0.80):\n    '''\n    Optimize prob. threshold on training dataset\n    :param df: pandas dataframe\n    :param recall: desired recall\n    :return: optimal prob. threshold\n    '''\n    # create features and target labels\n    X = features.transform(df)\n    y = df['churn'].values\n\n    # get scores for valid data\n    y_scores = clf.predict_proba(X)[:, 1]\n    # locate where recall is closest to 0.80\n    precisions, recalls, thresholds = precision_recall_curve(y, y_scores)\n    distance_to_optim = abs(recalls - recall)\n    optimal_idx = np.argmin(distance_to_optim)\n    thresh = thresholds[optimal_idx]\n    return thresh\n\nthresh = optimize_threshold(clf, df)\nprint(thresh)\n")),(0,i.kt)("p",null,"We get a threshold of 0.426. If we predict on our test set again then hopefully we will see something closer to our desired recall! And yes, I know I\u2019ve committed the cardinal sin of using the test set twice but this was for demonstration purposes. The test set is normally a \u2018one and done\u2019 situation. Our final results are:"),(0,i.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Average F1 = 0.79584, Recall = 0.85185, Precision = 0.74675"))),(0,i.kt)("p",null,"Awesome! We have done what we wanted to do, and things work well! It\u2019s also great to see test scores so similar to our earlier scores as it shows we haven\u2019t overfitted our model. Let\u2019s log this model on MLFLOW."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'# log metrics on mlflow\nwith mlflow.start_run(run_name="Final Decision Tree") as mlops_run:\n        mlflow.log_metric("F1", f1)\n        mlflow.log_metric("Recall", recall)\n        mlflow.log_metric("Preision", precision)\n        mlflow.log_params(params)\n        mlflow.log_params({"prob_thresh": 0.426})\n        # log model and pipeline\n        mlflow.sklearn.log_model(clf, "clf")\n        mlflow.sklearn.log_model(features, "features_pipeline")\n')),(0,i.kt)("h2",{id:"mlflow-experiment-session"},"MLFLOW Experiment Session"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"mlflow",src:a(4206).Z,width:"1708",height:"478"})),(0,i.kt)("h2",{id:"model-interpreting"},"Model Interpreting"),(0,i.kt)("p",null,"Tree-based models split the data multiple times according to certain cutoff values in the features. By splitting, different subsets of the dataset are created, where each instance belongs to a certain subset. To predict the outcome in each leaf node, the average outcome of the training data in this node is used."),(0,i.kt)("p",null,"The interpretation for decision trees is very easy: Starting from the root node, you go to the next nodes and the edges tell you which subsets you are looking at. Once you reach the leaf node, the node tells you the predicted outcome. All the edges are connected by \u2018AND\u2019."),(0,i.kt)("p",null,"So the general way we can predict is: If feature x is ",(0,i.kt)("strong",{parentName:"p"},"smaller")," (/bigger) than threshold c ",(0,i.kt)("strong",{parentName:"p"},"AND")," \u2026 then the predicted outcome is the most common value of the target of the instances in that node.\nLet\u2019s plot out the tree and see what we can infer!"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/plot_model.py"',title:'"/src/plot_model.py"'},"# get feature names\nnumeric_features = [\n    'total_charge', 'number_customer_service_calls', 'total_day_minutes', 'total_day_charge',\n    'total_minutes', 'total_intl_calls', 'total_intl_minutes', 'number_vmail_messages', 'total_intl_charge'\n]\nbinary_features = ['international_plan', 'voice_mail_plan']\nfeature_names = np.append(numeric_features, binary_features)\n\n# plot tree from our model\nimport graphviz\nfrom sklearn import tree\ntree_graph = tree.export_graphviz(clf, out_file=None,\n                              feature_names=feature_names,\n                              class_names=['No churn', 'Churn'],\n                              filled=True, rounded=True,\n                              special_characters=True)\n\ngraph = graphviz.Source(tree_graph)\ngraph\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"model",src:a(121).Z,width:"1436",height:"1002"})),(0,i.kt)("p",null,"Each node in the tree will give a condition and the left node below is True and the right node is False. The first split was performed with the total day minutes feature, which counts the total minutes of all calls made in the day. We can see for example that if the total minutes are less than 71 we follow the tree left and if the minutes are greater than 71 we go right."),(0,i.kt)("p",null,"Each prediction from the tree is made by following the tree down until a root node is hit. For example, if a customer has less than 71 total day minutes and their total charge is between 0.04 and 1 then we would predict them to churn."),(0,i.kt)("p",null,"We can see that the charge from the telecom provider seems to be a big distinguishing factor between customers and this is confirmed by the SHAP feature importance plot earlier. By following the tree left we can see that customers with a high day charge but low day minutes tend to churn more than stay. If the day charge is less than 3 however, the customers tend to stay no matter what the minutes are! One possible explanation for this could be that the customers churning are on mobile plans that don\u2019t correctly suit their needs, this would need to be investigated further though."),(0,i.kt)("p",null,"Another interesting observation is that if a customer has a high total day minute (>71) and they do not speak to customer service (<0.965 calls i.e. no calls) they are more likely to churn than customers that do speak to customers service. Again, this would need further investigation to draw conclusions as to why this is true.\nAs with most data problems, it quite often leads to more questions to be answered!"),(0,i.kt)("h2",{id:"conclusion"},"Conclusion"),(0,i.kt)("p",null,"We have built an interpretable machine learning model that can identify customers that are likely to churn with our desired recall of 80% (we actually achieved 85% on the test set) and a precision of 75%. That is we identify 85% of the churned customers correctly and 75% of our churn predictions are accurate. Using this model we can then understand the key factors driving customers to leave and hopefully we can use this to try and keep more customers in the long run."),(0,i.kt)("p",null,"Thanks for reading and I hope you enjoyed it. If you have any comments or questions please feel free to reach out."))}u.isMDXComponent=!0},3318:function(e,t,a){t.Z=a.p+"assets/images/confusion_matix-e357a717bc8192f47b033a6f877cf244.png"},5067:function(e,t,a){t.Z=a.p+"assets/images/corr_matrix-5cf7d8f20147d8ab23b4a23fc511c2d4.png"},3630:function(e,t,a){t.Z=a.p+"assets/images/data_head-cc6017c6d4e6eea7eab7fbcf9f0451f0.png"},1512:function(e,t,a){t.Z=a.p+"assets/images/feature_importance-ed9a5feb9f46dd316e6f387ec1b8265d.png"},8285:function(e,t,a){t.Z=a.p+"assets/images/imbalanced_meme-bab6c51c1b7314b45a9e489782d448fa.jpeg"},4206:function(e,t,a){t.Z=a.p+"assets/images/mlflow-911bee12339357fd21202de8dd94380b.png"},121:function(e,t,a){t.Z=a.p+"assets/images/model-05c504e24eb39a276f0676113850575a.png"},949:function(e,t,a){t.Z=a.p+"assets/images/objective-b0417870e41659c4158dca2307e66701.png"},8691:function(e,t,a){t.Z=a.p+"assets/images/precision_recall-caa96de7f5f5c2be9663c03532fd58f1.png"}}]);