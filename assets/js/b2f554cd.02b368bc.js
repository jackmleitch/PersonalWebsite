"use strict";(self.webpackChunkjackmleitch_com_np=self.webpackChunkjackmleitch_com_np||[]).push([[1477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"Essay-Companion","metadata":{"permalink":"/blog/Essay-Companion","source":"@site/blog/2022-06-10-EssayCompanion.md","title":"Building NLP Powered Applications with Hugging Face Transformers","description":"... and deploying on Google Chrome with FastAPI and\xa0Docker","date":"2022-06-10T00:00:00.000Z","formattedDate":"June 10, 2022","tags":[{"label":"NLP","permalink":"/blog/tags/nlp"},{"label":"Hugging Face","permalink":"/blog/tags/hugging-face"},{"label":"Chrome Extension","permalink":"/blog/tags/chrome-extension"},{"label":"Docker","permalink":"/blog/tags/docker"},{"label":"Python","permalink":"/blog/tags/python"}],"readingTime":11.235,"truncated":true,"authors":[{"name":"Jack Leitch","title":"Machine Learning Engineer","url":"https://github.com/jackmleitch","imageURL":"https://github.com/jackmleitch.png","key":"jack"}],"frontMatter":{"slug":"Essay-Companion","title":"Building NLP Powered Applications with Hugging Face Transformers","tags":["NLP","Hugging Face","Chrome Extension","Docker","Python"],"authors":"jack"},"nextItem":{"title":"Building Interpretable Models on Imbalanced Data","permalink":"/blog/Churn-Prediction"}},"content":"**... and deploying on Google Chrome with FastAPI and\xa0Docker**\\n\\nI recently finished the fantastic new Natural Language Processing with Transformers book written by a few guys on the Hugging Face team and was inspired to put some of my newfound knowledge to use with a little NLP-based project. While searching for some ideas I came across an [excellent blog post](https://medium.com/data-science-at-microsoft/developing-microsoft-edge-extensions-powered-by-sota-nlp-models-f3991f18daa4) by Tezan Sahu in which he built a Microsoft Edge extension to paraphrase text highlighted on your screen. I wanted to take this a step further by:\\n\\n1. optimizing model inference with **ONNX runtime** and **quantization**,\\n\\n2. include features such as summarization, name entity recognition (NER), and keyword extraction.\\n\x3c!--truncate--\x3e\\n\\n![gif](./images/EssayCompanion/EssayCompanionDemoShort.gif)\\n\\nThe idea is that this creates the ultimate essay companion as it can help quickly understand text with the summaries and NER, and it can get those creative juices flowing with the paraphrased text and keyword synonyms. Obviously, I would hope people use it to rewrite their own work and not other peoples\u2026\\n\\n:::info\\n\\n**TL;DR:** [This repository](https://github.com/jackmleitch/EssayCompanion) contains all the code mentioned in this article. ML stuff can be found in the [src](https://github.com/jackmleitch/EssayCompanion/tree/main/src) folder and Chrome extension stuff is in the [extension](https://github.com/jackmleitch/EssayCompanion/tree/main/extension) folder.\\n\\n:::\\n\\n## Models\\n\\nThere are four different models (and tokenizers) in action in this extension, three of which were found on Hugging Face!\\n\\n### [T5 Paraphrasing Model](https://huggingface.co/ramsrigouthamg/t5-large-paraphraser-diverse-high-quality)\\n\\nThis was the largest model used, coming in at 2.75Gb (!), and was fine-tuned for paraphrasing by Ramsri Goutham. **T5** is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format.\\n\\nThe text is first split into sentences using **NLTK**\'s sentence tokenizer sent_tokenize. Each sentence was then passed through the T5 model and the paraphrased output for each sentence was joined to get a new paraphrased paragraph.\\n\\n```py title=\\"/src/paraphrase/paraphrasing_model.py\\"\\nclass ParaphraseModel:\\n    \\"\\"\\"Provides utility to load HuggingFace paraphrasing model and generate paraphrased text.\\"\\"\\"\\n\\n    def __init__(self, model_ckpt: str=\\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\\",\\n        num_beams: int=5) -> None:\\n        \\"\\"\\"\\n        :param model_ckpt: path to HuggingFace model checkpoint, default is the T5 paraphraser\\n        :param num_beams: number of beams to perform beam search when generating new text\\n        \\"\\"\\"\\n        self.num_beams = num_beams\\n        self.model_ckpt = model_ckpt\\n        self.torch_device = \\"cuda\\" if torch.cuda.is_available() else \\"cpu\\"\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\\n\\n    def paraphrase_text_model(self, input_text: str) -> str:\\n        \\"\\"\\"\\n        Tokenize sentences and then pass to model to paraphrase text.\\n        :param input_text: input text to feed to model\\n        :return: paraphrased text\\n        \\"\\"\\"\\n        sentences = sent_tokenize(input_text)\\n        batch = (self.tokenizer(sentences, truncation=True, padding=\\"longest\\",\\n            max_length=100, return_tensors=\\"pt\\").to(self.torch_device))\\n        translated = self.model.generate(**batch, max_length=100, num_beams=self.num_beams,\\n            num_return_sequences=1, temperature=1.5)\\n        paraphrased_text = self.tokenizer.batch_decode(translated, skip_special_tokens=True)\\n        if self.model_ckpt == \\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\\":\\n            # remove \'paraphrasedoutput: \' from result\\n            paraphrased_text = [sentence[19:] for sentence in paraphrased_text]\\n        paraphrased_text = \\" \\".join(paraphrased_text)\\n        return paraphrased_text\\n```\\n\\nSome example output:\\n\\n_The ultimate test of your knowledge is your capacity to convey it to another **=>** Your ability to pass it from one to another is the ultimate measure of your intelligence._\\n\\nThat\'s pretty good, we can see our paraphrased text is coherent and has a different structure from the original text!\\n\\n### [BART Summarization Model](https://huggingface.co/facebook/bart-large-cnn)\\n\\n**BART** is also an encoder-decoder (seq2seq) model with a bidirectional (like BERT) encoder and an autoregressive (like GPT) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\\n\\nAs an example, a recent 250-word long [news article](https://www.eurogamer.net/portable-electronic-devices-sold-in-the-eu-will-require-usb-c-charging-by-autumn-2024) regarding USB-C rule enforcements in the EU is summarized in 55 words:\\n\\n_By autumn 2024, all portable electronic devices sold in the EU will need to use USB Type-C for charging. The aim is to reduce electronic waste and be more consumer-friendly by having just one \\"common charger\\". The biggest impact will be Apple\'s iPhones and iPads, which will no longer be able to use lightning cables._\\n\\nWow! It\'s worked superbly and picks out all the key points made in the article concisely.\\n\\n### [DistilBERT NER\xa0Model](https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english)\\n\\nA **DistilBERT** base uncased model fine-tuned for **NER** using the conll03 English dataset was used. DistilBERT is a smaller and faster model than **BERT**, which was pre-trained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. The NER model is simply a DistilBERT encoder with a token classification head added to the end to predict each entity: person, location, organization, and misc.\\n\\n```py title=\\"/src/ner/ner_model.py\\"\\nclass NERModel:\\n    \\"\\"\\"Load name entity recognition model and build prediction pipeline\\"\\"\\"\\n\\n    def __init__(self,\\n        model_ckpt: str=\\"elastic/distilbert-base-uncased-finetuned-conll03-english\\") -> None:\\n        self.model_ckpt = model_ckpt\\n        self.tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\\n        self.model = AutoModelForTokenClassification.from_pretrained(model_ckpt)\\n        # id to label mapping to convert predictions back to entities\\n        self.id2label = self.model.config.id2label\\n\\n    def ner_pipeline(self, query: str) -> List[dict]:\\n        \\"\\"\\"\\n        Build ner pipeline and return parsed model output\\n        :param query: query to pass to the model\\n        \\"\\"\\"\\n        pipe = pipeline(model=self.model_ckpt, tokenizer=self.tokenizer, task=\'ner\',\\n            aggregation_strategy=\\"simple\\")\\n        return pipe(query)\\n```\\n\\nIn the chrome extension, the NER results were rendered in HTML using **SpaCy**. Some example output is below.\\n\\n![image](./images/EssayCompanion/ner_results.png)\\n\\n### [KeyBERT Keyword Extraction Model](https://maartengr.github.io/KeyBERT/)\\n\\nThis model leverages BERT text embeddings and cosine similarity to find the sub-phrases in a document that are the most similar to the document itself. The idea is that these sub-phrases are the most important phrases in the text.\\n\\nKeywords are first extracted from the text using the **KeyBERT** model. Once the keywords are found, **WordNet** is used to find synonyms for each keyword. It\'s worth noting that in WordNet, similar words are grouped into a set known as a Synset and the words in a Synset are lemmatized.\\n\\n```py title=\\"/src/keyword/keybert_model.py\\"\\nclass GetKeywords():\\n    \\"\\"\\"Extract keywords and find synonyms for them\\"\\"\\"\\n\\n    def __init__(self) -> None:\\n        self.model = KeyBERT()\\n\\n    def get_keywords(self, query: str) -> List[str]:\\n        \\"\\"\\"Get keywords from query text using KeyBERT\\"\\"\\"\\n        keywords = self.model.extract_keywords(query)\\n        return [word[0] for word in keywords]\\n\\n    def get_synonyms_for_keyword(self, keyword: str, max_synonyms: int=5) -> List[str]:\\n        \\"\\"\\"Find synonyms for a given word\\"\\"\\"\\n        synonyms = []\\n        for synonym in wordnet.synsets(keyword):\\n            for lemma in synonym.lemmas():\\n                synonyms.append(lemma.name().replace(\\"_\\", \\" \\"))\\n        return [synonym for synonym in list(set(synonyms))\\n                    if synonym.lower() != keyword.lower()][:max_synonyms]\\n\\n    def get_synonyms_for_keywords(self, query: str, max_synonyms: int=5) -> Dict[str, List]:\\n        \\"\\"\\"Find synonyms for all keywords and return them\\"\\"\\"\\n        keywords = self.get_keywords(query)\\n        keyword_synonyms = {}\\n        for keyword in keywords:\\n            synonyms = self.get_synonyms_for_keyword(keyword, max_synonyms)\\n            if len(synonyms) > 0:\\n                keyword_synonyms[keyword] = synonyms\\n        return keyword_synonyms\\n```\\n\\nThe keywords and associated synonyms found in the sentence \'The ultimate test of your **knowledge** is your capacity to **convey** it to another\' are:\\n\\n- Knowledge: comprehension, cognition, grasp\\n- Convey: transport, carry, deliver\\n\\n## Model Optimization\\n\\nThe initial performance of the models was _OK_ when running inference on GPU but very poor when running on CPU. Even GPU inference wasn\'t as fast as I\'d like since the decoder models have to sequentially decode the model output which cannot be parallelized. Memory was also an issue as I wanted to be able to use this extension on devices with limited memory and compute resources (I have my _ancient_ 2015 MacBook pro in mind\u2026) and two of the models were over 1.5Gb in size! My goal was to reduce the memory footprint of these models as much as possible and optimize performance on CPU inference while maintaining model performance.\\n\\n**Quantization** and **distillation** are two techniques commonly used to deal with size and performance challenges. Distillation was already used with the NER model as DistilBERT is a distilled version of the O.G. BERT model. In my case, distillation of T5/BART was out of the question due to my limited compute resources.\\n\\nThe first step I took was to convert the PyTorch models to ONNX, an open representation format for machine learning algorithms, this allows us to optimize inference for a targeted device (CPU in this case). The Hugging Face Transformer library includes a [tool](https://github.com/huggingface/transformers/blob/main/src/transformers/convert_graph_to_onnx.py) to easily convert models to ONNX and this was used to convert the DistilBERT model. Converting the encoder-decoder models was a little trickier as seq2seq conversions currently aren\'t supported by Hugging Face\'s ONNX converter. I was able to make use of [this](https://github.com/Ki6an/fastT5) fantastic GitHub repository, however, which converts the encoder and decoder separately and wraps the two converted models in the Hugging Face Seq2SeqLMOutput class.\\n\\nAfter the models were converted to ONNX, QInt8 quantization was used to approximate floating-point numbers with lower bit width numbers, dramatically reducing the memory footprint of the model and accelerating performance as computations can be further optimized. Quantization can however introduce a loss in performance as we lose information in the transformation, but it has been extensively demonstrated (e.g. see [here](https://arxiv.org/pdf/2103.13630.pdf)) that weights can be represented in 8-bit integers without a significant drop in performance. It comes as no surprise that the quantization of ONNX models is super easy!\\n\\n```py title=\\"/src/ner/quantize_ner_model.py\\"\\nfrom onnxruntime.quantization import quantize_dynamic, QuantType\\n\\nmodel_input = \\"models/ner/model/model.onnx\\"\\nmodel_output = \\"models/ner/model/model_quant.onnx\\"\\nquantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)\\n```\\n\\nWe can then optimize model inference for CPU usage!\\n\\n```py title=\\"/src/ner/onnx_ner_model.py\\"\\nfrom onnxruntime import InferenceSession, SessionOptions, GraphOptimizationLevel\\nfrom transformers import AutoTokenizer\\n\\ndef create_model_for_provider(model_path, provider=\'CPUExecutionProvider\'):\\n    \\"\\"\\"\\n    Create CPU inference session for ONNX runtime to boost performance\\n    :param model_path: path to *.onnx model\\n    :param provider: CPU/CUDA\\n    :return: onnx runtime session\\n    \\"\\"\\"\\n    options = SessionOptions()\\n    options.intra_op_num_threads = 1\\n    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\\n    session = InferenceSession(str(model_path), options, providers=[provider])\\n    session.disable_fallback()\\n    return session\\n\\nclass NEROnnxModel():\\n    \\"\\"\\"Build NER onnx model and aggregate results into data to be rendered\\"\\"\\"\\n    def __init__(self) -> None:\\n        self.model_path = \\"models/ner/model/model_quant.onnx\\"\\n        self.model = create_model_for_provider(self.model_path)\\n        self.tokenizer = AutoTokenizer.from_pretrained(\\"models/ner/tokenizer/\\")\\n        self.id2label = {0: \\"O\\", 1: \\"B-PER\\", 2: \\"I-PER\\", 3: \\"B-ORG\\",\\n            4: \\"I-ORG\\", 5: \\"B-LOC\\", 6: \\"I-LOC\\", 7: \\"B-MISC\\", 8: \\"I-MISC\\"}\\n\\n    def __call__(self, sentence: str) -> str:\\n        # get model inputs and other required arrays\\n        model_inputs = self.tokenizer(sentence, return_tensors=\\"np\\",\\n            return_special_tokens_mask=True, return_offsets_mapping=True)\\n        special_tokens_mask = model_inputs.pop(\\"special_tokens_mask\\")[0]\\n        offset_mapping = model_inputs.pop(\\"offset_mapping\\")[0]\\n        # pass to model\\n        logits = self.model.run(None, dict(model_inputs))[0]\\n        predictions = np.argmax(logits, axis=2)[0]\\n        input_ids = model_inputs[\\"input_ids\\"][0]\\n        model_outputs = {\\"sentence\\": sentence, \\"input_ids\\": input_ids, \\"predictions\\": predictions,\\n            \\"offset_mapping\\": offset_mapping, \\"special_tokens_mask\\": special_tokens_mask}\\n        # aggregate entitity information\\n        pre_entities = self.gather_pre_entities(**model_outputs)\\n        entities = self.aggregate_words(pre_entities)\\n        entities = self.group_entities(entities)\\n        results = self.build_final_output(sentence, entities)\\n        return results\\n\\n    ...\\n```\\n\\nThe results from the model optimizations are shown below. We can see that our efforts resulted in a **~2x reduction in size** and a **~3x latency boost**!\\n\\n![alt-text-1](./images/EssayCompanion/model_size.png \\"title-1\\") ![alt-text-2](./images/EssayCompanion/model_latency.png \\"title-2\\")\\n\\n## Deploying a Model Endpoint with FastAPI +\xa0Docker\\n\\nTo deploy our application, I used two tools as the main building blocks: [FastAPI](https://fastapi.tiangolo.com) and [Docker](https://www.docker.com). FastAPI makes building a web framework around the models super easy and Docker is a containerization tool allowing us to easily package and run the application in any environment.\\n\\nUsing FastAPI we package the model and build an API to communicate with it. We also use Pydantic to validate user input and model output, we can never be too careful! For example, we ensure the input text is a string, the response from the summarization model is a string, and the keyword extraction model returns a dictionary containing a list of strings.\\n\\n```py title=\\"/src/main.py\\"\\nfrom fastapi import FastAPI\\nfrom pydantic import BaseModel\\nfrom starlette.middleware.cors import CORSMiddleware\\n\\nparaphrasing_pipeline = ParaphraseOnnxPipeline(num_beams=8)\\nner_pipeline = NEROnnxModel()\\nsummarization_pipeline = SummarizeOnnxPipeline(num_beams=8)\\nkeyword_pipeline = GetKeywords()\\n\\napp = FastAPI()\\n\\n# allow CORS requests from any host so that the JavaScript can communicate with the server\\napp.add_middleware(\\n    CORSMiddleware, allow_origins=[\\"*\\"], allow_methods=[\\"*\\"], allow_headers=[\\"*\\"])\\n\\nclass Request(BaseModel):\\n    text: str\\n\\n...\\n\\nclass KeywordResponse(BaseModel):\\n    response: Dict[str, List[str]]\\n\\nclass AllModelsResponse(BaseModel):\\n    original: str\\n    paraphrased: ParagraphResponse\\n    name_entities: NERResponse\\n    summarized: ParagraphResponse\\n    keyword_synonyms: KeywordResponse\\n\\n@app.post(\\"/predict\\", response_model=AllModelsResponse)\\nasync def predict(request: Request):\\n    paraphrased_text = ParagraphResponse(text=paraphrasing_pipeline(request.text))\\n    ner_text = NERResponse(render_data=ner_pipeline(request.text))\\n    summarized_text = ParagraphResponse(text=summarization_pipeline(request.text))\\n    keyword_synonyms = KeywordResponse(response=keyword_pipeline.get_synonyms_for_keywords(request.text))\\n    return AllModelsResponse(\\n        original=request.text, paraphrased=paraphrased_text,\\n        name_entities=ner_text, summarized=summarized_text,\\n        keyword_synonyms=keyword_synonyms\\n    )\\n```\\n\\nDocker is then used to containerize the API server, this allows us to run the application on any machine without worrying about the faff of reproducing my exact environment. To build a Docker image of the server, I created a Dockerfile in the root folder of the project.\\n\\n```js title=\\"Dockerfile\\"\\nFROM ubuntu:20.04\\n\\nCOPY ./src /api/api\\nCOPY ./models /api/models\\nCOPY requirements.txt /requirements.txt\\n\\nRUN apt-get update \\\\\\n    && apt-get install python3-dev python3-pip -y \\\\\\n    && pip3 install -r requirements.txt\\n\\nRUN python3 -m nltk.downloader punkt wordnet omw-1.4\\n\\nENV PYTHONPATH=/api\\nWORKDIR /api\\n\\nEXPOSE 8000\\n\\nENTRYPOINT [\\"uvicorn\\"]\\nCMD [\\"api.main:app\\", \\"--host\\", \\"0.0.0.0\\"]\\n```\\n\\nTo host the extension online we could use either **Azure\'s Container Service** or **AWS\' Elastic Container Service**. I haven\'t gotten around to this yet but plan on doing it at some point soon!\\n\\n## Building The Google Chrome Extension\xa0\\n\\nThe last piece of the puzzle was to build the chrome extension which consisted of 3 parts:\\n\\n- `manifest.json` containing the configuration of the extension\\n- `popup.html` & style.css defining the user interface\\n- `popup.js` to communicate with the API and implement the actual functionality of the extension.\\n\\n```js title=\\"/extension/popup/popup.js\\"\\n// Issue a POST request with a request body\\nfunction doPost(url, body, callback) {\\n  var xmlHttp = new XMLHttpRequest();\\n  xmlHttp.onreadystatechange = function () {\\n    if (xmlHttp.readyState == 4 && xmlHttp.status == 200)\\n      callback(xmlHttp.responseText);\\n  };\\n  xmlHttp.open(\\"POST\\", url, true);\\n  xmlHttp.setRequestHeader(\\"Content-type\\", \\"application/json; charset=utf-8\\");\\n  xmlHttp.send(body);\\n}\\n```\\n\\nI followed [this](https://medium.com/data-science-at-microsoft/developing-microsoft-edge-extensions-powered-by-sota-nlp-models-f3991f18daa4) fantastic tutorial to build the web extension so please check it out if you\'re looking to do something similar. The only thing I changed myself was changing the output to include more models and rendering the NER results in HTML. This is what the final product looks like for some example text found online!\\n\\n| ![alt](./images/EssayCompanion/demo_pt1.png) | ![alt](./images/EssayCompanion/demo_pt2.png) |\\n| -------------------------------------------- | -------------------------------------------- |\\n| ![alt](./images/EssayCompanion/demo_pt3.png) | ![alt](./images/EssayCompanion/demo_pt4.png) |\\n\\n## Concluding Remarks\xa0\\n\\nIn this blog post, I\'ve shown how you can leverage state-of-the-art transformer models to build \'smart\' text-based applications. I started by finding suitable models for paraphrasing, summarization, name entity recognition, and keyword extraction before optimizing them for model inference on CPU. I then deployed the models at an API endpoint using FastAPI and containerized the application for reproducibility.\\n\\n:::info\\nAll the code for this project can be found in [this GitHub repository](https://github.com/jackmleitch/EssayCompanion).\\n:::\\n\\nFeel free to reach out to me on [LinkedIn](https://www.linkedin.com/in/jackmleitch/)!"},{"id":"Churn-Prediction","metadata":{"permalink":"/blog/Churn-Prediction","source":"@site/blog/2022-01-04-ChurnPrediction.md","title":"Building Interpretable Models on Imbalanced Data","description":"Predicting customer churn from a telecom provider","date":"2022-01-04T00:00:00.000Z","formattedDate":"January 4, 2022","tags":[{"label":"Python","permalink":"/blog/tags/python"},{"label":"Classification","permalink":"/blog/tags/classification"},{"label":"Imbalanced Classification","permalink":"/blog/tags/imbalanced-classification"},{"label":"Scikit-Learn","permalink":"/blog/tags/scikit-learn"},{"label":"Optuna","permalink":"/blog/tags/optuna"},{"label":"MLFlow","permalink":"/blog/tags/ml-flow"}],"readingTime":24.86,"truncated":true,"authors":[{"name":"Jack Leitch","title":"Machine Learning Engineer","url":"https://github.com/jackmleitch","imageURL":"https://github.com/jackmleitch.png","key":"jack"}],"frontMatter":{"slug":"Churn-Prediction","title":"Building Interpretable Models on Imbalanced Data","tags":["Python","Classification","Imbalanced Classification","Scikit-Learn","Optuna","MLFlow"],"authors":"jack"},"prevItem":{"title":"Building NLP Powered Applications with Hugging Face Transformers","permalink":"/blog/Essay-Companion"},"nextItem":{"title":"Predicting Strava Kudos","permalink":"/blog/Strava-Kudos"}},"content":"**Predicting customer churn from a telecom provider**\\n\\nI\u2019ve always believed that to truly learn data science you need to practice data science and I wanted to do this project to practice working with imbalanced classes in classification problems. This was also a perfect opportunity to start working with [mlflow](https://mlflow.org/) to help track my machine learning experiments: it allows me to track the different models I have used, the parameters I\u2019ve trained with, and the metrics I\u2019ve recorded.\\n\\nThis project was aimed at predicting customer churn using the telecommunications data found on [Kaggle](https://www.kaggle.com/c/customer-churn-prediction-2020/overview) (which is a publicly available synthetic dataset). That is, we want to be able to predict if a given customer is going the leave the telecom provider based on the information we have on that customer. Now, why is this useful? Well, if we can predict which customers we think are going to leave **before** they leave then we can try to do something about it! For example, we could target them with specific offers, and maybe we could even use the model to provide us insight into what to offer them because we will know, or at least have an idea, as to why they are leaving.\\n\\n\x3c!--truncate--\x3e\\n\\n## Performance vs Interpretability\\n\\nIt\u2019s very important to know and understand the problem/task at hand before we start to even think about writing any code. Would it be useful in this case to build a really powerful model like XGBOOST? No, of course not. Our goal isn\u2019t to squeeze every drop of performance out of our model. Our goal is to **understand** why people are leaving so we can do something about it and try to get them to stay. In an ideal world, we would build a very interpretable model, but in reality, we may have to find a happy medium between performance and interpretability. As always, logistic regression would be a good start.\\n\\n## Model Evaluation\\n\\nWe now need to decide how we are going to evaluate our models and what we are going to be happy with. I think it\u2019s important to decide an end goal beforehand as otherwise, it\u2019s going to be hard to decide when to stop, squeezing out those extra 1%s is not worth it a lot of the time.\\n\\nDue to the nature of our data, our classes are likely going to be highly imbalanced, with the case we are interested in (customers leaving) being the minority class. This makes selecting the right metric super important.\\n\\n![alt text](./images/ChurnPrediction/imbalanced_meme.jpeg)\\n\\nThe metrics we are going to be interested in are **precision**, **recall**, and other metrics associated with these. Precision is the ratio of correct positive predictions to the overall number of positive predictions. Recall is the ratio of correct positive predictions to the overall number of positive predictions in the dataset. In our case we are looking at trying to retain customers by predicting which customers are going to leave: so we aren\u2019t too fussed if we miss-classify some customers as \u2018churn\u2019 when they are not (false positives). If anything, these miss-classifications might be customers that would soon become \u2018churn\u2019 if nothing changes as they may lie on the edge of the decision boundary. So, we are looking to **maximize recall** as it will minimize the number of false negatives.\\n\\nWe are also going to look at the **F-measure** as it provides a way to express both concerns of precision and recall with a single score \u2014 we don\u2019t just want to forfeit precision to get 100% recall!\\n\\n## Model Specifications\\n\\nOnce we have built our final model, we can then use a precision-recall curve to optimize our performance on the positive (minority class). In this case, we are going to assume that stakeholders in our imaginary telecoms business want to **achieve a recall of 0.80** (i.e. we identify 80% of the positive samples correctly) while maximizing precision.\\n\\n## Data\\n\\n- `train.csv` \u2014 the training set. Contains 4250 rows with 20 columns. 3652 samples (85.93%) belong to class churn=no and 598 samples (14.07%) belong to class churn=yes.\\n\\n- `test.csv` \u2014 the test set. Contains 850 rows with 18 columns.\\n\\n```py title=\\"/src/read_data.py\\"\\nimport pandas as pd\\nfrom src.data.load_data import read_params\\n# load in training data\\nconfig = read_params(\\"params.yaml\\")\\nexternal_data_path = config[\\"external_data_config\\"][\\"external_data_csv\\"]\\ndf = pd.read_csv(external_data_path, sep=\\",\\", encoding=\\"utf-8\\")\\n# check we have our 20 cols\\nassert len(df.columns) == 20\\n```\\n\\nEDA (along with all the modeling etc.) was done in different python scripts and I have chosen to not include it here as it is irrelevant to the topic I am writing about. It is nonetheless very important and you can find this whole project on my [Github page](https://github.com/jackmleitch).\\n\\nDue to the high cardinality of the state columns, we need to be careful when encoding them otherwise we will end up with 50 different features!\\n\\n`df.head()`\\n![alt text](./images/ChurnPrediction/data_head.png)\\n\\nWe can look at a correlation matrix to see any initial promising features.\\n\\n```py title=\\"/src/plot_correlation_matrix.py\\"\\nimport numpy as np\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nsns.set_theme(style=\\"white\\")\\n# Compute the correlation matrix\\ncorr = df.corr()\\n# Generate a mask for the upper triangle\\nmask = np.triu(np.ones_like(corr, dtype=bool))\\n# Set up the matplotlib figure\\nf, ax = plt.subplots(figsize=(11, 9))\\n# Generate a custom diverging colormap\\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\\n# Draw the heatmap with the mask and correct aspect ratio\\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\\n            square=True, linewidths=.5, cbar_kws={\\"shrink\\": .5})\\n```\\n\\n![alt text](./images/ChurnPrediction/corr_matrix.png)\\n\\nOk, this doesn\u2019t look great but we can see that churn is somewhat correlated with total_day_minutes, total_day_charge, and number_customer_service_calls. Let\u2019s build a simple **baseline model** with total_day_minutes and number_customer_service_calls (we omit total_day_charge because it\u2019s strongly correlated with total_day_minutes).\\n\\n## Feature Engineering\\n\\nWe can generate a few features to encapsulate daily totals. We also map the state feature to the regions the state belongs to as it massively reduces the feature dimensionality. Finally, we can map churn target value to binary as this is required for a lot of models.\\n\\n```py title=\\"/src/preprocess_data.py\\"\\ndef preprocess(df):\\n    # add new features\\n    df[\'total_minutes\'] = df[\'total_day_minutes\'] + df[\'total_eve_minutes\'] + df[\'total_night_minutes\']\\n    df[\'total_calls\'] = df[\'total_day_calls\'] + df[\'total_eve_calls\'] + df[\'total_night_calls\']\\n    df[\'total_charge\'] = df[\'total_day_charge\'] + df[\'total_eve_charge\'] + df[\'total_night_charge\']\\n    # target mapping\\n    target_mapping = {\\"no\\": 0, \\"yes\\": 1}\\n    df.loc[:, \'churn\'] = df[\'churn\'].map(target_mapping)\\n    # map state\\n    state_mapping = {\\n        \'AK\': \'O\', \'AL\': \'S\', \'AR\': \'S\', \'AS\': \'O\', \'AZ\': \'W\', \'CA\': \'W\', \'CO\': \'W\', \'CT\': \'N\', \'DC\': \'N\', \'DE\': \'N\', \'FL\': \'S\', \'GA\': \'S\',\\n        \'GU\': \'O\', \'HI\': \'O\', \'IA\': \'M\', \'ID\': \'W\', \'IL\': \'M\', \'IN\': \'M\', \'KS\': \'M\', \'KY\': \'S\', \'LA\': \'S\', \'MA\': \'N\', \'MD\': \'N\', \'ME\': \'N\',\\n        \'MI\': \'W\', \'MN\': \'M\', \'MO\': \'M\', \'MP\': \'O\', \'MS\': \'S\', \'MT\': \'W\', \'NA\': \'O\',  \'NC\': \'S\', \'ND\': \'M\', \'NE\': \'W\', \'NH\': \'N\', \'NJ\': \'N\',\\n        \'NM\': \'W\', \'NV\': \'W\', \'NY\': \'N\', \'OH\': \'M\', \'OK\': \'S\', \'OR\': \'W\', \'PA\': \'N\', \'PR\': \'O\', \'RI\': \'N\', \'SC\': \'S\', \'SD\': \'M\', \'TN\': \'S\',\\n        \'TX\': \'S\', \'UT\': \'W\', \'VA\': \'S\', \'VI\': \'O\', \'VT\': \'N\', \'WA\': \'W\', \'WI\': \'M\', \'WV\': \'S\', \'WY\': \'W\'\\n    }\\n    df.loc[:, \'state\'] = df[\'state\'].map(state_mapping)\\n    return df\\n# preprocess dataframe and add features\\ndf = preprocess(df)\\ndf.head()\\n```\\n\\n## Modeling\\n\\nThe first thing that always needs to be done is to split the data into train and validation sets as it is a vital step to avoid overfitting, improve generalizability, and help us compare potential models. In this case, we use stratified K-fold cross-validation as our dataset is highly imbalanced and we want to ensure the class distribution is consistent across folds.\\n\\n```py title=\\"/src/kfold.py\\"\\nfrom sklearn.model_selection import StratifiedKFold\\n\\ndef stratKFold(df, n_splits=5):\\n    \\"\\"\\"\\n    Perform stratified K fold cross validation on training set\\n    :param df: pd dataframe to split\\n    :param n_splits: number of folds\\n    :return: df with kfold column\\n    \\"\\"\\"\\n    # create new column \'kfold\' with val -1\\n    df[\\"kfold\\"] = -1\\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\\n    # target values\\n    y = df[\'churn\'].values\\n    # initialise kfold class\\n    kf = StratifiedKFold(n_splits=n_splits)\\n    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\\n        df.loc[v_, \\"kfold\\"] = f\\n    return df\\n\\ndf = stratKFold(df)\\n```\\n\\n### Baseline Model\\n\\nWe start by building a simple baseline model so that we have something to compare our later models to. In a regression scenario we could simply use the average of the target variable at every prediction, in our classification case however we are going to use a logistic regression model trained on our two most correlated features.\\n\\nBefore we start let\'s initialize Mlflow and write a general scoring function to evaluate our model.\\n\\n```py title=\\"/src/model_scoring.py\\"\\nimport mlflow\\nfrom sklearn.metrics import f1_score, recall_score, precision_score\\n\\n# initialize mlflow\\nmlflow.set_experiment(\\"mlflow/customer_churn_model\\")\\n\\n# scoring function\\ndef score(y, preds):\\n    \\"\\"\\"\\n    Returns corresponding metric scores\\n    :param y: true y values\\n    :param preds: predicted y values\\n    :return: f1_score, recall, and precision scores\\n    \\"\\"\\"\\n    f1 = f1_score(y, preds)\\n    recall = recall_score(y, preds)\\n    precision = precision_score(y, preds)\\n    return [f1, recall, precision]\\n```\\n\\nNow let\u2019s build our baseline model and see how it does on each validation set!\\n\\n```py title=\\"/src/evaluate_baseline.py\\"\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\n# baseline model\\nf1_scores, recall_scores, precision_scores = [], [], []\\nfor fold in range(5):\\n    # define train and validation set\\n    features = [\\"total_day_minutes\\", \\"number_customer_service_calls\\"]\\n    df_train = df[df.kfold != fold].reset_index(drop=True)\\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\\n    # target and features\\n    y_train = df_train[\'churn\'].values\\n    y_valid = df_valid[\'churn\'].values\\n    # init and fit scaler\\n    scaler = StandardScaler()\\n    x_train = scaler.fit_transform(df_train[features])\\n    x_valid = scaler.transform(df_valid[features])\\n    # create and train model\\n    clf = LogisticRegression()\\n    clf.fit(x_train, y_train)\\n    preds = clf.predict(x_valid)\\n    # score model\\n    scores = score(y_valid, preds)\\n    f1_scores.append(scores[0])\\n    recall_scores.append(scores[1])\\n    precision_scores.append(scores[2])\\n# average scores over each fold\\nf1_avg = np.average(f1_scores)\\nrecall_avg = np.average(recall_scores)\\nprecision_avg = np.average(precision_scores)\\nprint(f\\"Average F1 = {f1_avg}, Recall = {recall_avg}, Precision = {precision_avg}\\")\\n\\n# log metrics on mlflow\\nwith mlflow.start_run(run_name=\\"lr_baseline\\") as mlops_run:\\n        mlflow.log_metric(\\"F1\\", f1_avg)\\n        mlflow.log_metric(\\"Recall\\", recall_avg)\\n        mlflow.log_metric(\\"Preision\\", precision_avg)\\n```\\n\\n:::info\\nAverage F1 = 0.08725, Recall = 0.04847, Precision = 0.44093\\n:::\\n\\nSo yea, the results aren\u2019t great (actually they are terrible) but that only means we are going to get better. This was only a baseline! We can try a few things to improve our model:\\n\\n- We can balance out our classes by over and under-sampling as the imbalance is causing bias towards the majority class in our model.\\n\\n- We can train on more features.\\n\\n### SMOTE: To Over-Sample the Minority Class\\n\\nOne problem we have with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary. One way to solve this problem would be to over-sample the examples in the minority class. This could be achieved by simply duplicating examples from the minority class in the training dataset, although this does not provide any additional information to the model. An improvement in duplicating examples from the minority class is to synthesize new examples from the minority class. A common technique for this, introduced in [this paper](https://arxiv.org/abs/1106.1813), is SMOTE. It\u2019s worth noting that oversampling isn\u2019t our only option, we could for example under-sample (or combine a mix of the two) by using a technique such as [TOMEK-links](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis#Tomek_links) (SMOTE-TOMEK helps do both under and over-sampling in one go). In our case, however, the best performance boost came from SMOTE alone.\\n\\nBefore we do this, however, let\'s write a general feature processing pipeline to get our data ready for modeling. Our function returns a Sklearn pipeline object that we can use to fit and transform our data. It first splits the data into numeric, categorical features, and binary features as we process each of these differently. The categorical features are encoded using one-hot encoding, while the binary features are left alone. Finally, the numeric features have their missing values imputed. Scaling the numeric features was also tried but it didn\u2019t lead to a performance increase. It\u2019s also in our best interest to not scale these as it makes interpreting the results harder.\\n\\n```py title=\\"/src/feature_pipeline.py\\"\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline, FeatureUnion\\nfrom mlxtend.feature_selection import ColumnSelector\\nfrom category_encoders import HashingEncoder\\n\\ndef feature_pipeline(config_path=\\"params.yaml\\"):\\n    \\"\\"\\"\\n    :param config_path: path to params.yaml file\\n    :return: preprocessing feature pipeline\\n    \\"\\"\\"\\n    # load in config information\\n    config = read_params(config_path)\\n    num_features = config[\\"raw_data_config\\"][\\"model_features\\"][\\"numeric\\"]\\n    cat_features = config[\\"raw_data_config\\"][\\"model_features\\"][\\"categorical\\"]\\n    binary_features = config[\\"raw_data_config\\"][\\"model_features\\"][\\"binary\\"]\\n\\n    # transformers\\n    transforms = []\\n    # categorical pipeline\\n    transforms.append(\\n        (\\n            \\"categorical\\",\\n            Pipeline(\\n                [\\n                    (\\"select\\", ColumnSelector(cols=cat_features)),\\n                    (\\"encode\\", OneHotEncoder()),\\n                ]\\n            ),\\n        )\\n    )\\n    transforms.append(\\n        (\\n            \\"binary\\",\\n            Pipeline(\\n                [\\n                    (\\"select\\", ColumnSelector(cols=binary_features)),\\n                    (\\"encode\\", OrdinalEncoder()),\\n                ]\\n            ),\\n        )\\n    )\\n    # numeric pipeline\\n    transforms.append(\\n        (\\n            \\"numeric\\",\\n            Pipeline(\\n                [\\n                    (\\"select\\", ColumnSelector(cols=num_features)),\\n                    (\\"impute\\", SimpleImputer(missing_values=np.nan, strategy=\\"median\\")),\\n                ]\\n            ),\\n        )\\n    )\\n\\n    # combine features\\n    features = FeatureUnion(transforms)\\n    return features\\n```\\n\\nA general training function is written below, notice we can choose whether we want to use SMOTE or not. The sampling strategy in SMOTE controls how much we resample the minority class and it\u2019s something we can tune later.\\n\\n```py title=\\"/src/train.py\\"\\nfrom imblearn.over_sampling import SMOTE, SMOTENC\\ndef train(fold, df, model=LogisticRegression(solver=\'newton-cg\'), smote=False):\\n    \\"\\"\\"\\n    :param fold: fold to train model on\\n    :param df: pandas dataframe containing our data\\n    :param model: model to train data on\\n    :param smote: float, if named it is the sampling strategy for SMOTE\\n    :return: f1, recall, precision validation score for fold, as well as y_valid and preds\\n    \\"\\"\\"\\n    # feature pipeline\\n    features = feature_pipeline()\\n\\n    # define train and validation set\\n    df_train = df[df.kfold != fold].reset_index(drop=True)\\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\\n    # target and features\\n    y_train = df_train[\'churn\'].values\\n    y_valid = df_valid[\'churn\'].values\\n\\n    # create training and validation features\\n    x_train = features.fit_transform(df_train)\\n    x_valid = features.transform(df_valid)\\n\\n    # smote\\n    if smote:\\n        smt = SMOTE(random_state=42, sampling_strategy=smote)\\n        x_train, y_train = smt.fit_resample(x_train, y_train)\\n\\n    # create and train model\\n    clf = model\\n    clf.fit(x_train, y_train)\\n    preds = clf.predict(x_valid)\\n    # score model\\n    scores = score(y_valid, preds)\\n    return scores, [y_valid, preds]\\n```\\n\\nBefore we use SMOTE, let\u2019s train a logistic regression model on all of the features to try and get a new baseline.\\n\\n```py title=\\"/src/logistic_regression.py\\"\\nf1_scores, recall_scores, precision_scores = [], [], []\\nfor fold in range(5):\\n    scores, _ = train(fold,df, smote=False)\\n    f1, recall, precision = scores\\n    f1_scores.append(f1)\\n    recall_scores.append(recall)\\n    precision_scores.append(precision)\\n# average scores over each fold\\nf1_avg = np.average(f1_scores)\\nrecall_avg = np.average(recall_scores)\\nprecision_avg = np.average(precision_scores)\\nprint(f\\"Average F1 = {f1_avg}, Recall = {recall_avg}, Precision = {precision_avg}\\")\\n# log metrics on mlflow\\nwith mlflow.start_run(run_name=\\"lr_all_features\\") as mlops_run:\\n        mlflow.log_metric(\\"F1\\", f1_avg)\\n        mlflow.log_metric(\\"Recall\\", recall_avg)\\n        mlflow.log_metric(\\"Preision\\", precision_avg)\\n```\\n\\n:::info\\nAverage F1 = 0.30667, Recall = 0.21075, Precision = 0.57206\\n:::\\n\\nThe results are definitely better than before but still not great. We\u2019ve waited long enough, let\u2019s try using SMOTE! We are going to use SMOTE to over-sample our churn data points so that we end up with equal class distributions.\\n\\n```py title=\\"/src/train_and_eval.py\\"\\ndef train_and_eval(df, model=LogisticRegression(solver=\'newton-cg\'), smote=0.75, model_name=\\"\\", params = {}, log_mlflow=True):\\n    \'\'\'\\n    train model and evaluate it on each fold\\n    :param df: pandas dataframe containing our data\\n    :param model: model to train data on\\n    :param model_name: string, for tracking on mlflow\\n    :param params: dict, for tracking on mlflow\\n    :param log_mlflow: boolean, if true then log results using mlflow\\n    :return: average score for each metric\\n    \'\'\'\\n    f1_scores, recall_scores, precision_scores = [], [], []\\n    for fold in range(5):\\n        scores, _ = train(fold, df, model=model, smote=smote)\\n        f1, recall, precision = scores\\n        f1_scores.append(f1)\\n        recall_scores.append(recall)\\n        precision_scores.append(precision)\\n    # average scores over each fold\\n    f1_avg = np.average(f1_scores)\\n    recall_avg = np.average(recall_scores)\\n    precision_avg = np.average(precision_scores)\\n    print(f\\"Average F1 = {f1_avg}, Recall = {recall_avg}, Precision = {precision_avg}\\")\\n    # log metrics on mlflow\\n    if log_mlflow:\\n        with mlflow.start_run(run_name=model_name) as mlops_run:\\n                mlflow.log_metric(\\"F1\\", f1_avg)\\n                mlflow.log_metric(\\"Recall\\", recall_avg)\\n                mlflow.log_metric(\\"Preision\\", precision_avg)\\n                if params:\\n                    mlflow.log_params(params)\\n    return f1_avg\\n```\\n\\nAn important side note: when using SMOTE we need to evaluate performance on a validation set that has **not** been over-sampled. Otherwise, we will not be getting a true performance measure.\\n\\n`train_and_eval(df, model_name=\\"lr_all_features_smote\\")`\\n:::info\\nAverage F1 = 0.49789, Recall = 0.69068, Precision = 0.38947\\n:::\\n\\nWow! We have boosted the F1 score from 0.31 to 0.50, and the recall has gone from 0.21 to 0.69! An important note is that the precision has practically stayed the same. Why is that? Well, what is precision measuring? Mathematically, precision is the number of true positives divided by the number of true positives plus the number of false positives. It tells us that our model is correct 47% of the time when trying to predict positive samples. So by over-sampling, we have decreased the number of false negatives but we have also increased the number of false positives. This is OK as we decided we will favor false positives over false negatives. An intuitive way to see this change is by looking at a **confusion matrix**.\\n\\n```py title=\\"/src/confusion_matrix.py\\"\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\\n# get preds for non-smote and smote models\\n_, evals = train(0, df, smote=False)\\n_, evals_smote = train(0, df, smote=True)\\n# set axis and plot\\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,10))\\nax1.set_title(\\"Model without SMOTE\\")\\nConfusionMatrixDisplay.from_predictions(*evals, ax=ax1)\\nax2.set_title(\\"Model with SMOTE\\")\\nConfusionMatrixDisplay.from_predictions(*evals_smote, ax=ax2)\\nplt.tight_layout()\\nplt.show()\\n```\\n\\n![confusion_matrix](./images/ChurnPrediction/confusion_matix.png)\\n\\nWe can see that the TP number goes from 33 to 90 and the FN number goes from 86 to 29, great! However, as a consequence of this, we see the FP number goes from 21 to 158. But, as mentioned earlier, we are ok with that as we care more about finding out which customers are going to leave. A little side note: the FP and FN rates can be tuned using the probability threshold and the easiest way to compare the two models is to compare F1 scores.\\n\\n### Feature Selection\\n\\nWe can train a more complicated model and then use this to select features. Specifically, we train a random forest classifier and then use SHAP values to select the most promising features. Narrowing down the feature space helps reduce dimensionality and generalizability while also making interpreting results easier. It\u2019s a win-win!\\n\\n```py title=\\"/src/feature_selection.py\\"\\nfrom sklearn.ensemble import RandomForestClassifier\\nfeatures = feature_pipeline()\\nX = np.asarray(features.fit_transform(df).todense())\\ny = df[\'churn\'].values\\n\\nclf = RandomForestClassifier()\\nmodel = clf.fit(X,y)\\n\\nimport shap\\nexplainer = shap.TreeExplainer(model)\\nshap_values = explainer.shap_values(X)\\nshap.summary_plot(shap_values, features=X, feature_names=feature_names, plot_type=\'bar)\\n```\\n\\n![imp](./images/ChurnPrediction/feature_importance.png)\\n\\nWe can now define a new pipeline to preprocess and only select our needed features. With this, let\u2019s see how our logistic regression model does with our narrowed-down features!\\n\\n`train_and_eval(df, model_name=\\"lr_selected_features_smote\\")`\\n:::info\\nAverage F1 = 0.50255, Recall = 0.69403, Precision = 0.39407\\n:::\\n\\nAwesome, we\u2019ve removed features and our performance increases slightly. This confirms to us that the other features weren\u2019t important.\\n\\n### But can we do better than our logistic regression?\\n\\nIt would be easy here to go all guns blazing and train an XGBOOST model, but remember that is **not** our goal. Our goal is to build an interpretable model that we can use to try and keep customers from leaving. As well as logistic regression, decision tree classifiers are very interpretable. Let\u2019s see how it gets on.\\n\\n```py title=\\"/src/random_forest.py\\"\\nfrom sklearn.tree import DecisionTreeClassifier\\ntrain_and_eval(df, model=DecisionTreeClassifier(), model_name=\\"dt_selected_features_smote\\")\\n```\\n\\n:::info\\nAverage F1 = 0.81529, Recall = 0.84614, Precision = 0.78694\\n:::\\n\\nIt does well! We need to be **very** careful though as decision trees overfit like there is no tomorrow. Let\u2019s go ahead and tune hyperparameters on both models to see if we can optimize things a little more. We will use [Optuna](https://optuna.org/) as I just love how easy and fast it is.\\n\\n### Hyperparameter Tuning\\n\\nWe need a be super careful here, decision trees are very prone to overfitting and this is why random forest models are usually preferred. The random forest can generalize over the data in a better way as the randomized feature selection acts as a form of regularization. As discussed earlier though, in our case we care more about interpretability than performance. Now, although cross-validation is great for seeing how the model is generalizing, it doesn\u2019t necessarily prevent overfitting as we will just end up overfitting the validation sets.\\n\\nOne measure of overfitting is when the training score is much higher than the testing score. I initially tried setting the objective function in the Optuna trial to the cross-validated validation scores but this still lead to overfitting as DTs don\u2019t have much regularization.\\n\\nAnother possibility, that is this case worked superbly, is weighting the difference between cross-validated training scores and validation scores vs the validation score itself. For example, for F1 scores, a possible objective function is\\n\\n![alt](./images/ChurnPrediction/objective.png)\\n\\nIn this case, the RMSE of the difference between validation and training is weighted four times less than the validation F1 score. Optimizing this function forces the train and valid score to stay close, while also maximizing the validation score.\\n\\n```py title=\\"/src/optuna.py\\"\\nfrom optuna import Trial, create_study\\nfrom optuna.samplers import TPESampler\\nfrom scipy.stats import loguniform\\n\\ndef objective(trial, n_jobs=-1, random_state=42):\\n    \'\'\'\\n    Objective function to optimize our custom metric using Optuna\'s TPE sampler\\n    \'\'\'\\n    # smote param space\\n    smote_space = {\'sampling_strategy\': trial.suggest_uniform(\'sampling_strategy\', 0.5, 1)}\\n    # define search spaces\\n    if model == LogisticRegression:\\n        params = {\\n            \'solver\': trial.suggest_categorical(\'solver\', [\'liblinear\', \'saga\']),\\n            \'penalty\': trial.suggest_categorical(\'penalty\', [\'l1\', \'l2\']),\\n            \'C\': trial.suggest_float(\\"C\\", 1.0, 10.0, log=True),\\n            \'tol\': trial.suggest_float(\\"tol\\", 0.0001, 0.01, log=True),\\n            \'max_iter\': trial.suggest_int(\'max_iter\', 100, 1000)\\n\\n        }\\n    else:\\n        params = {\\n            \'max_depth\': trial.suggest_int(\'max_depth\',2,10),\\n            \'min_samples_leaf\': trial.suggest_int(\'min_samples_leaf\',1,30),\\n            \'min_samples_split\': trial.suggest_int(\'min_samples_split\',2,10),\\n            \'criterion\': trial.suggest_categorical(\'criterion\', [\\"gini\\", \\"entropy\\"])\\n        }\\n\\n    # feature pipeline\\n    features = feature_pipeline()\\n    # create training and validation features\\n    X = features.fit_transform(df)\\n    y = df[\'churn\'].values\\n\\n    train_f1, valid_f1 = [], []\\n    # Create StratifiedKFold object.\\n    strat = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\\n    for train_index, test_index in strat.split(X, y):\\n        # split data\\n        x_train, x_valid = X[train_index], X[test_index]\\n        y_train, y_valid = y[train_index], y[test_index]\\n        # feature transformations and smote\\n        smt = SMOTE(random_state=42, **smote_space)\\n        x_train_smt, y_train_smt = smt.fit_resample(x_train, y_train)\\n        # train model\\n        clf = model(**params)\\n        clf.fit(x_train_smt, y_train_smt)\\n        # compute f1 score on valid and training data (without SMOTE!)\\n        preds_train = clf.predict(x_train)\\n        preds_valid = clf.predict(x_valid)\\n        train_f1.append(f1_score(y_train, preds_train))\\n        valid_f1.append(f1_score(y_valid, preds_valid))\\n    # compute mean of f1 train/valid scores\\n    train_f1_mean, valid_f1_mean = np.array(train_f1).mean(), np.array(valid_f1).mean()\\n    # train/test cross score\\n    return train_valid_f1_score(train_f1_mean, valid_f1_mean)\\n```\\n\\nThis gives us the following results:\\n\\n- Logistic Regression Best Hyperparameters: {\u2018solver\u2019 : \u2018liblinear\u2019, \u2018penalty\u2019 : \u2018l2\u2019, \u2018C\u2019 : 1.14, \u2018tol\u2019 : 0.0002, max_iter : 150}\\n- Decision Tree Best Hyperparameters: {\u2018max_depth\u2019: 7, \u2018min_samples_leaf\u2019: 4, \u2018min_samples_split\u2019: 10, \u2018criterion\u2019: \u2018gini\u2019},\\n- SMOTE Sampling Strategy 0.70 for LR and 0.56 for DT\\n\\nLet\u2019s train some models with these parameters and see what we get!\\n\\n:::info\\nLOGISTIC REGRESSION:\\nAverage training F1 score: 0.50095, Average validation F1 score: 0.50340, Overfit score: 1.98883\\n\\nDECISION TREE\\nAverage training F1 score: 0.91534, Average validation F1 score: 0.89280, Overfit score: 0.45129\\n:::\\n\\nAlthough this model looks great and doesn\u2019t appear to be overfitting we are going to go with the model below that has been tuned with a lower maximum depth. Our goal is interpretability and a depth of 7 doesn\u2019t really give us that. So we are sacrificing a little bit of accuracy for interpretability.\\n\\n:::info\\nAverage training F1 score: 0.82174, Average validation F1 score: 0.81610, Overfit score: 0.74122\\n:::\\n\\nGreat, so we now have a few potential models. We are going to move forward with the decision tree model as the logistic regression model isn\u2019t quite up to scratch.\\n\\n### Pruning the Decision Tree\\n\\nAlthough our model doesn\u2019t appear to be overfitting, we are still going to prune the decision tree as it helps us get rid of sub-nodes that don\u2019t have much predictive power. We do this with the hope that this helps our model generalize better. An added bonus, that tends to come with most regularization, is that it also helps improve the interpretability of the model.\\n\\nWe can prune our tree by picking the right cost complexity parameter. We will start by training a model on the whole dataset, with our chosen hyperparams, to find our space of **\u03b1**\u2019s \u2014 the cost complexity parameter. We can then score each of these **\u03b1**\u2019s in a cross-validated way to find the best complexity to choose.\\n\\nWe found that the value **\u03b1** = 0.00811411 is the best complexity to choose. In general, as **\u03b1** increases the number of nodes and depth decreases. So we pick the highest \u03b1 value that still has a good average F1 score.\\n\\nWe can now train our final model!\\n\\n### How Does It Perform on Test Data\\n\\n```py title=\\"/src/predict.py\\"\\ndef predict(X, clf, feature_pipeline, thresh=0.5):\\n    \'\'\'\\n    Predict customer churn on new data\\n    :param X: data containing features\\n    :param clf: trained model\\n    :param feature_pipeline: trained feature processing pipeline\\n    :param thresh: prediction threshold\\n    :return: predictions\\n    \'\'\'\\n    X = feature_pipeline.transform(X)\\n    preds = (clf.predict_proba(X)[:,1] >= thresh).astype(int)\\n    return preds\\n\\ndf_test = pd.read_csv(\\"data/external/test.csv\\")\\ndf_test = preprocess(df_test, target=\'churn\')\\ny_test = df_test[\'churn\'].values\\npreds = predict(df_test, clf, features)\\nf1, recall, precision = score(y_test, preds)\\nprint(f\\"Average F1 = {f1}, Recall = {recall}, Precision = {precision}\\")\\n```\\n\\n:::info\\nAverage F1 = 0.77729, Recall = 0.65925, Precision = 0.94680\\n:::\\n\\nThe model does well on the test data! We can see that the precision is a lot higher than the recall however but this is something that can be tuned by changing the prediction probability threshold. In our case, we are trying to get to 80% recall while maximizing precision.\\n\\n### Picking the Optimal Probability Threshold\\n\\nWe can now tune the probability threshold to try and optimize our precision-recall trade-off. Let\u2019s plot a precision-recall curve and find the optimal threshold to achieve 80% recall.\\n\\n```py title=\\"/src/precision_recall.py\\"\\nfrom sklearn.metrics import PrecisionRecallDisplay\\n# training data\\nx_train = features.transform(df)\\ny_train = df[\'churn\'].values\\n# precision recall curve\\ndisplay = PrecisionRecallDisplay.from_estimator(\\n    clf, x_train, y_train, name=\\"Decision tree classifier\\"\\n)\\n_ = display.ax_.set_title(\\"Precision-Recall curve\\")\\n```\\n\\n![recall](./images/ChurnPrediction/precision_recall.png)\\n\\n```py title=\\"/src/optimize_threshold.py\\"\\nfrom sklearn.metrics import precision_recall_curve\\ndef optimize_threshold(clf, df, recall = 0.80):\\n    \'\'\'\\n    Optimize prob. threshold on training dataset\\n    :param df: pandas dataframe\\n    :param recall: desired recall\\n    :return: optimal prob. threshold\\n    \'\'\'\\n    # create features and target labels\\n    X = features.transform(df)\\n    y = df[\'churn\'].values\\n\\n    # get scores for valid data\\n    y_scores = clf.predict_proba(X)[:, 1]\\n    # locate where recall is closest to 0.80\\n    precisions, recalls, thresholds = precision_recall_curve(y, y_scores)\\n    distance_to_optim = abs(recalls - recall)\\n    optimal_idx = np.argmin(distance_to_optim)\\n    thresh = thresholds[optimal_idx]\\n    return thresh\\n\\nthresh = optimize_threshold(clf, df)\\nprint(thresh)\\n```\\n\\nWe get a threshold of 0.426. If we predict on our test set again then hopefully we will see something closer to our desired recall! And yes, I know I\u2019ve committed the cardinal sin of using the test set twice but this was for demonstration purposes. The test set is normally a \u2018one and done\u2019 situation. Our final results are:\\n\\n:::info\\nAverage F1 = 0.79584, Recall = 0.85185, Precision = 0.74675\\n:::\\n\\nAwesome! We have done what we wanted to do, and things work well! It\u2019s also great to see test scores so similar to our earlier scores as it shows we haven\u2019t overfitted our model. Let\u2019s log this model on MLFLOW.\\n\\n```\\n# log metrics on mlflow\\nwith mlflow.start_run(run_name=\\"Final Decision Tree\\") as mlops_run:\\n        mlflow.log_metric(\\"F1\\", f1)\\n        mlflow.log_metric(\\"Recall\\", recall)\\n        mlflow.log_metric(\\"Preision\\", precision)\\n        mlflow.log_params(params)\\n        mlflow.log_params({\\"prob_thresh\\": 0.426})\\n        # log model and pipeline\\n        mlflow.sklearn.log_model(clf, \\"clf\\")\\n        mlflow.sklearn.log_model(features, \\"features_pipeline\\")\\n```\\n\\n## MLFLOW Experiment Session\\n\\n![mlflow](./images/ChurnPrediction/mlflow.png)\\n\\n## Model Interpreting\\n\\nTree-based models split the data multiple times according to certain cutoff values in the features. By splitting, different subsets of the dataset are created, where each instance belongs to a certain subset. To predict the outcome in each leaf node, the average outcome of the training data in this node is used.\\n\\nThe interpretation for decision trees is very easy: Starting from the root node, you go to the next nodes and the edges tell you which subsets you are looking at. Once you reach the leaf node, the node tells you the predicted outcome. All the edges are connected by \u2018AND\u2019.\\n\\nSo the general way we can predict is: If feature x is **smaller** (/bigger) than threshold c **AND** \u2026 then the predicted outcome is the most common value of the target of the instances in that node.\\nLet\u2019s plot out the tree and see what we can infer!\\n\\n```py title=\\"/src/plot_model.py\\"\\n# get feature names\\nnumeric_features = [\\n    \'total_charge\', \'number_customer_service_calls\', \'total_day_minutes\', \'total_day_charge\',\\n    \'total_minutes\', \'total_intl_calls\', \'total_intl_minutes\', \'number_vmail_messages\', \'total_intl_charge\'\\n]\\nbinary_features = [\'international_plan\', \'voice_mail_plan\']\\nfeature_names = np.append(numeric_features, binary_features)\\n\\n# plot tree from our model\\nimport graphviz\\nfrom sklearn import tree\\ntree_graph = tree.export_graphviz(clf, out_file=None,\\n                              feature_names=feature_names,\\n                              class_names=[\'No churn\', \'Churn\'],\\n                              filled=True, rounded=True,\\n                              special_characters=True)\\n\\ngraph = graphviz.Source(tree_graph)\\ngraph\\n```\\n\\n![model](./images/ChurnPrediction/model.png)\\n\\nEach node in the tree will give a condition and the left node below is True and the right node is False. The first split was performed with the total day minutes feature, which counts the total minutes of all calls made in the day. We can see for example that if the total minutes are less than 71 we follow the tree left and if the minutes are greater than 71 we go right.\\n\\nEach prediction from the tree is made by following the tree down until a root node is hit. For example, if a customer has less than 71 total day minutes and their total charge is between 0.04 and 1 then we would predict them to churn.\\n\\nWe can see that the charge from the telecom provider seems to be a big distinguishing factor between customers and this is confirmed by the SHAP feature importance plot earlier. By following the tree left we can see that customers with a high day charge but low day minutes tend to churn more than stay. If the day charge is less than 3 however, the customers tend to stay no matter what the minutes are! One possible explanation for this could be that the customers churning are on mobile plans that don\u2019t correctly suit their needs, this would need to be investigated further though.\\n\\nAnother interesting observation is that if a customer has a high total day minute (>71) and they do not speak to customer service (<0.965 calls i.e. no calls) they are more likely to churn than customers that do speak to customers service. Again, this would need further investigation to draw conclusions as to why this is true.\\nAs with most data problems, it quite often leads to more questions to be answered!\\n\\n## Conclusion\\n\\nWe have built an interpretable machine learning model that can identify customers that are likely to churn with our desired recall of 80% (we actually achieved 85% on the test set) and a precision of 75%. That is we identify 85% of the churned customers correctly and 75% of our churn predictions are accurate. Using this model we can then understand the key factors driving customers to leave and hopefully we can use this to try and keep more customers in the long run.\\n\\nThanks for reading and I hope you enjoyed it. If you have any comments or questions please feel free to reach out."},{"id":"Strava-Kudos","metadata":{"permalink":"/blog/Strava-Kudos","source":"@site/blog/2021-10-25-StravaKudos.md","title":"Predicting Strava Kudos","description":"An end-to-end data science project, from data collection to model deployment, aimed at predicting user interaction on Strava activities based on the given activity\u2019s attributes.","date":"2021-10-25T00:00:00.000Z","formattedDate":"October 25, 2021","tags":[{"label":"Python","permalink":"/blog/tags/python"},{"label":"Regression","permalink":"/blog/tags/regression"},{"label":"Scikit-Learn","permalink":"/blog/tags/scikit-learn"},{"label":"Optuna","permalink":"/blog/tags/optuna"},{"label":"XGBoost","permalink":"/blog/tags/xg-boost"}],"readingTime":20.325,"truncated":true,"authors":[{"name":"Jack Leitch","title":"Machine Learning Engineer","url":"https://github.com/jackmleitch","imageURL":"https://github.com/jackmleitch.png","key":"jack"}],"frontMatter":{"slug":"Strava-Kudos","title":"Predicting Strava Kudos","tags":["Python","Regression","Scikit-Learn","Optuna","XGBoost"],"authors":"jack"},"prevItem":{"title":"Building Interpretable Models on Imbalanced Data","permalink":"/blog/Churn-Prediction"},"nextItem":{"title":"Building a Recipe Recommendation System","permalink":"/blog/Recipe-Recommendation"}},"content":"**An end-to-end data science project, from data collection to model deployment, aimed at predicting user interaction on Strava activities based on the given activity\u2019s attributes.**\\n\\nStrava is a service for tracking human exercise which incorporates social network type features. It is mostly used for cycling and running, with an emphasis on using GPS data. A typical Strava post from myself is shown below and we can see that it contains a lot of information: distance, moving time, pace, elevation, weather, GPS route, who I ran with, etc., etc.\\n\\n\x3c!--truncate--\x3e\\n\\n![alt](./images/StravaPrediction/strava1.png)\\n\\nNow, Strava is also a social media and each activity can be given kudos, which is the equivalent of likes on other platforms. So given that I\u2019ve posted over 4000 activities on Strava, it was a great place to start a unique little data science project. My goal was to predict the number of kudos each activity gets based on the activity\'s attributes.\\n\\nPredicting Kudos wasn\'t necessarily an easy task, however. Of the data points extracted from Strava, the mean Kudos count was 38 with a standard deviation of 24. The minimum was 0 and the maximum was 171. There is a lot of variability in the data\u2026 so hopefully we can create some useful features to help explain this!\\n\\n![kudos](./images/StravaPrediction/kudos_dist.png)\\n\\nIf you want to fast forward to the end project, you can find the finished product [here](http://strava-kudos.herokuapp.com/).\\n\\n## Data Collection\\n\\nStrava has a feature that allows the user to download all of their previous data, however, this can only be done once a week. A much better solution for data collection was to use the Strava API which allows you to make 100 requests every 15 minutes, and a total of 1000 daily i.e. more than enough calls for our intended goal.\\n\\nUsing this [API](https://developers.strava.com/docs/reference/), I was able to efficiently gather all my activity data very quickly. Lovely. Another benefit of this API was that it allowed me to extract my most recent activities so that I could predict kudos on the fly, which was one of the end goals for this project. A GIF of this real-time Kudos prediction on my Streamlit app can be seen below.\\n\\n![gif](./images/StravaPrediction/streamlit.gif)\\n\\nIf you\'re interested in getting started with the Strava API, I would highly recommend that you start with [this blog post](https://medium.com/r?url=https%3A%2F%2Ftowardsdatascience.com%2Fusing-the-strava-api-and-pandas-to-explore-your-activity-data-d94901d9bfde).\\n\\nOnce I had set things up and had the required keys, getting recent activities was easy!\\n\\n```py title=\\"/src/strava_api.py\\"\\ndef load_recent_data(recent=10):\\n    # get payload information\\n    auth_url = \\"https://www.strava.com/oauth/token\\"\\n    activites_url = \\"https://www.strava.com/api/v3/athlete/activities\\"\\n    payload = {\\n        \\"client_id\\": \\"XXXX\\",\\n        \\"client_secret\\": \\"XXXX\\",\\n        \\"refresh_token\\": \\"XXXX\\",\\n        \\"grant_type\\": \\"refresh_token\\",\\n        \\"f\\": \\"json\\",\\n    }\\n    # check if there is a new request token\\n    res = requests.post(auth_url, data=payload, verify=False)\\n    access_token = res.json()[\\"access_token\\"]\\n    header = {\\"Authorization\\": \\"Bearer \\" + access_token}\\n\\n    # initialise dataframe with first few activities\\n    param = {\\"per_page\\": recent, \\"page\\": 1}\\n    my_dataset = requests.get(activites_url, headers=header, params=param).json()\\n    activities = pd.json_normalize(my_dataset)\\n    # keep useful features\\n    columns_to_keep = [\\"name\\", \\"kudos_count\\", \\"distance\\", \\"moving_time\\", ...]\\n    activities = activities[columns_to_keep]\\n    return activities\\n```\\n\\nBy simply adding a while loop to this code, it was easy to get all the Strava data I needed for my project!\\n\\n| name                             | distance           | moving_time       | total_elevation_gain | workout_type | kudos_count | max_speed | average_heartrate | max_heartrate | pr_count | suffer_score | is_named | local_date | run_per_day | max_run | run_area           | average_speed_mpk | datetime            | hour | uk_awake |\\n| -------------------------------- | ------------------ | ----------------- | -------------------- | ------------ | ----------- | --------- | ----------------- | ------------- | -------- | ------------ | -------- | ---------- | ----------- | ------- | ------------------ | ----------------- | ------------------- | ---- | -------- |\\n| Plastic bag floating in the wind | 16.1041            | 75.0              | 181.2                | 0.0          | 45          | 5.4       | 144.0             | 165.0         | 0        | 33.0         | 1        | 2021-09-09 | 1           | 1       | 12.377207167446613 | 4.656607991058955 | 2021-09-09 13:57:28 | 13   | 1        |\\n| Cake loop \ud83c\udf82                     | 21.0416            | 99.05             | 346.9                | 0.0          | 54          | 5.0       | 151.0             | 167.0         | 1        | 83.0         | 1        | 2021-09-08 | 1           | 1       | 84.87930754199624  | 4.706580062129342 | 2021-09-08 13:43:26 | 13   | 1        |\\n| Doubl\xe9                           | 8.0736             | 38.78333333333333 | 92.0                 | 0.0          | 30          | 4.4       | 137.7             | 161.0         | 0        | 12.0         | 1        | 2021-09-07 | 2           | 0       | 4.8462999984622    | 4.802881844380403 | 2021-09-07 20:07:54 | 20   | 0        |\\n| Moderate beanage\ud83d\udc18               | 19.607200000000002 | 87.51666666666667 | 23.0                 | 3.0          | 52          | 7.3       | 142.9             | 191.0         | 0        | 44.0         | 1        | 2021-09-07 | 2           | 1       | 6.491399835795164  | 4.463310123192287 | 2021-09-07 13:12:26 | 13   | 1        |\\n\\n## Exploratory Data Analysis (EDA)\\n\\nOnce I had all the data, the next thing on the agenda was some EDA. This involved exploring the data, looking at distributions, correlations, and general information about different features. The notebook I created for this can be found on [my Github page](https://github.com/jackmleitch/StravaKudos).\\n\\nSome notable findings include:\\n\\n- The Kudos received depends heavily on how many followers I had at the time. Unfortunately, there was no way to see how many followers I had at each point in time so I was unable to weight the Kudos with respect to follower count. I therefore could only use my most recent 1125 activities (from Dec 2019 to present) to train my model as the kudos stayed fairly consistent in this interval.\\n\\n![kudos_time](./images/StravaPrediction/kudos_time.png)\\n\\n- It was found that the target variable (Kudos) is skewed right (also bimodal) and there are some extreme values above ~100. When using certain models, for example, linear regression, would like to see more of a normal distribution in the target variable. A BoxCox transform could therefore be used to see a more \u2018normal-looking\u2019 distribution in the target variable (normality can then be checked by plotting a q-q plot, for example).\\n\\n- An [isolation forest model](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) was used to detect outliers and out of the 800 training points, 17 outliers were found. Upon further investigation, these were predominantly activity uploads that were initially set to private but then switched to public at a later date. This meant that other users couldn\'t see or interact with the activity. These were removed from the training set.\\n\\n- Features such as distance, moving_time, and average_speed_mpk seem to share a similar distribution to the one we have with kudos_count. This was confirmed when looking at a correlation matrix.\\n\\n- The photo count feature only has a few different data points in each nonzero category so we can change this to a binary feature of contains photos/no photos.\\n\\n- Workout type seems to correlate strongly with Kudos, however, there aren\'t that many data points for workout type 1 (a race). Intuitively, I also believe that races receive more Kudos in general. [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) was therefore used to oversample this workout type to give more predictive power to this feature and it was later found that this reduced the RMSE.\\n\\n- By looking at time distribution between activities, it was found that runs that are quickly followed in succession by other runs tend to receive less kudos than runs that were the only activity that day. To add to this, the longest activity of the day tends to receive more kudos than the other runs that day.\\n\\n## Preprocessing\\n\\n### Train, Test, and Cross-Validation\\n\\nI first needed to split my dataset into a train and test set. I used an 80/20 split and as my data contained dates, I just used the most recent 20% of data as the test set. I did not want to randomly shuffle the dataset as I created some time-based features.\\n\\nIt is extremely important to have a good cross-validation scheme in which validation data is representative of training and real-world data, as it makes the model you build highly generalizable. Now, because I only had a small number of training examples, it would be prohibitive to have both validation and test sets. Cross-validation was therefore used on the training set to simulate a validation set.\\n\\nI split the training set into 5 folds using Sklearn\u2019s [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html), where [Sturge\u2019s rule](https://www.statology.org/sturges-rule/) was used to calculate the appropriate number of bins.\\n\\n```py title=\\"/src/create_folds.py\\"\\ndef create_folds(data):\\n    # create a new col kfold and fill with -1\\n    data[\\"kfold\\"] = -1\\n    # randomize the rows of the data\\n    data = data.sample(frac=1).reset_index(drop=True)\\n    # Sturge\'s rule to calc bins\\n    num_bins = int(np.floor(1 + np.log2(len(data))))\\n    # bin targets\\n    data.loc[:, \\"bins\\"] = pd.cut(data[\\"kudos_count\\"], bins=num_bins, labels=False)\\n    # initiate kfold class\\n    kf = model_selection.StratifiedKFold(n_splits=5)\\n    # fill the new kfold col\\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\\n        data.loc[v_, \\"kfold\\"] = f\\n    # drop bins col\\n    data = data.drop(\\"bins\\", axis=1)\\n    # return df with folds\\n    return data\\n```\\n\\nUsing this cross-validation scheme I was able to confidently compare different models to select the most promising models without worrying about overfitting the validation sets. This scheme was also used to tune the hyperparameters of the promising models (the folds were reshuffled before hyperparameter tuning though). It is worth noting that the only time the model was evaluated on the test set was right at the end of the project when I wanted to get a real gauge of the true performance.\\n\\n### Missing Values\\n\\nThere was not a huge amount of missing values in the dataset, and most of the missing values came from features that are fairly new to Strava/features I didn\'t always use. For example, the workout_type feature didn\'t always have a \u2018race\u2019 option (which turned out to have a lot of predicting power!). Before imputing missing values, it\'s always really important to first investigate missing values and try to find out if there is a reason why they are missing, it might be important!\\n\\nAfter some inspection, I manually entered some missing values for workout_type as I believed this to be very important and some older data didn\u2019t have labels. I chose to manually enter the values as any simple imputation technique would\'ve assigned all the missing values to the default Strava category of \u2018easy run\u2019. A KNN imputer was also considered but there weren\'t many unlabeled data points so manual entry was quicker.\\n\\nA heuristic function was also used to fill in activities with titles that contain race positions e.g. 10th or 1st, as these activities were obviously races. This was done because races seemed to always have a big influence on Kudos.\\n\\nThe general imputation scheme used on the whole dataset was:\\n\\n- Any missing values in a categorical feature were just assigned a new category \u2018NONE\u2019 \u2014 this seems to work well most of the time when dealing with missing categories.\\n\\n- All missing values in numeric features were imputed using the median strategy (mean was also tried but median worked better).\\n\\n### Other Preprocessing\\n\\nOther data preparation steps included: dropping activities that weren\'t runs, dropping activities that had no Kudos (mostly because run would\'ve been on private and then switched to public), and formatting DateTime information.\\n\\n## Feature Engineering\\n\\nFeature engineering is one of the most crucial parts of building a good machine learning model. If we have useful features, the model will perform better.\\n\\n### Time-Based Features\\n\\nWe have date and time data for each activity so we can easily make some trivial time-based features. Some of these features include year, week of the year, month, day of the week, time of day, weekend, etc.\\n\\nFrom experience, I also know that the more activities I post on Strava a day the fewer kudos I tend to receive so I can try to incorporate this into a few features too. The features I made to encapsulate this:\\n\\n- max_run (binary) \u2014 is the activity the longest in distance of that day?\\n- run_per_day (ordinal) \u2014 how many total activities were done that day.\\n\\nI am from the U.K. but I am currently studying in Boise, Idaho so the majority of my friends/followers are from the U.K. To try to account for this, I made a binary feature \u2018is_uk_awake\u2019 that checks if the U.K. is awake when that activity was uploaded. I generally find that posting an activity outside of this time window results in fewer kudos.\\n\\n### Other Features\\n\\n- Runs that are left with default names such as \u2018Afternoon Run\u2019 tend to get less interaction than runs that have custom names. A binary feature was thus created to check if the run was named or not.\\n\\n- I\u2019ve always had an intuition that runs with really nice aesthetic GPS traces tend to receive more attention than runs that don\'t. To try to encapsulate this into a feature I used the runs GPS trace to workout the area enclosed by the run loop (an out and back run will return a value of 0). My idea is that bigger loops get more Kudos than smaller loops or out and back runs.\\n\\n- Some other features were added and can be found in my code on Github.\\n\\n### Feature Encoding\\n\\nOne-hot-encoding was used to encode categorical features and ordinal encoding was used to encode ordinal features (categorical features where the order means something).\\n\\nThe workout-type feature seemed to struggle with predictive power even though I know it is a very helpful and correlated feature, it might have something to do with the uneven distribution of categories. So to help it out I used target encodings. We need to be very careful here though as this might overfit the model. Target encoding is a technique in which you map each category in a given feature to its mean target value, but this needs to always be done in a cross-validated manner!\\n\\n```py title=\\"/src/target_encoding.py\\"\\ndef mean_target_encoding(df):\\n    # list categorical columns\\n    cat_cols = [\\"workout_type\\", \\"is_named\\", ...]\\n\\n    # we label encode all the features\\n    for col in cat_cols:\\n        # initialise label encoder\\n        lbl = preprocessing.LabelEncoder()\\n        # fit label encoder\\n        lbl.fit(df[col])\\n        # transform all of the data\\n        df.loc[:, col] = lbl.transform(df[col])\\n\\n    # a list to store 5 validation dataframes\\n    encoded_dfs = []\\n    # loop over every fold\\n    for fold in range(3):\\n        # get training and validation data\\n        df_train = df[df.kfold != fold].reset_index(drop=True)\\n        df_valid = df[df.kfold == fold].reset_index(drop=True)\\n        # for all cat columns\\n        for column in cat_cols:\\n            # create dict of category:mean target\\n            mapping_dict = dict(df_train.groupby(column)[\\"kudos_count\\"].mean())\\n            # column_enc is the new column we have with mean encodings\\n            df_valid.loc[:, column + \\"_enc\\"] = df_valid[column].map(mapping_dict)\\n        # append to our list of encoded validation dfs\\n        encoded_dfs.append(df_valid)\\n\\n    # create full dataframe again and return\\n    encoded_df = pd.concat(encoded_dfs, axis=0)\\n    return encoded_df\\n```\\n\\n### Feature Scaling\\n\\nFinally, we need to consider scaling. As a rule of thumb, when we use a model that computes distance like K.N.N or assumes normality like linear regression, we need to scale our numeric features. Typically, we use either [min-max scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) or [standardization](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and to figure out which to use we just have to try both and see which results in the best performance. Tree-based models, however, are not distance-based and can handle varying ranges of features, and hence scaling is not required.\\n\\n## Modeling\\n\\n### Initial Model Selection\\n\\nIn this step, I built a few different candidate models and compared different metrics to determine which was the best model for deployment. Model performances and features used were saved in a CSV file to help keep track of which model was performing the best. It is very important when comparing models to train and evaluate in a cross-validated manner!\\n\\nThe training script I used is shown below (note: some things were removed to declutter, e.g. target encodings, etc.).\\n\\n```py title=\\"/src/train.py\\"\\ndef run(fold, model, scale_features=False):\\n\\n    # read training data with folds\\n    df = pd.read_csv(config.STRAVA_TRAIN_KFOLD_PATH)\\n\\n    # list all features\\n    num_cols = [\\"distance\\", \\"average_speed_mpk\\", \\"suffer_score\\", ...]\\n    cat_cols = [\\"max_run\\", \\"workout_type\\", ...]\\n    # all cols are features except for target and kfold\\n    features = num_cols + cat_cols\\n\\n    # fill cat column NaN values with NONE\\n    for col in cat_cols:\\n        df.loc[:, col] = df[col].astype(str).fillna(\\"NONE\\")\\n\\n    # training data is where kfold is not equal to fold\\n    df_train = df[df.kfold != fold].reset_index(drop=True)\\n    y_train = df_train.kudos_count.values\\n\\n    # validation data is where kfold = fold\\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\\n    y_valid = df_valid.kudos_count.values\\n\\n    # pipelines for model transformation\\n    if scale_features:\\n        num_pipeline = Pipeline(\\n            [\\n                (\\"imputer\\", impute.SimpleImputer(strategy=\\"median\\")),\\n                (\\"std_scaler\\", preprocessing.StandardScaler()),\\n            ]\\n        )\\n    else:\\n        num_pipeline = Pipeline([(\\"imputer\\", impute.SimpleImputer(strategy=\\"median\\"))])\\n\\n    cat_pipeline = Pipeline(\\n        [(\\"cat\\", preprocessing.OneHotEncoder(handle_unknown=\\"ignore\\"))]\\n    )\\n\\n    # transforms columns and drops columns not specified\\n    x_train_num = num_pipeline.fit_transform(df_train[num_cols])\\n    x_train_cat = cat_pipeline.fit_transform(df_train[cat_cols])\\n    x_valid_num = num_pipeline.transform(df_valid[num_cols])\\n    x_valid_cat = cat_pipeline.transform(df_valid[cat_cols])\\n\\n    # join numeric data and categorical data\\n    x_train = hstack((x_train_num, x_train_cat), format=\\"csr\\")\\n    x_valid = hstack((x_valid_num, x_valid_cat), format=\\"csr\\")\\n\\n    # initialize model\\n    model = model_dispatcher.models[model]\\n\\n    # fit model on training data\\n    model.fit(x_train, y_train)\\n\\n    # predict on validation data\\n    valid_preds = model.predict(x_valid)\\n\\n    # get rmse, and mape\\n    rmse = metrics.mean_squared_error(y_valid, valid_preds, squared=False)\\n    max_error = metrics.max_error(y_valid, valid_preds)\\n    print(f\\"Fold = {fold}, rmse = {rmse}, max error = {max_error}\\")\\n\\n    data = [x_train, y_train, x_valid, y_valid]\\n\\n    return rmse, model, data\\n```\\n\\nSome shortlisted models that performed well were: [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_api.html), [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html), and [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). These models were then taken forward to the feature selection stage.\\n\\n### Feature Selection\\n\\nFeature selection was then performed to help reduce the dimensionality of the dataset with the hope that this creates a more generalizable model, we\'ve all heard of the [curse of dimensionality](https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e).\\n\\nDifferent feature selection techniques were used for the different models. For example, we did not need to do much selection for the Lasso model. When trying to minimize the cost function, Lasso regression will automatically select those features that are useful, discarding the useless or redundant features. Therefore, all we needed to do was remove features with coefficients equal to zero.\\n\\nWith the tree-based models, the final methodology used a combination of [Shapley values](https://towardsdatascience.com/a-novel-approach-to-feature-importance-shapley-additive-explanations-d18af30fc21b) and the built-in feature importance values. Although, care needs to be taken when looking at the feature importance from tree-based models which you can read more about it [here](https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7).\\n\\nPCA was also trialed to help combat the dimensionality but it did not improve the model performance or generalizability. One reason might be that the feature space was only reduced by 2 when accounting for 90% of the variance.\\n\\nThe final features chosen were: distance, average speed, max run (was the run the longest run of the day), workout type (+target encodings), total elevation gain, moving time, suffer score, run per day (number of runs that day), max speed, run area (area enclosed by GPS trace), and UK awake (was the UK awake when activity was posted?).\\n\\n![alt](./images/StravaPrediction/feature_imp.png)\\n\\n### Hyperparameter Tuning\\n\\n[Optuna](https://optuna.org/) was used to tune all three shortlisted models, an open-source framework that is very easy to use and incredibly good. In particular, the Tree-structured Parzen Estimator (TPE) was used to sample the parameter space, which uses Bayesian reasoning to construct a surrogate model and can select the next hyperparameters using [Expected Improvement](<http://krasserm.github.io/2018/03/21/bayesian-optimization/#:~:text=Expected%20improvement%20is%20defined%20as,if%20\u03c3(x)%3D0>). A good tutorial on hyperparameter tuning with Optuna can be found [here](https://medium.com/subex-ai-labs/efficient-hyperparameter-optimization-for-xgboost-model-using-optuna-3ee9a02566b1).\\n\\nThe code I used to tune my XGB model is shown below.\\n\\n```py title=\\"/src/tune_hyperparameters/py\\"\\ndef objective(trial, n_jobs=-1, random_state=42):\\n    # read in the data\\n    data = pd.read_csv(\\"input/data_train.csv\\")\\n    # prepare data\\n    df = process(data)\\n    df.loc[:, \\"kudos_count\\"] = data.kudos_count\\n\\n    # stratisfied kfold\\n    df = create_folds(df)\\n\\n    # define parameter search space for XGB\\n    params = {\\n        \\"tree_method\\": \\"gpu_hist\\",\\n        \\"verbosity\\": 0,  # 0 (silent) - 3 (debug)\\n        \\"objective\\": \\"reg:squarederror\\",\\n        \\"n_estimators\\": trial.suggest_int(\\"n_estimators\\", 100, 5000),\\n        \\"max_depth\\": trial.suggest_int(\\"max_depth\\", 4, 12),\\n        \\"learning_rate\\": trial.suggest_loguniform(\\"learning_rate\\", 0.005, 0.05),\\n        \\"colsample_bytree\\": trial.suggest_loguniform(\\"colsample_bytree\\", 0.2, 0.6),\\n        \\"subsample\\": trial.suggest_loguniform(\\"subsample\\", 0.4, 0.8),\\n        \\"alpha\\": trial.suggest_loguniform(\\"alpha\\", 0.01, 10.0),\\n        \\"lambda\\": trial.suggest_loguniform(\\"lambda\\", 1e-8, 10.0),\\n        \\"gamma\\": trial.suggest_loguniform(\\"lambda\\", 1e-8, 10.0),\\n        \\"min_child_weight\\": trial.suggest_loguniform(\\"min_child_weight\\", 10, 1000),\\n        \\"seed\\": random_state,\\n        \\"n_jobs\\": n_jobs,\\n    }\\n\\n    # add pruning callback which prunes unpromising trials\\n    pruning_callback = XGBoostPruningCallback(trial, \\"validation_0-rmse\\")\\n\\n    # scores for each fold\\n    scores = []\\n    # everything is a feature apart from the target and kfold col\\n    features = [f for f in df.columns if f not in (\\"kudos_count\\", \\"kfold\\")]\\n    # loop over each fold\\n    for fold in range(5):\\n        # training data is where kfold is not equal to fold\\n        x_train = df[df.kfold != fold].reset_index(drop=True)\\n        y_train = x_train.kudos_count.values\\n        x_train = x_train[features]\\n        # validation data is where kfold = fold\\n        x_valid = df[df.kfold == fold].reset_index(drop=True)\\n        y_valid = x_valid.kudos_count.values\\n        x_valid = x_valid[features]\\n        # initialize model and fit on data\\n        model = xgb.XGBRegressor(**params)\\n        model.fit(\\n            x_train,\\n            y_train,\\n            eval_set=[(x_valid, y_valid)],\\n            early_stopping_rounds=100,\\n            verbose=False,\\n            callbacks=[pruning_callback],\\n        )\\n        # predict and score model on validation set\\n        preds = model.predict(x_valid)\\n        scores.append(mean_squared_error(y_valid, preds, squared=False))\\n\\n    # return average rmse across the 5 folds\\n    score = sum(scores) / len(scores)\\n    return score\\n\\n\\nif __name__ == \\"__main__\\":\\n    # initialize Optuna TPE sampler\\n    sampler = TPESampler()\\n    # create study in which we minimize rmse\\n    study = create_study(direction=\\"minimize\\", sampler=sampler)\\n    # optimize hyperparams on XGB model\\n    study.optimize(objective, n_trials=100)\\n\\n    # display params\\n    best = study.best_params\\n    for key, value in best.items():\\n        print(f\\"{key:>20s} : {value}\\")\\n    print(f\\"{\'best objective value\':>20s} : {study.best_value}\\")\\n\\n    # save best hyperparams\\n    with open(\\"models/production/xgb_params.pickle\\", \\"wb\\") as f:\\n        pickle.dump(best, f)\\n```\\n\\nThe optimization history plot below shows how quickly the TPE sampler converges to the optimal hyperparameters. The two other plots show the importance of each hyperparameter and the high-dimensional parameter relationships, respectively.\\n\\n![alt](./images/StravaPrediction/hyper1.png)\\n![alt](./images/StravaPrediction/hyper2.png)\\n![alt](./images/StravaPrediction/hyper3.png)\\n\\n### Model Selection\\n\\nAfter performing the hyperparameter tuning, it was found that the XGB model consistently outperformed the other two shortlisted models. As seen in the table below, the average RMSE over 5 folds was lower. It was also found that the variance of the error was lower. From this, I concluded that with the data I had, the XGB model was more likely to generalize better to unseen data.\\n\\n| Model                 | Fold 1 | Fold 2 | Fold 3 | Fold 4 | Fold 5 | Mean  | Variance |\\n| --------------------- | ------ | ------ | ------ | ------ | ------ | ----- | -------- |\\n| XGBRegressor          | 9.12   | 9.47   | 9.33   | 9.59   | 9.54   | 9.41  | 0.036    |\\n| RandomForestRegressor | 10.51  | 10.94  | 11.09  | 11.16  | 10.75  | 10.89 | 0.070    |\\n| Lasso                 | 9.78   | 9.99   | 10.93  | 11.04  | 10.96  | 10.54 | 0.36     |\\n\\nThe final XGB model was then trained on the **whole** training dataset using the hyperparameters found earlier.\\n\\nIt was comforting to see that when these three models were used to predict on the test dataset, the XGB model outperformed both other models. Disclaimer: this was (and needs to be) done after model selection, you don\'t want to overfit the test set! The test set scores were 9.78, 11.83, and 10.68 for XGB, RF, and Lasso, respectively.\\n\\n## Deployment\\n\\nThe model, along with some other features, was deployed using Streamlit and Heroku. Tutorials on Streamlit and Heroku can be found [here](https://towardsdatascience.com/build-your-first-interactive-data-science-web-app-with-streamlit-d4892cbe4792) and [here](https://towardsdatascience.com/a-quick-tutorial-on-how-to-deploy-your-streamlit-app-to-heroku-874e1250dadd). An interface was built in which you could predict on my real-time Strava data, explore the model explainability, and explore the dataset. The app can be found [here](http://strava-kudos.herokuapp.com/) (it may take a minute or so to boot up, apologies).\\n\\n## Model Explainability\\n\\nThe final part of this project was investigating the model to try and see, in general, what attributes caused activities to get more/fewer kudos.\\n\\nInterpreting XGBoost models is a notoriously hard task but with the combination of a solid theoretical backing and a fast practical algorithm, SHAP values are an incredibly powerful tool for confidently interpreting tree-based models.\\n\\nThe mean SHAP values for the final features can be seen below.\\n![alt](./images/StravaPrediction/shap.png)\\n\\nWe can see that the distance feature is actually the most important, followed by the max_run feature (which is great to see as this is a feature I created!). Because SHAP gives us individualized explanations for every activity, we can do more than just make a bar chart. We can see how each feature affects the final Kudos value:\\n![alt](./images/StravaPrediction/shap2.png)\\n\\nThe color in this plot represents the feature value \u2014 red indicating high and blue indicating low. So we can see that longer faster runs get more Kudos. More effort (higher heart rate and suffer score) also results in higher Kudos. On the other hand, easy runs get fewer Kudos than workouts, long runs, and races.\\n\\n## Closing Thoughts\\n\\nSo there we go, a unique and successful data science project has been finished. Well, kind of\u2026 just because we have built a good model doesn\'t mean the model is always going to be a good one, it might drift.\\n\\nModel drift refers to the degradation of model performance due to changes in data and relationships between input and output variables. So its important to check in every and then to see if the model is still performing at an adequate level. Maybe this is a topic for a future blog though.\\n\\nThanks for reading and I hope you enjoyed it. If you have any comments or questions please feel free to reach out."},{"id":"Recipe-Recommendation","metadata":{"permalink":"/blog/Recipe-Recommendation","source":"@site/blog/2021-07-28-RecipeRecomm.md","title":"Building a Recipe Recommendation System","description":"Using Word2Vec, Scikit-Learn, and Streamlit","date":"2021-07-28T00:00:00.000Z","formattedDate":"July 28, 2021","tags":[{"label":"Python","permalink":"/blog/tags/python"},{"label":"NLP","permalink":"/blog/tags/nlp"},{"label":"Recommendation System","permalink":"/blog/tags/recommendation-system"}],"readingTime":12.565,"truncated":true,"authors":[{"name":"Jack Leitch","title":"Machine Learning Engineer","url":"https://github.com/jackmleitch","imageURL":"https://github.com/jackmleitch.png","key":"jack"}],"frontMatter":{"slug":"Recipe-Recommendation","title":"Building a Recipe Recommendation System","tags":["Python","NLP","Recommendation System"],"authors":"jack"},"prevItem":{"title":"Predicting Strava Kudos","permalink":"/blog/Strava-Kudos"},"nextItem":{"title":"Automating Mundane Web-Based Tasks With Selenium and Heroku","permalink":"/blog/Strava-AP"}},"content":"**Using Word2Vec, Scikit-Learn, and Streamlit**\\n\\nFirst things first, If you would like to play around with the finished app. You can here: https://share.streamlit.io/jackmleitch/whatscooking-deployment/streamlit.py.\\n\\n![alt](./images/RecipeRecommendation/app.png)\\n\\nIn a previous blog post (Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku) I wrote about how I went about building a recipe recommendation system. To summarize: I first cleaned and parsed the ingredients for each recipe (for example, 1 diced onion becomes onion), next I **encoded each recipe ingredient list using TF-IDF**. From here I applied a similarity function to find the similarity between **ingredients for known recipes and the ingredients given by the end-user**. Finally, we can get the top-recommended recipes according to the similarity score.\\n\\n\x3c!--truncate--\x3e\\n\\n![alt](./images/RecipeRecommendation/flowchart.png)\\n\\nHowever, after using my new API for a while I wanted to improve on two things:\\n\\n1. Sometimes the ingredients in the recommendations didn\'t line up well with the input ingredients, so I needed some better embeddings.\\n\\n2. The Flask API was well in need for a good makeover.\\n\\nEnter Gensim\u2019s **Word2Vec** and **Streamlit**.\\n\\n## Preprocessing and Parsing of Ingredients\\n\\nTo understand the task at hand here, let\u2019s look at an example. The delicious \u2018Gennaro\u2019s classic spaghetti carbonara\u2019 recipe found on Jamie Oliver\u2019s [website](https://www.jamieoliver.com/recipes/pasta-recipes/gennaro-s-classic-spaghetti-carbonara/) requires the ingredients:\\n\\n- 3 large free-range egg yolks\\n\\n- 40 g Parmesan cheese, plus extra to serve\\n\\n- 1 x 150 g piece of higher-welfare pancetta\\n\\n- 200 g dried spaghetti\\n\\n- 1 clove of garlic\\n\\n- extra virgin olive oil\\n\\nThere is a lot of redundant information here; for example, weights and measures are not going to add value to the vector encodings of the recipe. If anything, it is going to make distinguishing between recipes more difficult. So we need to get rid of those. A quick google search led me to a [Wikipedia page](https://en.wikibooks.org/wiki/Cookbook:Units_of_measurement) containing a list of standard cooking metrics e.g. clove, gram (g), teaspoon, etc. Removing all these words in my ingredient parser worked really well.\\n\\nWe also want to remove stop words from our ingredients. In NLP \u2018stop words\u2019 refer to the most common words in a language. For example, the sentence \u2018learning about what stop words are\u2019, becomes, \u2018learning stop words\u2019. NLTK provides us with an easy way to remove (most of) these words.\\n\\nThere are also other words in the ingredients that are useless to us \u2014 these are the words that are very common among recipes. Oil, for example, is used in most recipes and will provide little to no distinguishing power between recipes. Also, most people have oil in their homes so having to write oil every time you use the API is cumbersome and pointless. There are advanced NLP techniques, for example using [conditional random fields](https://open.blogs.nytimes.com/2015/04/09/extracting-structured-data-from-recipes-using-conditional-random-fields/) (a class of statistical modeling), that can calculate the probability that a word is an ingredient, as opposed to a measure, texture, or another type of word that surrounds the ingredient word. But, simply removing the most common words seemed to be very effective, so I did this. Occam\u2019s razor and all that jazz\u2026 To get the most common words we can do the following:\\n\\n```py title=\\"/src/get_word_freq.py\\"\\nimport nltk\\nvocabulary = nltk.FreqDist()\\n# This was done once I had already preprocessed the ingredients\\nfor ingredients in recipe_df[\'ingredients\']:\\n    ingredients = ingredients.split()\\n    vocabulary.update(ingredients)\\nfor word, frequency in vocabulary.most_common(200):\\n    print(f\'{word};{frequency}\')\\n```\\n\\nWe have one final obstacle to get over, however. When we try to remove these \u2018junk\u2019 words from our ingredient list, what happens when we have different variations of the same word? What happens if we want to remove every occurrence of the word \u2018pound\u2019 but the recipe ingredients say \u2018pounds\u2019? Luckily there is a pretty trivial workaround: **lemmatization** and **stemming**. Stemming and lemmatization both generate the root form of inflected words \u2014 the difference is that a stem might not be an actual word whereas, a lemma is an actual language word. Although lemmatization is often slower, I chose to use this technique as I know the ingredients will be actual words which is useful for debugging and visualization (the results turned out to be practically identical using stemming instead). When the user feeds ingredients to the API we also lemmatize those words as the lemmatized words are the words with the corresponding vectorizations.\\n\\nWe can put this all together in a function, `ingredient_parser`, along with some other standard preprocessing: getting rid of punctuation, removing accents, making everything lowercase, getting rid of Unicode.\\n\\n```py title=\\"/src/parse_ingredients.py\\"\\ndef ingredient_parser(ingredients):\\n    # measures and common words (already lemmatized)\\n    measures = [\'teaspoon\', \'t\', \'tsp.\', \'tablespoon\', \'T\', ...]\\n    words_to_remove = [\'fresh\', \'oil\', \'a\', \'red\', \'bunch\', ...]\\n    # Turn ingredient list from string into a list\\n    if isinstance(ingredients, list):\\n       ingredients = ingredients\\n    else:\\n       ingredients = ast.literal_eval(ingredients)\\n    # We first get rid of all the punctuation\\n    translator = str.maketrans(\'\', \'\', string.punctuation)\\n    # initialize nltk\'s lemmatizer\\n    lemmatizer = WordNetLemmatizer()\\n    ingred_list = []\\n    for i in ingredients:\\n        i.translate(translator)\\n        # We split up with hyphens as well as spaces\\n        items = re.split(\' |-\', i)\\n        # Get rid of words containing non alphabet letters\\n        items = [word for word in items if word.isalpha()]\\n        # Turn everything to lowercase\\n        items = [word.lower() for word in items]\\n        # remove accents\\n        items = [unidecode.unidecode(word) for word in items]\\n        # Lemmatize words so we can compare words to measuring words\\n        items = [lemmatizer.lemmatize(word) for word in items]\\n        # get rid of stop words\\n        stop_words = set(corpus.stopwords.words(\'english\'))\\n        items = [word for word in items if word not in stop_words]\\n        # Gets rid of measuring words/phrases, e.g. heaped teaspoon\\n        items = [word for word in items if word not in measures]\\n        # Get rid of common easy words\\n        items = [word for word in items if word not in words_to_remove]\\n        if items:\\n           ingred_list.append(\' \'.join(items))\\n    return ingred_list\\n```\\n\\nWhen parsing the ingredients for \u2018Gennaro\u2019s classic spaghetti carbonara\u2019 we get: egg yolk, parmesan cheese, pancetta, spaghetti, garlic. Perfect, that works fantastically! Using `df.apply` when can easily parse the ingredients for each recipe.\\n\\n## Word Embeddings Using Word2Vec\\n\\nWord2Vec is a 2-layer neural network that produces word embeddings. It takes in a corpus of text as an input (a collection of individual documents) and maps each word to a vector in Euclidean space. The end goal is to have a text representation that captures **distributional similarities** between words. That is, words that often appear in similar contexts are mapped to vectors separated by a shorter Euclidean distance (the L2 norm).\\n\\nIn the context of recipe ingredients, Word2vec allowed me to capture similarities between recipe ingredients that are commonly used together. For example, mozzarella cheese which is commonly used when making pizza is most similar to other cheeses and pizza-related ingredients.\\n\\n```py\\nprint(model_cbow.wv.most_similar(u\'mozzarella cheese\'))\\n>>> [(\'parmesan cheese\', 0.999547004699707), (\'cheddar cheese\', 0.9995082020759583),\\n    (\'tomato\', 0.9994533061981201), (\'mushroom\', 0.9994530081748962),\\n    (\'cheese\', 0.9994290471076965), (\'mozzarella\', 0.999423086643219),\\n    (\'sausage\', 0.9993994832038879), (\'pepperoni\', 0.9993913173675537),\\n    (\'onion\', 0.9993910193443298), (\'chicken breast\', 0.9993739724159241)]\\n```\\n\\nI chose to use the **Continuous bag of words** (CBOW) variant of Word2Vec. CBOW tries to learn a language model that tries to predict the center word in a sliding window. Due to the fact that Word2Vec tries to predict words based on the word\'s surroundings, it was vital to sort the ingredients alphabetically. If I did not sort the ingredients with respect to a consistent criterion, the model would interpret ingredients in a different order as having different contexts.\\n\\nThe hyperparameters chosen are below:\\n\\n- `size=100`, this refers to the dimensionality of the embeddings. I settled on 100 by trying 50, 100, 150 and seeing which performed best.\\n\\n- `sg = 0`, this creates a CBOW model as opposed to a SkipGram model.\\n\\n- `workers = 8`, the number of CPUs I have.\\n\\n- `window = 6`, this is the size of the sliding window in which the center word is predicted. 6 was chosen as it is the average length of each document (ingredient list).\\n\\n- `min_count = 1`, words below the min_count frequency are dropped before training occurs. 1 was chosen as I want all ingredients to have an embedding.\\n\\nThe Word2Vec training code is below. I used the popular python library Gensim.\\n\\n```py title=\\"/src/train_word2vec.py\\"\\nfrom gensim.models import Word2Vec\\n\\n# get corpus with the documents sorted in alphabetical order\\ndef get_and_sort_corpus(data):\\n    corpus_sorted = []\\n    for doc in data.parsed.values:\\n        doc.sort()\\n        corpus_sorted.append(doc)\\n    return corpus_sorted\\n\\n# calculate average length of each document\\ndef get_window(corpus):\\n    lengths = [len(doc) for doc in corpus]\\n    avg_len = float(sum(lengths)) / len(lengths)\\n    return round(avg_len)\\n\\nif __name__ == \\"__main__:\\n    # load in data\\n    data = pd.read_csv(\'input/df_recipe.csv\')\\n    # parse the ingredients for each recipe\\n    data[\'parsed\'] = data.ingredients.apply(ingredient_parser)\\n    # get corpus\\n    corpus = get_and_sort_corpus(data)\\n    print(f\\"Length of corpus: {len(corpus)}\\")\\n    # train and save CBOW Word2Vec model\\n    model_cbow = Word2Vec(\\n      corpus, sg=0, workers=8, window=get_window(corpus), min_count=1, vector_size=100\\n    )\\n    model_cbow.save(\'models/model_cbow.bin\')\\n    print(\\"Word2Vec model successfully trained\\")\\n```\\n\\n## Document Embeddings\\n\\nIn order to build the recipe recommendation, I needed to represent each document (each ingredient list) as a single embedding. This then allowed me to calculate corresponding similarities. So how can we use our word embeddings to compute a representative vector for the whole ingredient list? I tried out two different methods: one simple and one slightly more complicated. For this, I followed this excellent [tutorial](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/).\\n\\n### Averaging Word Embeddings\\n\\nThis does what it says on the tin, it directly averages all word embeddings in each document. The code is adapted from [here](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/) and [here](https://towardsdatascience.com/nlp-performance-of-different-word-embeddings-on-text-classification-de648c6262b).\\n\\n```py title=\\"/src/mean_word_embeddings.py\\"\\nclass MeanEmbeddingVectorizer(object):\\n    def __init__(self, model_cbow):\\n        self.model_cbow = model_cbow\\n        self.vector_size = model_cbow.wv.vector_size\\n\\n    def fit(self):\\n        return self\\n\\n    def transform(self, docs):\\n        doc_vector = self.doc_average_list(docs)\\n        return doc_word_vector\\n\\n    def doc_average(self, doc):\\n        mean = []\\n        for word in doc:\\n            if word in self.model_cbow.wv.index_to_key:\\n                mean.append(self.model_cbow.wv.get_vector(word))\\n\\n        if not mean:\\n            return np.zeros(self.vector_size)\\n        else:\\n            mean = np.array(mean).mean(axis=0)\\n            return mean\\n\\n    def doc_average_list(self, docs):\\n        return np.vstack([self.doc_average(doc) for doc in docs])\\n```\\n\\n### Using TF-IDF to Aggregate Embeddings\\n\\nHere we do the same as above but instead, we do a weighted mean and scale each vector using its inverse document frequency (IDF). The IDF measures the importance of a term across the whole corpus. IDF will weigh down terms that are very common across a corpus (in our case words like olive oil, garlic, butter, etc.) and weighs up rare terms. This is useful for us as it will give us better distinguishing power between recipes as the ingredients that are scaled-down will be ingredients that the user will tend to not give as an input to the recommendation system.\\n\\n```py title=\\"/src/tfidf_word_embeddings.py\\"\\nclass TfidfEmbeddingVectorizer(object):\\n    def __init__(self, model_cbow):\\n\\n        self.model_cbow = model_cbow\\n        self.word_idf_weight = None\\n        self.vector_size = model_cbow.wv.vector_size\\n\\n    def fit(self, docs):\\n        \\"\\"\\"\\n\\tBuild a tfidf model to compute each word\'s idf as its weight.\\n\\t\\"\\"\\"\\n\\n        text_docs = []\\n        for doc in docs:\\n            text_docs.append(\\" \\".join(doc))\\n\\n        tfidf = TfidfVectorizer()\\n        tfidf.fit(text_docs)\\n        # if a word was never seen it is given idf of the max of known idf value\\n        max_idf = max(tfidf.idf_)\\n        self.word_idf_weight = defaultdict(\\n            lambda: max_idf,\\n            [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()],\\n        )\\n        return self\\n\\n    def transform(self, docs):\\n        doc_word_vector = self.doc_average_list(docs)\\n        return doc_word_vector\\n\\n    def doc_average(self, doc):\\n\\t\\"\\"\\"\\n\\tCompute weighted mean of documents word embeddings\\n\\t\\"\\"\\"\\n\\n        mean = []\\n        for word in doc:\\n            if word in self.model_cbow.wv.index_to_key:\\n                mean.append(\\n                    self.model_cbow.wv.get_vector(word) * self.word_idf_weight[word]\\n                )\\n\\n        if not mean:\\n            return np.zeros(self.vector_size)\\n        else:\\n            mean = np.array(mean).mean(axis=0)\\n            return mean\\n\\n    def doc_average_list(self, docs):\\n        return np.vstack([self.doc_average(doc) for doc in docs])\\n```\\n\\n## Recommendation System\\n\\nThis application simply consists of text data and there is no kind of ratings available, **so we can not use matrix decomposition** methods, such as SVD and correlation coefficient-based methods.\\nWe use content-based filtering which enables us to recommend recipes to people based on the attributes (ingredients) the user provides. To measure the similarity between documents I used Cosine similarity. I also tried using Spacy and KNN but cosine similarity won in terms of performance (and ease).\\n\\nMathematically, **cosine similarity** measures the cosine of the angle between two vectors. For the mathematically inclined out there, this is the same as the inner product of the same vectors normalized to both have length 1. With cosine similarity, the smaller the angle, the higher the cosine similarity: so we are trying to maximize this score.\\n\\n```py title=\\"/src/recommendation_system.py\\"\\nfrom gensim.models import Word2Vec\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom collections import defaultdict\\n\\nimport config\\nfrom ingredient_parser import ingredient_parser\\n\\ndef get_recommendations(N, scores):\\n    \\"\\"\\"\\n    Rank scores and output a pandas data frame containing all the details of the top N recipes.\\n    :param scores: list of cosine similarities\\n    \\"\\"\\"\\n    # load in recipe dataset\\n    df_recipes = pd.read_csv(config.PARSED_PATH)\\n    # order the scores with and filter to get the highest N scores\\n    top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:N]\\n    # create dataframe to load in recommendations\\n    recommendation = pd.DataFrame(columns=[\\"recipe\\", \\"ingredients\\", \\"score\\", \\"url\\"])\\n    count = 0\\n    for i in top:\\n        recommendation.at[count, \\"recipe\\"] = title_parser(df_recipes[\\"recipe_name\\"][i])\\n        recommendation.at[count, \\"ingredients\\"] = ingredient_parser_final(\\n            df_recipes[\\"ingredients\\"][i]\\n        )\\n        recommendation.at[count, \\"url\\"] = df_recipes[\\"recipe_urls\\"][i]\\n        recommendation.at[count, \\"score\\"] = f\\"{scores[i]}\\"\\n        count += 1\\n    return recommendation\\n\\ndef get_recs(ingredients, N=5, mean=False):\\n    \\"\\"\\"\\n    Get the top N recipe recomendations.\\n    :param ingredients: comma seperated string listing ingredients\\n    :param N: number of recommendations\\n    :param mean: False if using tfidf weighted embeddings, True if using simple mean\\n    \\"\\"\\"\\n    # load in word2vec model\\n    model = Word2Vec.load(\\"models/model_cbow.bin\\")\\n    # normalize embeddings\\n    model.init_sims(replace=True)\\n    if model:\\n        print(\\"Successfully loaded model\\")\\n    # load in data\\n    data = pd.read_csv(\\"input/df_recipes.csv\\")\\n    # parse ingredients\\n    data[\\"parsed\\"] = data.ingredients.apply(ingredient_parser)\\n    # create corpus\\n    corpus = get_and_sort_corpus(data)\\n\\n    if mean:\\n        # get average embdeddings for each document\\n        mean_vec_tr = MeanEmbeddingVectorizer(model)\\n        doc_vec = mean_vec_tr.transform(corpus)\\n        doc_vec = [doc.reshape(1, -1) for doc in doc_vec]\\n        assert len(doc_vec) == len(corpus)\\n    else:\\n        # use TF-IDF as weights for each word embedding\\n        tfidf_vec_tr = TfidfEmbeddingVectorizer(model)\\n        tfidf_vec_tr.fit(corpus)\\n        doc_vec = tfidf_vec_tr.transform(corpus)\\n        doc_vec = [doc.reshape(1, -1) for doc in doc_vec]\\n        assert len(doc_vec) == len(corpus)\\n\\n    # create embeddings for input text\\n    input = ingredients\\n    # create tokens with elements\\n    input = input.split(\\",\\")\\n    # parse ingredient list\\n    input = ingredient_parser(input)\\n    # get embeddings for ingredient doc\\n    if mean:\\n        input_embedding = mean_vec_tr.transform([input])[0].reshape(1, -1)\\n    else:\\n        input_embedding = tfidf_vec_tr.transform([input])[0].reshape(1, -1)\\n\\n    # get cosine similarity between input embedding and all the document embeddings\\n    cos_sim = map(lambda x: cosine_similarity(input_embedding, x)[0][0], doc_vec)\\n    scores = list(cos_sim)\\n    # Filter top N recommendations\\n    recommendations = get_recommendations(N, scores)\\n    return recommendations\\n\\nif __name__ == \\"__main__\\":\\n    # test\\n    input = \\"chicken thigh, onion, rice noodle, seaweed nori sheet, sesame, shallot, soy, spinach, star, tofu\\"\\n    rec = get_recs(input)\\n    print(rec)\\n```\\n\\nIt\u2019s worth noting that there was no concrete way of assessing the performance of the model so I had to evaluate the recommendations manually. To be honest, this ended up actually being quite fun\u2026 I also discovered a tonne of new recipes! It\u2019s comforting to see that when we put in the ingredients (before they are parsed of course!) for our beloved Gennaro\u2019s classic spaghetti carbonara we get the correct recommendation with a score of 1.\\n\\nAs of right now, a few items in my fridge/cupboards are: tomato sauce, pasta, zucchini, onion, cheese Our newly built recommendation system suggests we make:\\n\\n![alt](./images/RecipeRecommendation/test.png)\\n\\nSounds good to me \u2014 best get cooking! As we can see, all the ingredients in this recipe that I did not enter are all very common household items.\\n\\n## Creating an App Using Streamlit\\n\\nFinally, I created and deployed an app using Streamlit. It was fantastically easy to make and the [documentation](https://docs.streamlit.io/en/stable/api.html) was very easy to follow. Code for this app can be found on my [Github](https://github.com/jackmleitch/whatscooking-deployment).\\n\\n![alt](./images/RecipeRecommendation/app.png)\\n\\nThanks for reading and I hope you enjoyed it!"},{"id":"Strava-AP","metadata":{"permalink":"/blog/Strava-AP","source":"@site/blog/2021-07-18-Strava2AP.md","title":"Automating Mundane Web-Based Tasks With Selenium and Heroku","description":"Automating away those unceasing repetitive tasks the easy way","date":"2021-07-18T00:00:00.000Z","formattedDate":"July 18, 2021","tags":[{"label":"Python","permalink":"/blog/tags/python"},{"label":"Automation","permalink":"/blog/tags/automation"},{"label":"Web Scraping","permalink":"/blog/tags/web-scraping"}],"readingTime":7.375,"truncated":true,"authors":[{"name":"Jack Leitch","title":"Machine Learning Engineer","url":"https://github.com/jackmleitch","imageURL":"https://github.com/jackmleitch.png","key":"jack"}],"frontMatter":{"slug":"Strava-AP","title":"Automating Mundane Web-Based Tasks With Selenium and Heroku","tags":["Python","Automation","Web Scraping"],"authors":"jack"},"prevItem":{"title":"Building a Recipe Recommendation System","permalink":"/blog/Recipe-Recommendation"},"nextItem":{"title":"Automating the Setup of my Data Science Projects","permalink":"/blog/Project-Auto"}},"content":"**Automating away those unceasing repetitive tasks the easy way**\\n\\n## The Task At Hand\\n\\nBeing the avid runner and data lover that I am, naturally, I like to get the best out of logging my training. The solution I settled with a few years back was to log my training on both Strava, and Attackpoint. While both platforms provide a service for tracking exercise using GPS data, Strava has an emphasis on being a social network and it allows you to look into each activity in an in-depth way, something that Attackpoint lacks; Attackpoint on the other hand is more barebone and I personally prefer it for looking at my training as a whole (in timescales of weeks/months/years).\\n\\n\x3c!--truncate--\x3e\\n\\n| ![alt](./images/Strava2AP/strava.png) | ![alt](./images/Strava2AP/AP.png) |\\n| ------------------------------------- | --------------------------------- |\\n\\nThe one problem I have, however, is that I always struggle to keep both of these logs up to date. Garmin (the maker of my running watch) syncs up with both websites, via an API, so my runs magically appear on both websites, so the problem isn\'t that my runs are only appearing on one website. The problem I have is that keeping both websites up to date with run notes/descriptions is a tedious affair. I tend to always keep Strava up to date and dip in and out of posting on Attackpoint whenever I can be bothered. So why not just automate away my commitment issues to these websites?\\n\\n**The task: every time I run and write about it on Strava, take that description and add it to the corresponding run post on Attackpoint.**\\n\\n## The Game Plan\\n\\nSo I had a why, all that was left was to figure out how. Choosing to go from Strava to Attackpoint, and not the other way around, was an easy one. Firstly, Strava is way nicer to use, and secondly, the website is more modern, slick, and fancy so writing code to post and interact with it, although doable, would be more of a challenge. So how did I actually do this?\\n\\nEnter Selenium. Selenium is a Python package that launches and controls a web browser. It is able to fill in forms and simulate mouse clicks in this browser. An excellent tutorial on Selenium, which I used a lot, can be found [here](https://towardsdatascience.com/controlling-the-web-with-python-6fceb22c5f08).\\n\\nWith these kinds of things, I find that it\'s always good to write down a plan of action; whether that be on paper or on your computer, it\'s totally down to personal preference, I tend to go paper. More often than not I end up deviating substantially from the initial plan when things inevitably go wrong but either way, I find writing down initial ideas/thoughts provides clarity and gives me a sense of direction.\\n\\nSo here is a rough outline of how I chose to automate this task:\\n\\n- Make a call to the Strava API to get an ordered list of my recent run\'s unique IDs. This wasn\'t originally in the plan but obtaining my activities + the corresponding information in time order from Strava proved way harder than I initially thought due to added complications (for example, a lot of my runs group runs, and the html gets tricky here). Having these IDs made extracting the information was easier.\\n\\n```py title=\\"/src/strava_api.py\\"\\nimport pandas as pd\\nimport requests\\nimport json\\n\\nimport config\\n\\ndef get_strava_run_ids(id_num=10):\\n    auth_url = \\"https://www.strava.com/oauth/token\\"\\n    activites_url = \\"https://www.strava.com/api/v3/athlete/activities\\"\\n    # my personal strava API information\\n    payload = config.STRAVA_PAYLOAD\\n    # check if there is a new request token\\n    res = requests.post(auth_url, data=payload, verify=False)\\n    access_token = res.json()[\\"access_token\\"]\\n\\n    header = {\\"Authorization\\": \\"Bearer \\" + access_token}\\n    param = {\\"per_page\\": id_num, \\"page\\": 1}\\n    # make the request to the strava API\\n    my_dataset = requests.get(activites_url, headers=header, params=param).json()\\n\\n    # store list of recent (ordered) IDs\\n    id_list = []\\n    for activity in my_dataset:\\n        id_list.append(activity.get(\\"id\\"))\\n    return id_list\\n\\nif __name__ == \\"__main__\\":\\n    # test\\n    ids = get_strava_run_ids()\\n    print(ids)\\n```\\n\\n- Use Selenium to log in to Strava, navigate to the \u2018Your Activities\u2019 tab, retrieve the information for runs corresponding to each unique activity ID. As you can see in the code below, Selenium couldn\'t be easier to use as it\'s so intuitive. The code below is a rough skeleton, the full code + comments can be found on my [Github](https://github.com/jackmleitch/strava2ap) page.\\n\\n```py title=\\"/src/strava_selenium.py\\"\\nfrom selenium import webdriver\\nfrom webdriver_manager.chrome import ChromeDriverManager\\n\\nimport config\\nfrom utils import remove_emoji\\nfrom stravaAPI import get_strava_run_ids\\n\\n\\ndef fetch_strava_activities(num_of_activities=6):\\n    # website login details\\n    strava_login = config.STRAVA_LOGIN\\n    # use Chrome to access web (will update chrome driver if needed)\\n    driver = webdriver.Chrome(ChromeDriverManager().install())\\n    # open the website\\n    driver.get(\\"https://www.strava.com/login\\")\\n\\n    # add username and password and login\\n    name_box = driver.find_element_by_name(\\"email\\")\\n    name_box.send_keys(strava_login.get(\\"email\\"))\\n    pass_box = driver.find_element_by_name(\\"password\\")\\n    pass_box.send_keys(strava_login.get(\\"password\\"))\\n    login_button = driver.find_element_by_id(\\"login-button\\").click()\\n\\n    # filter the feed to only my activities\\n    following_button = driver.find_element_by_id(\\"feed-filter-btn\\").click()\\n    your_activities = driver.find_element_by_xpath(\\n        \\"//a[@href=\'/dashboard?feed_type=my_activity\']\\"\\n    ).click()\\n\\n    # use strava API to get ids for latest runs\\n    ids = get_strava_run_ids(num_of_activities)\\n\\n    # get i latest runs and run details\\n    strava_activities = {}\\n    for i, id in enumerate(ids):\\n\\n        # group runs and non-group runs have different css classes so we need to treat separately\\n        try:\\n            content = driver.find_element_by_xpath(\\n                f\'//div[@id=\\"Activity-{id}\\" and @class=\\"activity feed-entry card\\"]\'\\n            )\\n            entry_body = content.find_element_by_class_name(\\"entry-body\\")\\n        except:\\n            try:\\n                content = driver.find_element_by_xpath(\\n                        f\'//li[@id=\\"Activity-{id}\\" and @class=\\"activity child-entry\\"]\'\\n                    )\\n                entry_body = content.find_element_by_xpath(\'//div[@class=\\"entry-body entry-inset\\"]\')\\n            except:\\n                print(f\\"No activity with id = {id} found\\")\\n                continue\\n\\n        # get run title\\n        title = entry_body.find_elements_by_css_selector(\\"a\\")[0].text\\n        # some runs have no description\\n        if len(entry_body.find_elements_by_css_selector(\\"a\\")) > 1:\\n            description = entry_body.find_elements_by_css_selector(\\"a\\")[1].text\\n        else:\\n            description = \\"\\"\\n        # remove emojis from title and description as attackpoint cant deal with them\\n        title = remove_emoji(title)\\n        description = remove_emoji(description)\\n        # get run distance, pace, and time\\n        distance = entry_body.find_elements_by_css_selector(\\"b\\")[0].text\\n        pace = entry_body.find_elements_by_css_selector(\\"b\\")[1].text\\n        time = entry_body.find_elements_by_css_selector(\\"b\\")[2].text\\n\\n        # store results in a dictionary\\n        if title:\\n           strava_activities[f\\"activity_{i}\\"] = {\\n               \\"title\\": title,\\n               \\"description\\": description,\\n               \\"distance\\": distance,\\n               \\"time\\": time,\\n               \\"pace\\": pace,\\n           }\\n        # quit driver\\n        driver.quit()\\n        return strava_activities\\n\\n# check\\nif __name__ == \\"__main__\\":\\n    activities = fetch_strava_activities()\\n    print(activities)\\n```\\n\\n- Now we have all my most recent activities (in order of completion!) along with the activities details: time, title, description, distance, and pace. All that is left to do now is add the title and descriptions to my Attackpoint page, although there are a few minor subtleties. First of all, if any activities have descriptions already I want to leave them alone. Secondly, if any of the Strava titles are the default i.e. haven\'t been changed, I also want to leave the post alone as I haven\'t gotten around to name the Strava run yet. As with before, some code details have been left out for clarity.\\n\\n```py title=\\"/src/attackpoint_selenium.py\\"\\nfrom selenium import webdriver\\nfrom webdriver_manager.chrome import ChromeDriverManager\\n\\nimport config\\nfrom utils import format_time, get_pace, get_dates\\nfrom stravaSelenium import fetch_strava_activities\\n\\n\\ndef attackpoint_login():\\n    # similar to strava login\\n    ...\\n    return driver\\n\\ndef fetch_attackpoint_activities(days=3):\\n    ...\\n    return ap_activities\\n\\ndef update_description(days=2):\\n    # fetch recent activities from strava\\n    strava = fetch_strava_activities(num_of_activities=int(days * 4))\\n    # fetch recent activities from attackpoint\\n    attackpoint = fetch_attackpoint_activities(days=days)\\n\\n    # get a list of activity descriptions to post on attackpoint\\n    descriptions = []\\n    for i in range(0, min(len(strava), len(attackpoint))):\\n        # get activity titles and descriptions from the strava data collected\\n        activity_title = strava[f\\"activity_{i}\\"][\\"title\\"]\\n        activity_description = strava[f\\"activity_{i}\\"][\\"description\\"]\\n        ...\\n        descriptions.append(activity_post)\\n\\n    # counters to keep track of things\\n    counter_activities = -1\\n    activities_edited = 0\\n    # if these titles are on strava then dont update activity description on ap\\n    strava_default_titles = [\\"Morning Run\\", ...]\\n    # loop through days\\n    for day in get_dates(days=days):\\n        # go onto the days activity\\n        try:\\n            check_activity_date = driver.find_element_by_xpath(\\n                f\'//a[@href=\\"/viewlog.jsp/user_13190/period-1/enddate-{day}\\"]\'\\n            ).click()\\n        except:\\n            continue\\n        edit_button = driver.find_elements_by_xpath(\'//*[@title=\\"Edit this entry\\"]\')\\n        # loop over each activity on given day\\n        for i, item in enumerate(edit_button):\\n            edit_button = driver.find_elements_by_xpath(\'//*[@title=\\"Edit this entry\\"]\')\\n            # reverse edit_button to get activities in order\\n            edit_button = edit_button[::-1]\\n            edit_button[i].click()\\n            distance = driver.find_element_by_id(\\"distance\\").get_attribute(\\"value\\")\\n            # if not a run then ignore\\n            if not distance:\\n                driver.back()\\n            else:\\n                counter_activities += 1\\n                description = driver.find_element_by_class_name(\\n                    \\"logtextarea\\"\\n                ).get_attribute(\\"value\\")\\n                # if no description then ignore\\n                if description:\\n                    driver.back()\\n                # if there is no description then we can add one!\\n                if not description:\\n                    strava_title = strava[f\\"activity_{counter_activities}\\"][\\"title\\"]\\n                    # if title is not on default strava titles\\n                    if strava_title not in strava_default_titles:\\n                        activities_edited += 1\\n                        text_input = driver.find_element_by_class_name(\\"logtextarea\\")\\n                        text_input.send_keys(descriptions[counter_activities])\\n                        submit = driver.find_element_by_xpath(\\n                            \\"//input[@value=\'Submit\']\\"\\n                        ).click()\\n                    else:\\n                        driver.back()\\n        driver.get(\\"https://www.attackpoint.org/log.jsp/user_13190\\")\\n    driver.quit()\\n    print(f\\"\\\\n{activities_edited} activity descriptions posted\\")\\n\\n\\nif __name__ == \\"__main__\\":\\n    update_description()\\n```\\n\\nAnd we are done, if I run `attackpointSelenium.py` in the terminal it will add any missing descriptions to my Attackpoint!\\n\\n## Making The Automatic Fully Automatic\\n\\nObviously running a line of code once or twice a day is WAY too much effort so the last thing that was left to do was deploy the code online somewhere so that I was completely out of the loop. Following this excellent [tutorial](https://medium.com/analytics-vidhya/schedule-a-python-script-on-heroku-a978b2f91ca8) I was able to deploy my code to [Heroku](https://id.heroku.com/login) and schedule it so that my code checks to see if my Attackpoint needs updating every 10 minutes! And best of all, it\'s completely free.\\n\\n:::info\\nTL;DR wrote code to automatically post information from one website onto the corresponding section of another.\\n:::\\n\\nCode: https://github.com/jackmleitch/strava2ap"},{"id":"Project-Auto","metadata":{"permalink":"/blog/Project-Auto","source":"@site/blog/2020-12-21-ProjectAuto.md","title":"Automating the Setup of my Data Science Projects","description":"Using Python to automate the process of setting up new project directories and making the first commit to a new repository in Github","date":"2020-12-21T00:00:00.000Z","formattedDate":"December 21, 2020","tags":[{"label":"Python","permalink":"/blog/tags/python"},{"label":"Automation","permalink":"/blog/tags/automation"},{"label":"Github","permalink":"/blog/tags/github"}],"readingTime":4.84,"truncated":true,"authors":[{"name":"Jack Leitch","title":"Machine Learning Engineer","url":"https://github.com/jackmleitch","imageURL":"https://github.com/jackmleitch.png","key":"jack"}],"frontMatter":{"slug":"Project-Auto","title":"Automating the Setup of my Data Science Projects","tags":["Python","Automation","Github"],"authors":"jack"},"prevItem":{"title":"Automating Mundane Web-Based Tasks With Selenium and Heroku","permalink":"/blog/Strava-AP"},"nextItem":{"title":"Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku","permalink":"/blog/Recipe-Recommendation2"}},"content":"**Using Python to automate the process of setting up new project directories and making the first commit to a new repository in Github**\\n\\nI find myself doing the same thing over and over again when starting a new data science project. As well as it being extremely tedious (and frustrating when I can\u2019t remember how to use git\u2026), it\'s also just a waste of time. Inspired by the YouTuber [Kalle Hallden](https://www.youtube.com/channel/UCWr0mx597DnSGLFk1WfvSkQ), I decided to try and automate this process using Python.\\n\\n\x3c!--truncate--\x3e\\n\\nAs discussed in one of my previous blog posts, [Organizing machine learning projects](https://towardsdatascience.com/organizing-machine-learning-projects-e4f86f9fdd9c), whenever I start a new project I like to use the same file system. Putting everything into folders named src, input, models, notebooks, and notes make it super easy to find the files I\'m looking for lending itself nicely to code reproducibility. An example file structure looks like this:\\n\\n![alt](./images/ProjectAuto/folder.png)\\n\\nThe first thing I did was make a checklist of what I wanted my custom bash function to do:\\n\\n- Create a folder with the project name inside my Projects folder\\n\\n- Create folders inside this folder named: `src`, `input`, `models`, `notebooks`, `notes`\\n\\n- Use Github API to create a new repository with the project name\\n\\n- Add a `README.md`, a `.gitignore`, and a `notes.txt`\\n\\n- Do the standard git stuff \u2014 `init`, `add`, `commit`, `remote`, and `push`\\n\\n## The Build\\n\\nThis turned out to be a relatively simple build and the only packages that I need to install were **PyGithub** to use the Github API, and **python-dotenv** to make use of environment variables.\\n\\n```py\\npip install PyGithub\\npip install python-dotenv\\n```\\n\\nInside a `.env` file, I added the hardcoded stuff and my personal Github information. We can access our Github account using the API with our username and password but instead, I generated a personal token [here](https://github.com/settings/tokens). This is more secure and was actually easier to do.\\n\\nI then wrote a function `create.py` which creates the file system described above with the main directory being the project title. It also uses the Github API to create a repository on Github with the project name.\\n\\n```py title=\\"/src/create.py\\"\\nimport sys\\nimport os\\nfrom dotenv import load_dotenv\\nfrom github import Github\\n\\nload_dotenv()\\n\\n# Get data from a .env file thats in .gitignore\\npath = os.getenv(\\"FILEPATH\\")\\n# Access to token generated from github\\ntoken = os.getenv(\\"TOKEN\\")\\nsubFolders = [\'input\', \'src\', \'models\', \'notebooks\', \'notes\']\\n\\n\\ndef create():\\n    # Extract project name from the command line\\n    folderName = str(sys.argv[1])\\n    # Make directory in my files\\n    os.makedirs(path+str(folderName))\\n    # Adds in sub-directories of src, input, ...\\n    for i in range(0, len(subFolders)):\\n        subPath = str(path) + str(folderName) + \'/\' + str(subFolders[i])\\n        os.makedirs(subPath)\\n\\n    # Uses Githubs API to create repository\\n    user = Github(token).get_user()\\n    repo = user.create_repo(folderName)\\n    print(f\\"Succesfully created repository {folderName}\\")\\n\\nif __name__ == \\"__main__\\":\\n    create()\\n```\\n\\n`sys.argv` is the list of command-line arguments passed to the Python program. So `sys.argv[1]` gives us the project name if we, for example, write `python create.py ProjectName` in the terminal.\\n\\nI then use my personal Github token (stored in .env) to access my Github account and create a new repository named ProjectName (or whatever you write after you call `create.py`).\\n\\nThe next step was to write a shell script to call this function and then initialize the git repository.\\n\\n```sh title=\\"/src/.create.sh\\"\\n#!/bin/bash\\n\\n# . means to source the contents of this file into the current shell\\n\\nfunction create() {\\n    cd \'/Users/Jack/Documents/Projects/project_auto/\'\\n    source .env\\n    # $1 is the first positional argument in the script\\n    python create.py $1\\n\\n    # Add notes.txt file to notes folder\\n    cd $FILEPATH$1/notes/\\n    echo Notes > notes.txt\\n\\n    # The standard git stuff\\n    cd $FILEPATH$1\\n    echo \'notes/\' >> .gitignore\\n    echo \'__pycache__/\' >> .gitignore\\n    touch README.md\\n    git init -b main\\n    git add .\\n    git commit -m \\"Project creation\\"\\n    git remote add origin https://github.com/$USERNAME/$1\\n    git push -u origin main\\n}\\n```\\n\\nThis **create** function is the end product. It first sets the directory to the home of this project and calls the `create.py` function \u2014 the $1 here refers to the first argument after the terminal function. For example, if we run `create MNIST` in the terminal, this function will subsequently run `create.py MNIST` and create a project titled MNIST.\\n\\nA `notes.txt` file is then added to the notes folder, this is then added, along with the pycache, to the `.gitignore` file. After this, a `README.md` is created and the whole directory is then committed to our newly created repository. Finally, the remote is added to the directory and it is all pushed to Github.\\n\\nThe last piece of the puzzle was to add this `.create.sh` script to the `bash_profile`, this is the initialization file for configuring the user environment. To put it simply, the create function will now be loaded into the terminals \u2018library\u2019. So whenever I now load up my terminal, I can easily just call on the create function. On macOS we can do this by:\\n\\n```sh\\ncd\\nnano .bash_profile\\n```\\n\\nThis then opens up the `bash_profile`. We can then add the following line of code to the profile.\\n\\n```sh\\nsource /PATH_TO_THE_FILE/.create.sh\\n```\\n\\nLastly, to update the new changes we just run the same line again in the terminal.\\n\\n## Testing Everything Out\\n\\nLet\u2019s test our new found automatic abilities out! We can run the following in the terminal:\\n\\n```sh\\ncreate PneumothoraxSegmentation\\n```\\n\\nIn our projects folder, we can now see a new file!\\n\\n![alt](./images/ProjectAuto/files.png)\\n\\nAnd on my Github page, I now have a new repository!\\n\\n![alt](./images/ProjectAuto/github.png)\\n\\nJob done. This was a nice quick and easy automation that is going to make my life (and hopefully yours) a lot easier. With two simple scripts, we were able to fully automate the mundane tasks involved in creating a new data science project. I hope this post inspires you to start automating more of your data science workflow!\\n\\nThanks for reading and I hope you enjoyed it."},{"id":"Recipe-Recommendation2","metadata":{"permalink":"/blog/Recipe-Recommendation2","source":"@site/blog/2020-12-08-Recipe2.md","title":"Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku","description":"The journey to my first \u2018full-stack\u2019 data science project (part 2) \u2014 building and deployment","date":"2020-12-08T00:00:00.000Z","formattedDate":"December 8, 2020","tags":[{"label":"Python","permalink":"/blog/tags/python"},{"label":"NLP","permalink":"/blog/tags/nlp"},{"label":"Recommendation System","permalink":"/blog/tags/recommendation-system"},{"label":"API","permalink":"/blog/tags/api"}],"readingTime":14.03,"truncated":true,"authors":[{"name":"Jack Leitch","title":"Machine Learning Engineer","url":"https://github.com/jackmleitch","imageURL":"https://github.com/jackmleitch.png","key":"jack"}],"frontMatter":{"slug":"Recipe-Recommendation2","title":"Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku","tags":["Python","NLP","Recommendation System","API"],"authors":"jack"},"prevItem":{"title":"Automating the Setup of my Data Science Projects","permalink":"/blog/Project-Auto"},"nextItem":{"title":"Organizing Machine Learning Projects","permalink":"/blog/OrganizingML"}},"content":"**The journey to my first \u2018full-stack\u2019 data science project (part 2) \u2014 building and deployment**\\n\\nSo\u2026 the idea: Given a list of ingredients, what are different recipes I can make? That is, what recipes can I make with the food I have in my apartment?\\n\\n\x3c!--truncate--\x3e\\n\\nFirst things first, if you would like to see my API in action (or use it!) then please do so by following (I have updated the app and release a new blog on it soon):\\n\\nhttps://share.streamlit.io/jackmleitch/whatscooking-deployment/streamlit.py\\n\\nIn my first blog post on this project, I walked through how I scraped the data for this project. The data being cooking recipes and the corresponding ingredients. Since then I have added more recipes so we now have a total of 4647. Please feel free to make use of this dataset yourself, you can find it on my [Github](https://github.com/jackmleitch/Whatscooking-).\\n\\nThis post will be focused on preprocessing the data, building the recommendation system, and then finally deploying the model using Flask and Heroku.\\n\\nThe process to build the recommendation system is as follows:\\n![flowchart](./images/RecipeRecommendation/flowchart.png)\\n\\nWe start by cleaning and parsing the dataset, then we extract the numerical features from the data, from here we can apply a similarity function to find the similarity between **ingredients for known recipes** and the **ingredients given by the end-user**. Finally, we can get the top-recommended recipes according to the similarity score.\\n\\nUnlike the first article in this series, this will not be a tutorial of the tools I used, but it will describe how I built the system and why I made the decisions I did along the way. Although, the code comments do a good job of explaining things in my opinion \u2014 so check them out if you\u2019re curious. As with most projects, my aim was to create the simplest model I could to get the job done to the standard I desired.\\n\\n## Preprocessing and Parsing of Ingredients\\n\\nTo understand the task at hand here, let\'s look at an example. The delicious \u2018Gennaro\u2019s classic spaghetti carbonara\u2019 recipe found on Jamie Oliver\u2019s [website](https://www.jamieoliver.com/recipes/pasta-recipes/gennaro-s-classic-spaghetti-carbonara/) requires the ingredients:\\n\\n- 3 large free-range egg yolks\\n\\n- 40 g Parmesan cheese, plus extra to serve\\n\\n- 1 x 150 g piece of higher-welfare pancetta\\n\\n- 200 g dried spaghetti\\n\\n- 1 clove of garlic\\n\\n- extra virgin olive oil\\n\\nThere is a lot of redundant information here; for example, weights and measures are not going to add value to the vector encodings of the recipe. If anything, it is going to make distinguishing between recipes more difficult. So we need to get rid of those. A quick google search led me to a [Wikipedia page](https://en.wikibooks.org/wiki/Cookbook:Units_of_measurement) containing a list of standard cooking metrics e.g. clove, gram (g), teaspoon, etc. Removing all these words in my ingredient parser worked really well.\\n\\nWe also want to remove stop words from our ingredients. In NLP \u2018stop words\u2019 refer to the most common words in a language. For example, the sentence \u2018learning about what stop words are\u2019, becomes, \u2018learning stop words\u2019. NLTK provides us with an easy way to remove (most of) these words.\\n\\nThere are also other words in the ingredients that are useless to us \u2014 these are the words that are very common among recipes. Oil, for example, is used in most recipes and will provide little to no distinguishing power between recipes. Also, most people have oil in their homes so having to write oil every time you use the API is cumbersome and pointless. There are advanced NLP techniques, for example using [conditional random fields](https://open.blogs.nytimes.com/2015/04/09/extracting-structured-data-from-recipes-using-conditional-random-fields/) (a class of statistical modeling), that can calculate the probability that a word is an ingredient, as opposed to a measure, texture, or another type of word that surrounds the ingredient word. But, simply removing the most common words seemed to be very effective, so I did this. Occam\u2019s razor and all that jazz\u2026 To get the most common words we can do the following:\\n\\n```py\\nimport nltk\\nvocabulary = nltk.FreqDist()\\n# This was done once I had already preprocessed the ingredients\\nfor ingredients in recipe_df[\'ingredients\']:\\n    ingredients = ingredients.split()\\n    vocabulary.update(ingredients)\\nfor word, frequency in vocabulary.most_common(200):\\n    print(f\'{word};{frequency}\')\\n```\\n\\nWe have one final obstacle to get over, however. When we try to remove these \u2018junk\u2019 words from our ingredient list, what happens when we have different variations of the same word? What happens if we want to remove every occurrence of the word \u2018pound\u2019 but the recipe ingredients say \u2018pounds\u2019? Luckily there is a pretty trivial workaround: lemmatization and stemming. Stemming and lemmatization both generate the root form of inflected words \u2014 the difference is that a stem might not be an actual word whereas, a lemma is an actual language word. Although lemmatization is often slower, I chose to use this technique as I know the ingredients will be actual words which is useful for debugging and visualization (the results turned out to be practically identical using stemming instead). When the user feeds ingredients to the API we also lemmatize those words as the lemmatized words are the words with the embeddings.\\n\\nWe can put this all together in a function, `ingredient_parser`, along with some other standard preprocessing: getting rid of punctuation, removing accents, making everything lowercase, getting rid of Unicode.\\n\\n```py title=\\"/src/parse_ingredients.py\\"\\ndef ingredient_parser(ingredients):\\n    # measures and common words (already lemmatized)\\n    measures = [\'teaspoon\', \'t\', \'tsp.\', \'tablespoon\', \'T\', ...]\\n    words_to_remove = [\'fresh\', \'oil\', \'a\', \'red\', \'bunch\', ...]\\n    # Turn ingredient list from string into a list\\n    if isinstance(ingredients, list):\\n       ingredients = ingredients\\n    else:\\n       ingredients = ast.literal_eval(ingredients)\\n    # We first get rid of all the punctuation\\n    translator = str.maketrans(\'\', \'\', string.punctuation)\\n    # initialize nltk\'s lemmatizer\\n    lemmatizer = WordNetLemmatizer()\\n    ingred_list = []\\n    for i in ingredients:\\n        i.translate(translator)\\n        # We split up with hyphens as well as spaces\\n        items = re.split(\' |-\', i)\\n        # Get rid of words containing non alphabet letters\\n        items = [word for word in items if word.isalpha()]\\n        # Turn everything to lowercase\\n        items = [word.lower() for word in items]\\n        # remove accents\\n        items = [unidecode.unidecode(word) for word in items]\\n        # Lemmatize words so we can compare words to measuring words\\n        items = [lemmatizer.lemmatize(word) for word in items]\\n        # get rid of stop words\\n        stop_words = set(corpus.stopwords.words(\'english\'))\\n        items = [word for word in items if word not in stop_words]\\n        # Gets rid of measuring words/phrases, e.g. heaped teaspoon\\n        items = [word for word in items if word not in measures]\\n        # Get rid of common easy words\\n        items = [word for word in items if word not in words_to_remove]\\n        if items:\\n           ingred_list.append(\' \'.join(items))\\n           ingred_list = \' \'.join(ingred_list)\\n    return ingred_list\\n```\\n\\nWhen parsing the ingredients for \u2018Gennaro\u2019s classic spaghetti carbonara\u2019 we get: egg yolk parmesan cheese pancetta spaghetti garlic. Perfect, that works fantastically!\\n\\nUsing lambda functions, it\u2019s easy to parse all of the ingredients.\\n\\n```py\\nrecipe_df = pd.read_csv(config.RECIPES_PATH)\\nrecipe_df[\'ingredients_parsed\'] = recipe_df[\'ingredients\'].apply(lambda x: ingredient_parser(x))\\ndf = recipe_df.dropna()\\ndf.to_csv(config.PARSED_PATH, index=False)\\n```\\n\\n## Extracting Features\\n\\nWe now need to encode each document (recipe ingredients), and as before, simple models worked really well. One of the most basic models that should always be tried when doing NLP is **bag of words**. This entails creating a huge sparse matrix that stores counts of all the words in our corpus (all of the documents i.e. all of the ingredients for every recipe). Scikit-learn\u2019s CountVectorizer has a nice implementation for this.\\n\\nBag of words performed ok but **TF-IDF** (term frequencies-inverse document frequency) marginally out-performed it, so we opted for this instead. I\u2019m not going to go into the details of how tf-idf works as it is not relevant to the blog but it basically does what it says on the tin. As per usual, Scikit-learn has a lovely implementation of this: TfidfVectorizer. I then saved the model and encodings using pickle, as retraining the model every time the API is used would make it painfully slow.\\n\\n```py title=\\"/src/feature_extraction.py\\"\\nimport pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport pickle\\nimport config\\n# load in parsed recipe dataset\\ndf_recipes = pd.read_csv(config.PARSED_PATH)\\n# Tfidf needs unicode or string types\\ndf_recipes[\'ingredients_parsed\'] =       df_recipes.ingredients_parsed.values.astype(\'U\')\\n# TF-IDF feature extractor\\ntfidf = TfidfVectorizer()\\ntfidf.fit(df_recipes[\'ingredients_parsed\'])\\ntfidf_recipe = tfidf.transform(df_recipes[\'ingredients_parsed\'])\\n# save the tfidf model and encodings\\nwith open(config.TFIDF_MODEL_PATH, \\"wb\\") as f:\\n     pickle.dump(tfidf, f)\\nwith open(config.TFIDF_ENCODING_PATH, \\"wb\\") as f:\\n     pickle.dump(tfidf_recipe, f)\\n```\\n\\n## Recommendation System\\n\\nThis application simply consists of text data and there is no kind of ratings available, **so we can not use** matrix decomposition methods, such as SVD and correlation coefficient-based methods.\\n\\nWe use content-based filtering which enables us to recommend recipes to people based on the attributes (ingredients) the user provides. To measure the similarity between documents I used **Cosine similarity**. I also tried using Spacy and KNN but cosine similarity won in terms of performance (and ease).\\n\\nMathematically, cosine similarity measures the cosine of the angle between two vectors. For the mathematically inclined out there, this is the same as the inner product of the same vectors normalized to both have length 1. I chose to use this measure of similarity as even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. For example, if the user inputs a lot of ingredients and only the first half match with a recipe, we should, in theory, still get a good recipe match. In cosine similarity, the smaller the angle, the higher the cosine similarity: so we are trying to maximize this score.\\n\\n```py\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport pickle\\nimport config\\nfrom ingredient_parser import ingredient_parser\\n# load in tdidf model and encodings\\nwith open(config.TFIDF_ENCODING_PATH, \'rb\') as f:\\n     tfidf_encodings = pickle.load(f)\\nwith open(config.TFIDF_MODEL_PATH, \\"rb\\") as f:\\n     tfidf = pickle.load(f)\\n# parse the ingredients using my ingredient_parser\\ntry:\\n    ingredients_parsed = ingredient_parser(ingredients)\\nexcept:\\n    ingredients_parsed = ingredient_parser([ingredients])\\n# use our pretrained tfidf model to encode our input ingredients\\ningredients_tfidf = tfidf.transform([ingredients_parsed])\\n# calculate cosine similarity between actual recipe ingreds and test ingreds\\ncos_sim = map(lambda x: cosine_similarity(ingredients_tfidf, x), tfidf_encodings)\\nscores = list(cos_sim)\\n```\\n\\nI then wrote a function, `get_recommendations`, to rank these scores and output a pandas data frame containing all the details of the top **N** recipes.\\n\\n```py title=\\"/src/recommendation_system.py\\"\\ndef get_recommendations(N, scores):\\n    # load in recipe dataset\\n    df_recipes = pd.read_csv(config.PARSED_PATH)\\n    # order the scores with and filter to get the highest N scores\\n    top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:N]\\n    # create dataframe to load in recommendations\\n    recommendation = pd.DataFrame(columns = [\'recipe\', \'ingredients\', \'score\', \'url\'])\\n    count = 0\\n    for i in top:\\n        recommendation.at[count, \'recipe\'] = title_parser(df_recipes[\'recipe_name\'][i])\\n\\n        recommendation.at[count, \'ingredients\'] = ingredient_parser_final(df_recipes[\'ingredients\'][i])\\n\\n        recommendation.at[count, \'url\'] = df_recipes[\'recipe_urls\'][i]\\n        recommendation.at[count, \'score\'] = \\"{:.3f}\\".format(float(scores[i]))\\n\\n        count += 1\\n    return recommendation\\n```\\n\\nIt\u2019s worth noting that there was no concrete way of assessing the performance of the model so I had to evaluate the recommendations manually. To be honest though, this ended up actually being quite fun\u2026 I also discovered a tonne of new recipes! It\u2019s comforting to see that when we put in the ingredients (before they are parsed of course!) for our beloved Gennaro\u2019s classic spaghetti carbonara we get the correct recommendation with a score of 1.\\n\\nAs of right now, a few items in my fridge/cupboards are: ground beef, pasta, spaghetti, tomato pasta sauce, bacon, onion, zucchini, and, cheese. Our newly built recommendation system suggests we make:\\n\\n```py\\n{ \\"ingredients\\" :\\n    \\"1 (15 ounce) can tomato sauce, 1 (8 ounce) package uncooked pasta shells, 1 large zucchini - peeled and cubed, 1 teaspoon dried basil, 1 teaspoon dried oregano, 1/2 cup white sugar, 1/2 medium onion, finely chopped, 1/4 cup grated Romano cheese, 1/4 cup olive oil, 1/8 teaspoon crushed red pepper flakes, 2 cups water, 3 cloves garlic, minced\\",\\n\\n  \\"recipe\\" : \\"Zucchini and Shells\\",\\n\\n  \\"score: \\"0.760\\",\\n\\n  \\"url\\":\\"https://www.allrecipes.com/recipe/88377/zucchini-and-shells/\\"}\\n```\\n\\nSounds good to me \u2014 best get cooking!\\n\\n## Create an API to Deploy the Model\\n\\n### Deployment using Flask\\n\\nSo how can I serve this model I have built to an end-user? I created an API that can be used to input ingredients and in turn, it outputs the top 5 recipe recommendations based on those ingredients. To build this API I used Flask, which is a micro web service framework.\\n\\n```py title=\\"/src/app.py\\"\\nfrom flask import Flask, jsonify, request\\nimport json, requests, pickle\\nimport pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom ingredient_parser import ingredient_parser\\nimport config, rec_sys\\napp = Flask(__name__)\\n@app.route(\'/\', methods=[\\"GET\\"])\\ndef hello():\\n    # This is the homepage of our API.\\n    # It can be accessed by http://127.0.0.1:5000/\\n    return HELLO_HTML\\nHELLO_HTML = \\"\\"\\"\\n     <html><body>\\n         <h1>Welcome to my api: Whatscooking!</h1>\\n         <p>Please add some ingredients to the url to receive recipe recommendations.\\n            You can do this by appending \\"/recipe?ingredients= Pasta Tomato ...\\" to the current url.\\n         <br>Click <a href=\\"/recipe?ingredients= pasta tomato onion\\">here</a> for an example when using the ingredients: pasta, tomato and onion.\\n     </body></html>\\n     \\"\\"\\"\\n@app.route(\'/recipe\', methods=[\\"GET\\"])\\ndef recommend_recipe():\\n    # This is our endpoint. It can be accessed by http://127.0.0.1:5000/recipe\\n    ingredients = request.args.get(\'ingredients\')\\n    recipe = rec_sys.RecSys(ingredients)\\n\\n    # We need to turn output into JSON.\\n    response = {}\\n    count = 0\\n    for index, row in recipe.iterrows():\\n        response[count] = {\\n                            \'recipe\': str(row[\'recipe\']),\\n                            \'score\': str(row[\'score\']),\\n                            \'ingredients\': str(row[\'ingredients\']),\\n                            \'url\': str(row[\'url\'])\\n                          }\\n        count += 1\\n    return jsonify(response)\\nif __name__ == \\"__main__\\":\\n   app.run(host=\\"0.0.0.0\\", debug=True)\\n```\\n\\nWe can start this API by running the command `python src/app.py` and the API will start on localhost on port 5000. We can access recipe recommendations for the inputs pasta, tomato, and onion by visiting http://192.168.1.51:5000/recipe?ingredients=%20pasta%20tomato%20onion in our browser.\\n\\n![api_call](./images/RecipeRecommendation/api_call.png)\\n![api_response](./images/RecipeRecommendation/api_response.png)\\n\\n### Deploying Flask API to Heroku \u2014 It\u2019s Time To Go Online\\n\\nDeploying a Flask API to Heroku is extremely easy if you use Github! First, I created a file in my project folder called Procfile, with no extension. All you need to put inside this file is:\\n\\n`web: gunicorn app:app`\\n\\nThe next step was to create a file called `requirements.txt` which consists of all of the python libraries that I used for this project. If you\u2019re working in a virtual environment (I use conda) then it\u2019s as easy as `pip freeze > requirements.txt` \u2014 make sure you\'re in the correct working directory otherwise it\u2019ll save the file somewhere else.\\n\\nAll I had to do now was commit the changes to [my Github repository](https://github.com/jackmleitch/whatscooking-deployment) and follow the deployment steps on https://dashboard.heroku.com/apps. If you would like to try out or use my API, please do by visiting:\\n\\n- https://whats-cooking-recommendation.herokuapp.com/- if you are in USA\\n\\n- https://whatscooking-deployment.herokuapp.com/- if you are in Europe\\n\\n- Either should work if you are elsewhere, it\u2019ll just be a tad slower\\n\\n### Docker\\n\\nWe have now reached the stage where I am happy with the model I have built, so I want to be able to distribute my model to others so that they can use it too. I have uploaded my whole project to [Github](https://github.com/jackmleitch/Whatscooking-) but that\'s not quite enough. Just because the code works on my computer does not mean it\'s going to work on someone else\'s computer, this could be because of many reasons. It would be fantastic if when I distribute my code, I replicate my computer so that I know it is going to work. One of the most popular ways of doing this nowadays is by using [Docker Containers](https://github.com/jackmleitch/Whatscooking-).\\n\\nThe first thing I did was create a **docker file** called Dockerfile (it doesn\'t have an extension). Simply put, the docker file tells us how to build our environment and contains all the commands a user could call in the command line to assemble the image.\\n\\n```py title=\\"Dockerfile\\"\\n# Include where we get the image from (operating system)\\nFROM ubuntu:18.04\\nMAINTAINER Jack Leitch \'jackmleitch@gmail.com\'\\n# We cannot press Y so we do it automatically.\\nRUN apt-get update && apt-get install -y \\\\\\n    git \\\\\\n    curl \\\\\\n    ca-certificates \\\\\\n    python3 \\\\\\n    python3-pip \\\\\\n    sudo \\\\\\n    && rm -rf /var/lib/apt/lists/*\\n# Set working directory\\nWORKDIR /app\\n# Copy everything in currect directory into the app directory.\\nADD . /app\\n# Install all of the requirements\\nRUN pip3 install -r requirements.txt\\n# Download wordnet as its used to lemmatize\\nRUN python3 -c \\"import nltk; nltk.download(\'wordnet\')\\"\\n# CMD executes once the container is started\\nCMD [\\"python3\\", \\"app.py\\"]\\n```\\n\\nOnce I created the docker file, I then needed to build my container \u2014 and that\'s very easy. Side note: if you do this, make sure all your file paths (I keep mine in a `config.py` file) aren\u2019t specific to your computer because docker is like a virtual machine and contains its own file system e.g. you can put `./input/df_recipes.csv` instead.\\n\\n`docker build -f Dockerfile -t whatscooking:api`\\n\\nTo start our API, on any machine (!), all we have to do now is (assuming you have downloaded the docker container):\\n\\n`docker run -p 5000:5000 -d whatscooking:api`\\n\\nIf you want to check out the container yourself, here is a link to my [Docker Hub](https://hub.docker.com/repository/docker/jackmleitch/whatscooking). You can pull the image by:\\n\\n`docker pull jackmleitch/whatscooking:api`\\n\\nThe plan moving forward from here is to build a nicer interface to the API using Streamlit."},{"id":"OrganizingML","metadata":{"permalink":"/blog/OrganizingML","source":"@site/blog/2020-11-17-OrganizingML.md","title":"Organizing Machine Learning Projects","description":"A guide to organizing your machine learning projects","date":"2020-11-17T00:00:00.000Z","formattedDate":"November 17, 2020","tags":[{"label":"Python","permalink":"/blog/tags/python"}],"readingTime":8.69,"truncated":true,"authors":[{"name":"Jack Leitch","title":"Machine Learning Engineer","url":"https://github.com/jackmleitch","imageURL":"https://github.com/jackmleitch.png","key":"jack"}],"frontMatter":{"slug":"OrganizingML","title":"Organizing Machine Learning Projects","tags":["Python"],"authors":"jack"},"prevItem":{"title":"Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku","permalink":"/blog/Recipe-Recommendation2"},"nextItem":{"title":"The ins and outs of Gradient Descent","permalink":"/blog/Gradient-Descent"}},"content":"**A guide to organizing your machine learning projects**\\n\\nI just want to start with a brief disclaimer. This is how I personally organize my projects and it\u2019s what works for me, that doesn\'t necessarily mean it will work for you. However, there is definitely something to be said about how good organization streamlines your workflow. Building my ML framework the way I do allows me to work in a very plug n\u2019 play way: I can train, change, adjust my models without making too many changes to my code.\\n\\n## Why is it important to organize a project?\\n\\n1. Productivity. If you have a well-organized project, with everything in the same directory, you don\'t waste time searching for files, datasets, codes, models, etc.\\n\\n2. Reproducibility. You\u2019ll find that a lot of your data science projects have at least some repetitively to them. For example, with the proper organization, you could easily go back and find/use the same script to split your data into folds.\\n\\n3. Understandability. A well-organized project can easily be understood by other data scientists when shared on Github.\\n\x3c!--truncate--\x3e\\n\\n## File structure\\n\\nThe first thing I do whenever I start a new project is to make a folder and name it something appropriate, for example, \u201cMNIST\u201d or \u201cdigit_recognition\u201d. Inside the main project folder, I always create the same subfolders: **notes**, **input**, **src**, **models**, **notebooks**. Don\u2019t forget to add a `README.md` file as well! As intuitive as this breakdown seems, it makes one hell of a difference in productivity.\\n\\nWhat do I put in each folder?\\n\\n- `notes`: I add any notes to this folder, this can be anything! URLs to useful webpages, chapters/pages of books, descriptions of variables, a rough outline of the task at hand, etc.\\n\\n- `input`: This folder contains all of the input files and data for the machine learning project. For example, it may contain CSV files, text embedding, or images (in another subfolder though), to list a few things.\\n\\n- `src`: Any `.py` file is kept in this folder. Simple.\\n\\n- `models`: We keep all of our trained models in here (the useful ones\u2026).\\n\\n- `notebooks`: We can store our Jupyter notebooks here (any `.ipynb` file). Note that I tend to only use notebooks for data exploration and plotting.\\n\\n## A (basic) project example\\n\\nWhen I build my projects I like to automate as much as possible. That is, I try to repeat myself as little as possible and I like to change things like models and hyperparameters with as little code as I can. Let\'s look to build a very simple model to classify the MNIST dataset. For those of you that have been living under a rock, this dataset is the de facto \u201chello world\u201d of computer vision. The data files `train.csv` and `test.csv` contain gray-scale images of hand-drawn digits, from zero through nine. Given the pixel intensity values (each column represents a pixel) of an image, we aim to identify which digit the image is. This is a supervised problem. Please note that the models I am creating are by no means the best for classifying this dataset, that isn\'t the point of this blog post.\\n\\nSo, how do we start? Well, as with most things data science, we first need to decide on a metric. By looking at a count plot (`sns.countplot()` (this was done in a Jupyter notebook in the notebooks folder!)) we can see that the distribution of labels is fairly uniform, so plain and simple accuracy should do the trick!\\n\\nThe next thing we need to do is create some cross-validation folds. The first script I added to my src folder was to do exactly this. `create_folds.py` reads `mnist_train.csv` from the input folder and creates a new file `mnist_train_folds.csv` (saved to the input folder) which has a new column, kfolds, containing fold numbers. As with most classification problems, I used stratified k-folds. To see the script, please check out my [Github page](https://github.com/jackmleitch/Almost-any-ML-problem/tree/master/MNIST).\\n\\nNow that we have decided on a metric and created folds, we can start making some basic models. With the aim of this post in mind, I decided to completely skip any kind of preprocessing (sorry!). What we are going to do is create a general python script to train our model(s), `train.py`, and then we will make some changes so that we hardcode as little as possible. The aim here is to be able to change a lot of things without changing much (if any) code. So, let\'s get cracking!\\n\\n```py title=\\"/src/train.py\\"\\nimport joblib\\nimport pandas as pd\\nfrom sklearn import metrics, tree\\ndef run(fold):\\n    # we first read in data with our folds\\n    df = pd.read_csv(\\"../input/mnist_train_folds.csv\\")\\n\\n    # we then split it into our training and validation data\\n    df_train = df[df.kfold != fold].reset_index(drop=True)\\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\\n    # drop target column (label) and turn to np array\\n    x_train = df_train.drop(\'label\', axis=1).values\\n    y_train = df_train.label.values\\n    # same for validation\\n    x_valid = df_valid.drop(\'label\', axis=1).values\\n    y_valid = df_valid.label.values\\n    # initialize simple decision tree classifier and fit data\\n    clf = tree.DecisionTreeClassifier()\\n    clf.fit(x_train, y_train)\\n    # create predictions for validation data\\n    preds = clf.predict(x_valid)\\n    # calculate and print accuracy\\n    score = metrics.accuracy_score(y_valid, preds)\\n    print(f\\"Fold={fold}, Accuracy={score}\\")\\n    # save the model (not very necessary for a smaller model though)\\n    joblib.dump(clf, f\\"../models/dt_{fold}.bin\\")\\nif __name__ == \\"__main__\\":\\n    for i in range(3):\\n        run(fold=i)\\n```\\n\\nWe can then run this script by calling `python src/train.py` in the terminal. This script will read our data, train a decision tree classifier, score our predictions, and save the model for each fold. Note that you will need to set the working directory. For example:\\n\\n```sh\\n> cd \\"/Users/Jack/Documents/MNIST/src\\"\\n> python train.py\\nFold=0, Accuracy=0.858\\nFold=1, Accuracy=0.860\\nFold=2, Accuracy=0.859\\n```\\n\\nWe can do a lot better than this... We have hardcoded the fold numbers, the training file, the output folder, the model, and the hyperparameters. We can change all of this.\\n\\nThe first easy thing we can do is create a `config.py` file with all of the training files and output folder.\\n\\n```py title=\\"/src/config.py\\"\\nTRAINING_FILE = \\"../input/mnist_train_folds.csv\\"\\nOUTPUT_PATH = \\"../models/\\"\\n```\\n\\nChanging the script to incorporate this is easy! The changes are in bold.\\n\\n```py title=\\"/src/train.py\\"\\nimport os\\nimport config\\nimport joblib\\nimport pandas as pd\\nfrom sklearn import metrics, tree\\ndef run(fold):\\n    # we first read in data with our folds\\n    df = pd.read_csv(config.TRAINING_FILE)\\n    .\\n    .\\n    .\\n    # save the model\\n    joblib.dump(\\n        clf,\\n        os.path.join(config.OUTPUT_PATH, f\\"../models/dt_{fold}.bin\\")\\n    )\\nif __name__ == \\"__main__\\":\\n    for i in range(3):\\n        run(fold=i)\\n```\\n\\nIn our script, we call the **run** function for each fold. When training bigger models this can be an issue as running multiple folds in the same script will keep increasing the memory consumption. This could potentially lead to the program crashing. We can get around this by using **argparse**. Argparse allows us to specify arguments in the command line which get parsed through to the script. Let\'s see how to implement this.\\n\\n```py title=\\"/src/train.py\\"\\nimport argparse\\n.\\n.\\n.\\nif __name__ == \\"__main__\\":\\n    # initialize Argument Parser\\n    parser = argparse.ArgumentParser()\\n\\n    # we can add any different arguments we want to parse\\n    parser.add_argument(\\"--fold\\", type=int)\\n    # we then read the arguments from the comand line\\n    args = parser.parse_args()\\n    # run the fold that we specified in the command line\\n    run(fold=args.fold)\\n```\\n\\nSo what have we done here? We have allowed us to specify the fold in the terminal.\\n\\n```sh\\n> python train.py --fold 3\\nFold=3, Accuracy=0.861\\n```\\n\\nWe can use argparse in an even more useful way though, we can change the model with this! We can create a new python script, `model_dispatcher.py`, that has a dictionary containing different models. In this dictionary, the keys are the names of the models and the values are the models themselves.\\n\\n```py title=\\"/src/model_dispatcher.py\\"\\nfrom sklearn import tree, ensemble, linear_model, svm\\nmodels = {\\n     \\"decision_tree_gini\\": tree.DecisionTreeClassifier(\\n         criterion=\\"gini\\"\\n     ),\\n     \\"decision_tree_entropy\\": tree.DecisionTreeClassifier(\\n         criterion=\'entropy\'\\n     ),\\n     \\"rf\\": ensemble.RandomForestClassifier(),\\n     \\"log_reg\\": linear_model.LogisticRegression(),\\n     \\"svc\\": svm.SVC(C=10, gamma=0.001, kernel=\\"rbf\\")\\n}\\n```\\n\\nWe can then add the following things to our code.\\n\\n```py title=\\"/src/train.py\\"\\nimport os, argparse, joblib\\nimport pandas as pd\\nfrom sklearn import metrics\\nimport config\\nimport model_dispatcher\\ndef run(fold, model):\\n    # we first read in data with our folds\\n    df = pd.read_csv(config.TRAINING_FILE)\\n    # we then split it into our training and validation data\\n    df_train = df[df.kfold != fold].reset_index(drop=True)\\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\\n    # drop target column (label) and turn to np array\\n    x_train = df_train.drop(\'label\', axis=1).values\\n    y_train = df_train.label.values\\n    x_valid = df_valid.drop(\'label\', axis=1).values\\n    y_valid = df_valid.label.values\\n    # fetch model from model dispatcher\\n    clf = model_dispatcher.models[model]\\n    # fit model on training data\\n    clf.fit(x_train, y_train)\\n    # create predictions for validation data\\n    preds = clf.predict(x_valid)\\n    # calculate and print accuracy\\n    score = metrics.accuracy_score(y_valid, preds)\\n    print(f\\"Model={model}, Fold={fold}, Accuracy={score}\\")\\n    # save the model\\n    joblib.dump(\\n        clf,\\n        os.path.join(config.OUTPUT_PATH, f\\"../models/dt_{fold}.bin\\")\\n    )\\nif __name__ == \\"__main__\\":\\n  # initialize ArgumentParser class of argparse\\n  parser = argparse.ArgumentParser()\\n\\n  # add different arguments and their types\\n  parser.add_argument(\'--fold\', type=int)\\n  parser.add_argument(\'--model\', type=str)\\n\\n  # read arguments from command line\\n  args = parser.parse_args()\\n  # run with the fold and model specified by command line arguments\\n  run(\\n      fold=args.fold,\\n      model=args.model\\n  )\\n```\\n\\nAnd that\'s it, now we can choose the fold and model in the terminal! What\'s great about this is that to try a new model/tweak hyperparameters, all we need to do is change our model dispatcher.\\n\\n```sh\\n> python train.py --fold 2 --model rf\\n  Model=rf, Fold=2, Accuracy=0.9658333333333333\\n```\\n\\nWe also aren\u2019t limited to just doing this. We can make dispatchers for loads of other things too: categorical encoders, feature selection, hyperparameter optimization, the list goes on!\\n\\nTo go even further, we can create a shell script to try a few different models from our dispatcher at once.\\n\\n```sh title=\\"/src/run.sh\\"\\n#!/bin/sh\\npython train.py --fold 0 --model rf\\npython train.py --fold 0 --model decision_tree_gini\\npython train.py --fold 0 --model log_reg\\npython train.py --fold 0 --model svc\\n```\\n\\nWhich, when run, gives:\\n\\n```sh\\n> sh run.sh\\nModel=rf, Fold=0, Accuracy=0.9661666666666666\\nModel=decision_tree_gini, Fold=0, Accuracy=0.8658333333333333\\nModel=log_reg, Fold=0, Accuracy=0.917\\nModel=svc, Fold=0, Accuracy=0.9671812654676666\\n```\\n\\nI first learned how to do all of this in Abhishek Thakur\u2019s (quadruple Kaggle Grand Master) book: Approaching (Almost) Any Machine Learning Problem. It\u2019s a fantastic and pragmatic exploration of data science problems. I would highly recommend it.\\n\\n## A word on Github\\n\\nOnce you\u2019ve finished up (or during!) a problem that you find interesting make sure to create a Github repository and upload your datasets, python scripts, models, Jupyter notebooks, R scripts, etc. Creating Github repositories to showcase your work is extremely important! It also means you have some version control and you can access your code at all times."},{"id":"Gradient-Descent","metadata":{"permalink":"/blog/Gradient-Descent","source":"@site/blog/2020-10-28-GradientDescent.md","title":"The ins and outs of Gradient Descent","description":"Optimizing your choice of optimizer","date":"2020-10-28T00:00:00.000Z","formattedDate":"October 28, 2020","tags":[{"label":"Optimization","permalink":"/blog/tags/optimization"},{"label":"Mathematics","permalink":"/blog/tags/mathematics"},{"label":"Python","permalink":"/blog/tags/python"}],"readingTime":11.67,"truncated":true,"authors":[{"name":"Jack Leitch","title":"Machine Learning Engineer","url":"https://github.com/jackmleitch","imageURL":"https://github.com/jackmleitch.png","key":"jack"}],"frontMatter":{"slug":"Gradient-Descent","title":"The ins and outs of Gradient Descent","tags":["Optimization","Mathematics","Python"],"authors":"jack"},"prevItem":{"title":"Organizing Machine Learning Projects","permalink":"/blog/OrganizingML"},"nextItem":{"title":"Using BeautifulSoup to Help Make Beautiful Soups","permalink":"/blog/Recipe-Recommendation1"}},"content":"**Optimizing your choice of optimizer**\\n\\n**Gradient descent** is an optimization algorithm used to minimize some cost function by iteratively moving in the direction of **steepest descent**. That is, moving in the direction which has the most negative gradient. In machine learning, we use gradient descent to continually tweak the parameters in our model in order to minimize a cost function. We start with some set of values for our model parameters (weights and biases in a neural network) and improve them slowly. In this blog post, we will start by exploring some basic optimizers commonly used in classical machine learning and then move on to some of the more popular algorithms used in Neural Networks and Deep Learning.\\n\\n\x3c!--truncate--\x3e\\n\\nImagine you\'re out for a run (or walk, whatever floats your boat) in the mountains and suddenly a thick mist comes in impairing your vision. A good strategy to get down the mountain is to feel the ground in every direction and take a step in the direction in which the ground is descending the fastest. Repeating this you should end up at the bottom of the mountain (although you might also end up at something that looks like the bottom that isn\'t \u2014 more on this later). This is exactly what gradient descent does: it measures the local gradient of the cost function **J(\u03c9)** (parametrized by model parameters \u03c9), and moves in the direction of descending gradient. Once we reach a gradient of zero, we have reached our minimum.\\n\\nSuppose now that the steepness of the hill isn\'t immediately obvious to you (maybe you\'re too cold to feel your feet, use your imagination!), so you have to use your phone\'s accelerometer to measure the gradient. It\'s also really cold out so to check your phone you have to stop running and take your gloves off. Therefore, you need to minimize the use of your phone if you hope to make it down anytime soon! So you need to choose the right frequency at which you should measure the steepness of the hill so as not to go off track and at the same time reach home before the sunset. This amount of time between checks is what is known as the **learning rate** i.e. the size of steps we take downhill. If our learning rate is too small, then the algorithm will take a long time to converge. But, if our learning rate is too high the algorithm can diverge and just past the minimum.\\n\\n![img1](./images/GradientDescent/image1.png)\\n\\nAn important point to note is that not all cost functions are convex (the MSE cost function for Linear Regression is convex, however). A function is convex if the line segment between any two points on the graph of the function lies above the graph between the two points. Convexity is nice as it implies that there is a single global minimum. Problems start to occur when the cost functions aren\u2019t convex: there may be ridges, plateaus, holes, etc. This makes convergence to the minimum difficult. Fortunately, there are techniques available to us to help combat some of these issues.\\n\\n![img2](./images/GradientDescent/image2.png)\\n\\n## Batch Gradient Descent\\n\\nThe vanilla of the gradient descent world. In batch gradient descent we use the full training set X at each gradient step. Because of this, when using large training sets it can be agonizingly slow.\\nWe start by finding the gradient vector of the cost function, we denote this by\\n\\n![img3](./images/GradientDescent/math1.png)\\n\\nHere, **J(\u03c9)** is the cost function parametrized by the model parameters **\u03c9**) and **\u2207** denotes the vector differential operator. Now we have the gradient vector, we just need it to go in the opposite direction (downhill). Then, multiplying by the learning rate **\u03b7** we get our one-step gradient descent update equation:\\n\\n![img3](./images/GradientDescent/math2.png)\\n\\nImplementation of batch gradient descent in python couldn\'t be simpler!\\n\\n```py title=\\"/src/batch_gradient_descent.py\\"\\nimport numpy as np\\neta = 0.01 # our chosen learning rate\\niterations = 10^3\\nn = 100\\ntheta = np.random.randn(d,1) # random initialization, with d being\\n# the dimension of theta\\nfor i in range(iterations):\\n    gradients = 2/n * X.T.dot(X.dot(theta) - y)\\n    theta = theta - eta * gradients\\n```\\n\\nWe can use a grid search to find a good learning rate if we so desire. To find the correct number of iterations it normally suffices to set the number of iterations to a very large number but then interrupt the algorithm when the gradient vector (gradients) becomes smaller than some pre-set tolerance (normally denoted **\u03f5**). This works because when the gradient vector becomes tiny, we are extremely close to our minimum.\\n\\nOne other important note is that when using Gradient Descent you should ensure that all of your features have a similar scale as otherwise, it will take much longer for the algorithm to converge \u2014 Sckikit-Learn\u2019s StandardScaler usually does the trick. What can we do to speed up our optimizer though? Enter Stochastic Gradient Descent.\\n\\n## Stochastic Gradient Descent\\n\\nStochastic GD takes the opposite extreme to batch GD, rather than using the whole training set at each step, it takes a random instance of the training set at each step and uses this to work out the gradient. This is obviously going to be much faster than batch GD, especially when using huge training sets. The downside, however, is that due to its random behavior the algorithm is less regular: it only decreases on average as opposed to decreasing gradually at each step. Our final parameter values are therefore going to be good but not optimal (unlike batch gradient descent) as once we end up close to the minimum, the algorithm with continue to bounce around.\\n\\nAs unideal as this sounds, it can actually help! If for example, the cost function is very irregular (and not convex) the stochastic-ness can help in jumping out of local minima. So actually, stochastic GD actually has a better chance at finding the global minimum than plain old batch GD.\\n\\nWe can actually also do something to help deal with the lack of convergence when we get near the minimum: gradually reduce the learning rate. We can start with the steps being (relatively) large, this helps converge quicker to the minimum and also helps jump out of local minima. Gradually we can then reduce the learning rate, which allows the algorithm to settle down at the global minimum. This process is called a **learning schedule**. Learning rate scheduling is a pretty sweet technique, maybe I\u2019ll give it its own blog post in the future.\\n\\nThe following code implements Stochastic Gradient Descent along with a (simple) learning schedule:\\n\\n```py title=\\"/src/stochastic_gradient_descent.py\\"\\nepochs = 50\\nt0, t1 = 1, 10 # Learning schedule hyperparams\\nn = 100\\ndef learning_schedule(t):\\n    return t0/(t + t1)\\ntheta = np.random.randn(d,1) # random initialization, with d being\\n# the dimension of theta\\nfor epoch in range(epochs):\\n    for i in range(n):\\n        rand_idx = np.random.randint(n)\\n        xi = X[rand_idx:rand_idx+d]\\n        yi = y[rand_idx:rand_idx+d]\\n        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\\n        eta = learning_schedule(epoch * n + i)\\n        theta = theta - eta * gradients\\n```\\n\\nHere we iterate over rounds of n iterations, and each round is called an epoch.\\n\\nIt is also worth mentioning another method, **Mini-batch Gradient Descent**. To put it simply, it is just a combination of both Batch and Stochastic GD methods: at each step, Mini-batch GD computes the gradients on a small random subset of the training instances (a mini-batch). This has the advantage of being less erratic than Stochastic GD; however, it is more likely to get stuck in local minimums.\\n\\nSo far we have discussed the more classical GD algorithms which perform to a satisfactory level when training more classic ML models, for example, SVMs, Random Forests, etc. But, using these methods to train large deep natural networks can be mind-numbingly slow. Faster optimizers can provide us with an extra speed boost in training. A few of the most popular algorithms will be discussed below.\\n\\n## Momentum Optimisation\\n\\nInstead of using only the gradient of the loss function at the current step to guide the search, momentum optimization uses the concept of momentum and accumulates the previous gradients to help determine the direction to go. Thinking back to our analogy earlier: imagine you are now on a bike and you were riding down the mountain, you will start slow but you will gradually start to pick up more and more momentum until you reach a terminal velocity. This is exactly what the momentum algorithm does. Another way to think of it is that the gradient vector is now used for acceleration as opposed to speed. An update equation for our model parameters is hence given by:\\n\\n![img3](./images/GradientDescent/math3.png)\\n\\nAt each iteration, we subtract the local gradient (multiplied by **\u03b7**) from the momentum vector m and then update the weights **\u03c9** by adding the momentum vector. As well as converging faster in general, the momentum helps the algorithm escape from plateaus faster and also roll past local minima. The hyperparameter **\u03b2** is the amount of friction in the system, the higher the \u03b2 the slower the terminal velocity.\\n\\n## Nesterov Accelerated Gradient\\n\\nWith one slight adjustment to the momentum algorithm, we can almost always get faster convergence. The NAG method measures the gradient of the cost function slightly ahead of the local position **\u03c9**. The one-step update is given by\\n\\n![img3](./images/GradientDescent/math4.png)\\n\\nThe reason this works is simple: in general, the momentum vector is pointing towards the minimum, so measuring the gradient a little bit further in that direction will help. These small speed improvements then add up and ultimately the NAG algorithm ends up being significantly faster. Implementing this in Keras is incredibly easy, we can just put:\\n\\n```py\\noptimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\\n```\\n\\n## RMSProp\\n\\nThe RMSProp algorithm is an altered version of the **AdaGrad** algorithm, introduced by Geoffrey Hinton and Tijmen Tieleman in 2012 (so pretty recently in mathematics terms!). Imagine a scenario where the cost function looks like an elongated bowl, the GD algorithm will start going down the steepest slope but this slope doesn\'t point directly towards the global minimum. The AdaGrad algorithm provides us with a correction so that the algorithm does indeed actually move towards the global minimum. It does this by scaling down the gradient vector along the steepest dimensions. Another way to think of this is that we decaying the learning rate faster in steeper dimensions and slower in dimensions with a more gentle slope. The easiest way to understand how this works is to get hands-on with the one-step update equations:\\n\\n![img3](./images/GradientDescent/math5.png)\\n\\nHere the **\ud834\uddc7** represents element-wise multiplication. Therefore each element of s is given by\\n\\n![img3](./images/GradientDescent/math6.png)\\n\\nSo if the cost function **J(\u03c9)** is steep in the i-th dimension, then s is going to accumulate and get larger and larger as we iterate through. The second step here is practically the same as the usual gradient descent update except we scale down the gradient vector in relation to **s**. The **\u2298** represent element-wise division, thus the bigger each element of the gradient vector, the smaller the step we take in that direction. The \u03b5 term is a very small smoothing term (e.g. **\u03b5 = 1e-10**) and it is just there to avoid division by zero (I once divided by zero and had a nose bleed, its the devils work).\\n\\nThe problem with AdaGrad however, is that it often stops too early when training neural networks. This is because the learning rate gets scaled down too much. RMSProp makes one simple adjustment. Instead, we exponentially decay the gradients accumulated in the most recent iterations.\\n\\n![img3](./images/GradientDescent/math7.png)\\n\\nAlthough we have introduced a new hyperparameter to tune (**\u03c1**), the value **\u03c1 = 0.9** normally does the trick. As you can probably guess by now, implementing RMSProp in Keras is very easy:\\n\\n```py\\noptimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\\n```\\n\\nI guess now it\'s time to talk about the big daddy of optimizers.\\n\\n## Adam and Nadam Optimization\\n\\nAdaptive moment estimation (Adam) combines two of the methods we have already looked at: momentum and RMSProp optimization. It steals the idea of keeping track of an exponentially decaying average of previous gradients from momentum optimization and the idea of keeping track of exponentially decaying averages of past squared gradients from RMSProp.\\n\\n![img3](./images/GradientDescent/math8.png)\\n\\nHere, **t** is the iteration number (starting at 1). The first, second, and fifth equations are almost identical to the momentum and RMSProp optimization algorithms, with the difference coming from hyperparameters. The third and fourth equations are simply there to boost **m** and **s** at the start of the algorithm as initially, they are biased towards 0. As with all things in Keras, Adam is extremely easy to implement (the hyperparameters default values are given below, also).\\n\\n```py\\noptimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\\n```\\n\\nThe **Nadam** algorithm is the same as the Adam algorithm but it also includes the Nesterov trick, meaning it often converges slightly faster than standard Adam.\\n\\nAnother, not immediately obvious, advantage of Adam (and with Nadam and RMSProp) is that it requires less tuning of **\u03b7** \u2014 so, in some ways, it\'s even easier to implement than the standard vanilla GD method!\\n\\n## Conclusion\\n\\nWe went from classic vanilla gradient descent and gradually got more and more complex with our methodology. When working with the less computing power-intensive models I have found usually the simple SGD/mini-batch models work really well. When training large neural networks the likes of Adam/Nadam and RMSProp tend to work great. As with everything in the ML world, having a little play around experimenting with different methods often leads to the best results.\\n\\nOne closing remark: it has been [found](https://arxiv.org/abs/1705.08292) that sometimes adaptive optimization methodologies (Adam, Nadam, RMSProp) can sometimes lead to solutions that generalize poorly so if you find this problem, maybe give the NAG method a try!\\n\\nIf you found this article interesting, I would highly recommend picking up a copy of Hands-On Machine Learning with Scikit-Learn and TensorFlow by Aur\xe9lien G\xe9ron. It\'s an absolutely incredible book and it\u2019s where I first read about a lot of the content in this post!"},{"id":"Recipe-Recommendation1","metadata":{"permalink":"/blog/Recipe-Recommendation1","source":"@site/blog/2020-10-15-Recipe1.md","title":"Using BeautifulSoup to Help Make Beautiful Soups","description":"The journey to my first \u2018full-stack\u2019 data science project \u2014 a practical guide to web scraping","date":"2020-10-15T00:00:00.000Z","formattedDate":"October 15, 2020","tags":[{"label":"Python","permalink":"/blog/tags/python"},{"label":"Web Scraping","permalink":"/blog/tags/web-scraping"}],"readingTime":9.795,"truncated":true,"authors":[{"name":"Jack Leitch","title":"Machine Learning Engineer","url":"https://github.com/jackmleitch","imageURL":"https://github.com/jackmleitch.png","key":"jack"}],"frontMatter":{"slug":"Recipe-Recommendation1","title":"Using BeautifulSoup to Help Make Beautiful Soups","tags":["Python","Web Scraping"],"authors":"jack"},"prevItem":{"title":"The ins and outs of Gradient Descent","permalink":"/blog/Gradient-Descent"}},"content":"**The journey to my first \u2018full-stack\u2019 data science project \u2014 a practical guide to web scraping**\\n\\nI\u2019ve had my fun playing around with all the \u2018classic\u2019 ML datasets (the likes of MNIST, Titanic, Iris, etc.) and I decided it\'s time to start something of my own. The main reason being that I simply want something different. It all felt somewhat artificial, I want something new and challenging. Something that is not served to me on a silver platter.\\n\\nI spend a good while mulling over possible projects and came up with a few possible avenues, one of which I will go over in this post. To help get more out of the experience, I thought I would blog my journey.\\n\\n\x3c!--truncate--\x3e\\n\\nAs a student, I often fall into the trap of cooking the same recipes over and over again. On one hand, it\u2019s pretty great, I know what I am eating and I know that what I am putting in my meals works for me. But oh boy is it boring (as I\'m writing this I\'m tucking into my 3rd portion of bolognese in four days\u2026).\\n\\nSo\u2026 the idea: Given a list of ingredients, what are different recipes I can make? That is, what recipes can I make with the food I have in my apartment?\\n\\nI hope to build a recommendation system in which the ingredients available by the user is taken as input, and the appropriate dishes or recipes are recommended to the user by Machine Learning (most likely) using the K-Nearest Neighbors algorithm. I hope to then use Flask to deploy the model so that the output can be visualized in a convenient and user-friendly way.\\n\\nThis project idea appeals to me in a few ways:\\n\\n- I can find and [scrape](https://en.wikipedia.org/wiki/Web_scraping) the data myself\\n\\n- Hopefully, by the end of it, I will have a product that I\u2019ll actually use\\n\\n- It\u2019s open-ended, e.g. can I take a picture of the ingredients in my fridge as opposed to inputting ingredients manually?\\n\\nI have had a lot of practice in the machine learning aspect of data science but that isn\'t all there is. I want to practice using a diverse set of tools to solve a wide range of problems, and web scraping is definitely something I need to brush up on. So, this blog is dedicated to just that.\\n\\nFirst things first, I need a data source. I frequently use Jamie Oliver\u2019s [cooking website](https://www.jamieoliver.com/) and I really enjoy a lot of the recipes on there so this was a fairly obvious source of data for me. The other benefits are that is popular to the general public and it well maintained/monitored.\\n\\nThe complete code for scraping the recipe data from Jamie Oliver\u2019s website can be found on my [GitHub](https://github.com/jackmleitch/Whatscooking-). Side note: I prefer to not use Jupyter Notebooks for anything other than [EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis). Maybe I will do another blog post about this in the future but in the meantime, some food for thought on the topic can be found [here](https://towardsdatascience.com/the-case-against-the-jupyter-notebook-d4da17e97243).\\n\\n## Let\'s get some data!\\n\\nSo what libraries do we need? The most common for web scraping include `requests`, Beautiful Soup (`bs4`), and `Selenium`. In this case, we will be using requests and Beautiful Soup. Although Selenium is very adequate, it\u2019s just not needed in this here. We are pulling content from static HTML pages and you would use Selenium when crawling content that is added to the page via JavaScript. Using Selenium here is just going to slow us down.\\n\\nBefore we get going we should do a little bit of exploring.\\n\\n![alt](./images/WebScraping/jmo1.png)\\n![alt](./images/WebScraping/jmo2.png)\\n\\nThe \u2018[main recipes](https://www.jamieoliver.com/recipes/category/course/mains/)\u2019 section of the website contains 858 different recipes, perfect -we now know where to look. We can break this task up into two steps.\\n\\n- Step 1: obtain URLs to each recipe page\\n\\n- Step 2: Within each of these URLs, find recipe attributes: recipe name, ingredients, serving size, cooking time, and difficulty.\\n\\n## Set up\\n\\nWe want to import requests, BeautifulSoup, pandas and time (I will get to time later.\\n\\n```py\\nimport pandas as pd\\nfrom bs4 import BeautifulSoup\\nimport requests\\nimport time\\n```\\n\\nWe then want to specify the URL we want to scrape. It is worth noting that all the recipe links are on the same webpage, so we solely want to scrape URL links from https://www.jamieoliver.com/recipes/category/course/mains/. That is, we don\'t need to add `/page1`, `/page2` etc.\\n\\n```py\\n# Define the url in python\\nurl = \u201chttps://www.jamieoliver.com/recipes/category/course/mains/\\"\\n```\\n\\n## Step 1\\n\\nNow we use requests to retrieve content from the URL and BeautifulSoup to extract the required information from our requested content. Our target website is written in HTML so we need to use the `html.parser`.\\n\\n```py\\n# Fetching html from the website\\npage = requests.get(url)\\n# BeautifulSoup enables to find the elements/tags in a webpage\\nsoup = BeautifulSoup(page.text, \u201chtml.parser\u201d)\\n```\\n\\nSo, what are we working with? Let\'s print the soup variable\u2026\\n\\n```py\\nprint(soup)\\n...\\n<div class=\u201drecipe-block\u201d>\\n<a href=\u201d/recipes/fish-recipes/chargrilled-tuna-with-oregano-oil-and-beautifully-dressed-peas-and-broad-beans/\u201d id=\u201dgtm_recipe_subcat_816\\">\\n<div class=\u201drecipe-image\u201d>\\n<img alt=\u201dChargrilled tuna with oregano oil and beautifully dressed peas and broad beans\u201d data-src=\u201d//img.jamieoliver.com/jamieoliver/recipe-database/oldImages/large/117_1_1436538108.jpg?tr=w-330\\"\\n...\\n```\\n\\nWell, that\'s certainly intimidating. Our data is in there somewhere though (as shown in bold), we just need to extract it. We want to find all of the href\u2019s in our \u2018soup\u2019 so fairly intuitively we can just use BeautifulSoups `find_all` method. How convenient. The only information we want to extract from here are the `a` elements (the `a` tag is used to define hyperlinks, a complete list of HTML tags can be found [here](https://eastmanreference.com/complete-list-of-html-tags)). We then want to make use of the get method to extract the href from inside each `a` tag. We can print the first 5 URLs by running the following code:\\n\\n```py\\nlinks = []\\nfor link in soup.find_all(\'a\'):\\n    links.append(link.get(\'href\'))\\nprint(links[100:105])\\n...\\n[\'/recipes/pasta-recipes/beautiful-courgette-penne-carbonara/\', \'/recipes/vegetable-recipes/roasted-black-bean-burgers/\', \'/recipes/chicken-recipes/chicken-tofu-noodle-soup/\', \'/recipes/chicken-recipes/chicken-katsu-curry/\',\\n\'/recipes/salmon-recipes/roasted-salmon-artichokes/\']\\n```\\n\\nPerfect, that worked great. Well\u2026 kinda, there is a reason I chose to print from 100 to 105. There are a lot of elements in this list that aren\u2019t URLs for recipes, for example `/videos/` or `/recipes/category/course/healthy-snack-ideas/`. However, we can make use of the Pandas `series.str.contains` to see if a pattern or regex is contained within a string of the Series.\\n\\n```py\\n# Filtering the urls to only ones containing recipes\\nimport pandas as pd\\nrecipe_urls = pd.Series([a.get(\\"href\\") for a in soup.find_all(\\"a\\")])\\nrecipe_urls = recipe_urls[(recipe_urls.str.count(\u201c-\u201d)>0) &                    (recipe_urls.str.contains(\u201c/recipes/\u201d)==True) &        (recipe_urls.str.contains(\u201c-recipes/\u201d)==True) & (recipe_urls.str.contains(\u201ccourse\u201d)==False) & (recipe_urls.str.contains(\u201cbooks\u201d)==False) & (recipe_urls.str.endswith(\u201crecipes/\u201d)==False)].unique()\\n```\\n\\nThis piece of code makes convenient use of list comprehensions, then we filter the URLs a little bit e.g. all of the recipes contain at least 1 \u2018-\u2019 so we only take hrefs with `count(\u201c-\u201d)>0`.\\n\\nThe hrefs we have obtained are only the trail end of each URL so we finally just need to add in the start of the full URL to all these items.\\n\\n```py\\ndf[\u2018recipe_urls\u2019] = \u201chttps://www.jamieoliver.com\\" + df[\u2018recipe_urls\u2019].astype(\u2018str\u2019)\\n```\\n\\n## Step 2\\n\\nWe now have 858 URLs to the different recipe pages, so let\'s just pick one and figure what data we want to collect from each page. From there we then generalize and automate the process. I\u2019m a big pasta guy so let\'s look at some [Beautiful courgette carbonara](https://www.jamieoliver.com/recipes/pasta-recipes/beautiful-courgette-penne-carbonara/). I want to extract the recipe title, ingredients, number of people it serves, cooking time, and difficulty. I\u2019m conscious of repeating myself so I will just walk through getting the recipe title and ingredients (the most faffy one). As per usual, the inspecting element tool is our friend. If we inspect the title on the page we find the HTML:\\n\\n`<h1 class=\u201dhidden-xs\u201d>Beautiful courgette carbonara</h1>`\\n\\n`<h1>` tags are used to define HTML headings. Therefore, to get the main webpage title we can just do the following:\\n\\n```py\\nurl = \'https://www.jamieoliver.com/recipes/pasta-recipes/beautiful-courgette-penne-carbonara/\'\\nsoup = BeautifulSoup(requests.get(url).content, \'html.parser\')\\nprint(soup.find(\u2018h1\u2019).text.strip())\\n# Beautiful courgette carbonara\\n```\\n\\nThe find method allows us to just obtain the first `<h1>` tag rather than all of them (`find_all`). I then used `.strip()` to remove all the leading and trailing spaces from the string.\\n\\nRight, so what do we need to make our pasta? By inspecting we find the ingredients are in list tags `<li>`:\\n\\n```html\\n<ul class=\\"\u201dingred-list\\" \u201c>\\n  <li>6 medium green and yellow courgettes</li>\\n  <li>500 g penne</li>\\n  <li>4 large free-range eggs</li>\\n  <li>100 ml single cream</li>\\n  ...\\n</ul>\\n```\\n\\nWe can loop over all the elements in the list and append them to an ingredients list. There is a lot of empty space in each list element so we can split up each ingredient into single words (using `.split()`) and then join the string back up again (using `\' \'.join()`). A simple but elegant solution.\\n\\n```py\\ningredients = []\\nfor li in soup.select(\u2018.ingred-list li\u2019):\\n    ingred = \u2018 \u2018.join(li.text.split())\\n    ingredients.append(ingred)\\nprint(ingredients)\\n# [\'6 medium green and yellow courgettes\', \'500 g penne\', \'4 large free-range eggs\', \'100 ml single cream\', \'1 small handful of Parmesan cheese\', \'olive oil\', \'6 slices of higher-welfare back bacon\', \'\xbd a bunch of fresh thyme , (15g)\', \'a few courgette flowers , (optional)\']\\n```\\n\\nExcellent. I then created a class that I could call on to extract all the information from each URL. I mentioned earlier that there might be URLs that are not recipes so we can use a `try` `except` condition in case the information is not found (e.g. the [videos](https://www.jamieoliver.com/videos/) page does not contain an ingredient list). I just added in Na values here, which will make it very easy to remove these URLs later. Again, the full scripts can be found on my [Github](https://github.com/jackmleitch/Whatscooking-).\\n\\n```py title=\\"/src/web_scrape_jamie_oliver.py\\"\\nimport numpy as np\\nclass JamieOliver():\\n  def __init__(self, url):\\n     self.url = url\\n     self.soup = BeautifulSoup(requests.get(url,    headers=headers).content, \'html.parser\')\\n  def ingredients(self):\\n\u201c\u201d\u201d Creating a list containing the ingredients of the recipe \u201c\u201d\u201d\\n      try:\\n         ingredients = []\\n         for li in self.soup.select(\u2018.ingred-list li\u2019):\\n             ingred = \u2018 \u2018.join(li.text.split())\\n             ingredients.append(ingred)\\n         return ingredients\\n      except:\\n         return np.nan\\n...\\n```\\n\\n## All together now\\n\\nAll that\'s left for us to do now is to loop over each URL in our data frame (`recipe_df`). A word of caution though: we don\'t want to overload the website by making a tonne of requests in a short period of time (we also don\'t want to get blocked\u2026), this is where the `time.sleep` method comes in. It simply suspends a program for a given number of seconds. Using NumPy\'s `random.randint()` method we can pause our program by a random number of seconds, `random.randint(5,10,1)` pauses it anytime between 5 and 10 seconds for example.\\n\\nIf you want you can also identify yourself to the website by using a header in the following way.\\n\\n```py\\nheaders = {\'user-agent\' : \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5); Jack Leitch\'}\\npage = requests.get(url, headers=headers)\\n```\\n\\nThe following code finishes off the scrape and writes the data frame created to a CSV file:\\n\\n```py title=\\"/src/scrape_data.py\\"\\nattribs = [\u2018recipe_name\u2019, \u2018serves\u2019, \u2018cooking_time\u2019, \u2018difficulty\u2019, \u2018ingredients\u2019]\\n# For each url (i) we add the attribute data to the i-th row\\nJamieOliver_df = pd.DataFrame(columns=attribs)\\nfor i in range(0,len(recipe_df[\u2018recipe_urls\u2019])):\\n    url = recipe_df[\u2018recipe_urls\u2019][i]\\n    recipe_scraper = JamieOliver(url)\\n    temp.loc[i] = [getattr(recipe_scraper, attrib)() for attrib in attribs]\\n    if i % 25 == 0:\\n       print(f\u2019Step {i} completed\u2019)\\n    time.sleep(np.random.randint(1,5,1))\\n# Put all the data into the same dataframe\\nJamieOliver_df[\u2018recipe_urls\u2019] = recipe_df[\u2018recipe_urls\u2019]\\n# Re-organise columns\\ncolumns = [\u2018recipe_urls\u2019] + attribs\\nJamieOliver_df = JamieOliver_df[columns]\\n# Save dataframe to a csv file\\nJamieOliver_df.to_csv(r\\"/Users/Jack/Documents/ML/Projects/Whatscooking/input/JamieOliver_full.csv\\", index=False)\\n```\\n\\n## Summary\\n\\nWe started by connecting to the webpage, we then parsed the HTML using BeautifulSoup, then for each recipe webpage we looped through the soup object to extract the relevant information.\\n\\nThis is my first ever blog post so any feedback would be much appreciated and please give this a clap or comment if you enjoyed it/found it useful.\\n\\nI hope to continue with these blogs as I progress further with my first end to end data science project. In the next post, I will be preprocessing my data (using nltk) and doing some data visualization. Thanks for reading!"}]}')}}]);