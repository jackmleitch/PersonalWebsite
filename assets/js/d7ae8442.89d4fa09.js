"use strict";(self.webpackChunkjackmleitch_com_np=self.webpackChunkjackmleitch_com_np||[]).push([[2586],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return u}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),d=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=d(e.components);return a.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=d(n),u=r,h=m["".concat(s,".").concat(u)]||m[u]||p[u]||i;return n?a.createElement(h,o(o({ref:t},c),{},{components:n})):a.createElement(h,o({ref:t},c))}));function u(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var d=2;d<i;d++)o[d]=n[d];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},3377:function(e,t,n){n.r(t),n.d(t,{assets:function(){return c},contentTitle:function(){return s},default:function(){return u},frontMatter:function(){return l},metadata:function(){return d},toc:function(){return p}});var a=n(7462),r=n(3366),i=(n(7294),n(3905)),o=["components"],l={slug:"OrganizingML",title:"Organizing Machine Learning Projects",tags:["Python"],authors:"jack"},s=void 0,d={permalink:"/blog/OrganizingML",source:"@site/blog/2020-11-17-OrganizingML.md",title:"Organizing Machine Learning Projects",description:"A guide to organizing your machine learning projects",date:"2020-11-17T00:00:00.000Z",formattedDate:"November 17, 2020",tags:[{label:"Python",permalink:"/blog/tags/python"}],readingTime:8.69,truncated:!0,authors:[{name:"Jack Leitch",title:"Machine Learning Engineer",url:"https://github.com/jackmleitch",imageURL:"https://github.com/jackmleitch.png",key:"jack"}],frontMatter:{slug:"OrganizingML",title:"Organizing Machine Learning Projects",tags:["Python"],authors:"jack"},prevItem:{title:"Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku",permalink:"/blog/Recipe-Recommendation2"},nextItem:{title:"The ins and outs of Gradient Descent",permalink:"/blog/Gradient-Descent"}},c={authorsImageUrls:[void 0]},p=[{value:"Why is it important to organize a project?",id:"why-is-it-important-to-organize-a-project",level:2},{value:"File structure",id:"file-structure",level:2},{value:"A (basic) project example",id:"a-basic-project-example",level:2},{value:"A word on Github",id:"a-word-on-github",level:2}],m={toc:p};function u(e){var t=e.components,n=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"A guide to organizing your machine learning projects")),(0,i.kt)("p",null,"I just want to start with a brief disclaimer. This is how I personally organize my projects and it\u2019s what works for me, that doesn't necessarily mean it will work for you. However, there is definitely something to be said about how good organization streamlines your workflow. Building my ML framework the way I do allows me to work in a very plug n\u2019 play way: I can train, change, adjust my models without making too many changes to my code."),(0,i.kt)("h2",{id:"why-is-it-important-to-organize-a-project"},"Why is it important to organize a project?"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Productivity. If you have a well-organized project, with everything in the same directory, you don't waste time searching for files, datasets, codes, models, etc.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Reproducibility. You\u2019ll find that a lot of your data science projects have at least some repetitively to them. For example, with the proper organization, you could easily go back and find/use the same script to split your data into folds.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Understandability. A well-organized project can easily be understood by other data scientists when shared on Github."))),(0,i.kt)("h2",{id:"file-structure"},"File structure"),(0,i.kt)("p",null,"The first thing I do whenever I start a new project is to make a folder and name it something appropriate, for example, \u201cMNIST\u201d or \u201cdigit_recognition\u201d. Inside the main project folder, I always create the same subfolders: ",(0,i.kt)("strong",{parentName:"p"},"notes"),", ",(0,i.kt)("strong",{parentName:"p"},"input"),", ",(0,i.kt)("strong",{parentName:"p"},"src"),", ",(0,i.kt)("strong",{parentName:"p"},"models"),", ",(0,i.kt)("strong",{parentName:"p"},"notebooks"),". Don\u2019t forget to add a ",(0,i.kt)("inlineCode",{parentName:"p"},"README.md")," file as well! As intuitive as this breakdown seems, it makes one hell of a difference in productivity."),(0,i.kt)("p",null,"What do I put in each folder?"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"notes"),": I add any notes to this folder, this can be anything! URLs to useful webpages, chapters/pages of books, descriptions of variables, a rough outline of the task at hand, etc.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"input"),": This folder contains all of the input files and data for the machine learning project. For example, it may contain CSV files, text embedding, or images (in another subfolder though), to list a few things.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"src"),": Any ",(0,i.kt)("inlineCode",{parentName:"p"},".py")," file is kept in this folder. Simple.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"models"),": We keep all of our trained models in here (the useful ones\u2026).")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"notebooks"),": We can store our Jupyter notebooks here (any ",(0,i.kt)("inlineCode",{parentName:"p"},".ipynb")," file). Note that I tend to only use notebooks for data exploration and plotting."))),(0,i.kt)("h2",{id:"a-basic-project-example"},"A (basic) project example"),(0,i.kt)("p",null,"When I build my projects I like to automate as much as possible. That is, I try to repeat myself as little as possible and I like to change things like models and hyperparameters with as little code as I can. Let's look to build a very simple model to classify the MNIST dataset. For those of you that have been living under a rock, this dataset is the de facto \u201chello world\u201d of computer vision. The data files ",(0,i.kt)("inlineCode",{parentName:"p"},"train.csv")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"test.csv")," contain gray-scale images of hand-drawn digits, from zero through nine. Given the pixel intensity values (each column represents a pixel) of an image, we aim to identify which digit the image is. This is a supervised problem. Please note that the models I am creating are by no means the best for classifying this dataset, that isn't the point of this blog post."),(0,i.kt)("p",null,"So, how do we start? Well, as with most things data science, we first need to decide on a metric. By looking at a count plot (",(0,i.kt)("inlineCode",{parentName:"p"},"sns.countplot()")," (this was done in a Jupyter notebook in the notebooks folder!)) we can see that the distribution of labels is fairly uniform, so plain and simple accuracy should do the trick!"),(0,i.kt)("p",null,"The next thing we need to do is create some cross-validation folds. The first script I added to my src folder was to do exactly this. ",(0,i.kt)("inlineCode",{parentName:"p"},"create_folds.py")," reads ",(0,i.kt)("inlineCode",{parentName:"p"},"mnist_train.csv")," from the input folder and creates a new file ",(0,i.kt)("inlineCode",{parentName:"p"},"mnist_train_folds.csv")," (saved to the input folder) which has a new column, kfolds, containing fold numbers. As with most classification problems, I used stratified k-folds. To see the script, please check out my ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/jackmleitch/Almost-any-ML-problem/tree/master/MNIST"},"Github page"),"."),(0,i.kt)("p",null,"Now that we have decided on a metric and created folds, we can start making some basic models. With the aim of this post in mind, I decided to completely skip any kind of preprocessing (sorry!). What we are going to do is create a general python script to train our model(s), ",(0,i.kt)("inlineCode",{parentName:"p"},"train.py"),", and then we will make some changes so that we hardcode as little as possible. The aim here is to be able to change a lot of things without changing much (if any) code. So, let's get cracking!"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/train.py"',title:'"/src/train.py"'},'import joblib\nimport pandas as pd\nfrom sklearn import metrics, tree\ndef run(fold):\n    # we first read in data with our folds\n    df = pd.read_csv("../input/mnist_train_folds.csv")\n\n    # we then split it into our training and validation data\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    # drop target column (label) and turn to np array\n    x_train = df_train.drop(\'label\', axis=1).values\n    y_train = df_train.label.values\n    # same for validation\n    x_valid = df_valid.drop(\'label\', axis=1).values\n    y_valid = df_valid.label.values\n    # initialize simple decision tree classifier and fit data\n    clf = tree.DecisionTreeClassifier()\n    clf.fit(x_train, y_train)\n    # create predictions for validation data\n    preds = clf.predict(x_valid)\n    # calculate and print accuracy\n    score = metrics.accuracy_score(y_valid, preds)\n    print(f"Fold={fold}, Accuracy={score}")\n    # save the model (not very necessary for a smaller model though)\n    joblib.dump(clf, f"../models/dt_{fold}.bin")\nif __name__ == "__main__":\n    for i in range(3):\n        run(fold=i)\n')),(0,i.kt)("p",null,"We can then run this script by calling ",(0,i.kt)("inlineCode",{parentName:"p"},"python src/train.py")," in the terminal. This script will read our data, train a decision tree classifier, score our predictions, and save the model for each fold. Note that you will need to set the working directory. For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},'> cd "/Users/Jack/Documents/MNIST/src"\n> python train.py\nFold=0, Accuracy=0.858\nFold=1, Accuracy=0.860\nFold=2, Accuracy=0.859\n')),(0,i.kt)("p",null,"We can do a lot better than this... We have hardcoded the fold numbers, the training file, the output folder, the model, and the hyperparameters. We can change all of this."),(0,i.kt)("p",null,"The first easy thing we can do is create a ",(0,i.kt)("inlineCode",{parentName:"p"},"config.py")," file with all of the training files and output folder."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/config.py"',title:'"/src/config.py"'},'TRAINING_FILE = "../input/mnist_train_folds.csv"\nOUTPUT_PATH = "../models/"\n')),(0,i.kt)("p",null,"Changing the script to incorporate this is easy! The changes are in bold."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/train.py"',title:'"/src/train.py"'},'import os\nimport config\nimport joblib\nimport pandas as pd\nfrom sklearn import metrics, tree\ndef run(fold):\n    # we first read in data with our folds\n    df = pd.read_csv(config.TRAINING_FILE)\n    .\n    .\n    .\n    # save the model\n    joblib.dump(\n        clf,\n        os.path.join(config.OUTPUT_PATH, f"../models/dt_{fold}.bin")\n    )\nif __name__ == "__main__":\n    for i in range(3):\n        run(fold=i)\n')),(0,i.kt)("p",null,"In our script, we call the ",(0,i.kt)("strong",{parentName:"p"},"run")," function for each fold. When training bigger models this can be an issue as running multiple folds in the same script will keep increasing the memory consumption. This could potentially lead to the program crashing. We can get around this by using ",(0,i.kt)("strong",{parentName:"p"},"argparse"),". Argparse allows us to specify arguments in the command line which get parsed through to the script. Let's see how to implement this."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/train.py"',title:'"/src/train.py"'},'import argparse\n.\n.\n.\nif __name__ == "__main__":\n    # initialize Argument Parser\n    parser = argparse.ArgumentParser()\n\n    # we can add any different arguments we want to parse\n    parser.add_argument("--fold", type=int)\n    # we then read the arguments from the comand line\n    args = parser.parse_args()\n    # run the fold that we specified in the command line\n    run(fold=args.fold)\n')),(0,i.kt)("p",null,"So what have we done here? We have allowed us to specify the fold in the terminal."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},"> python train.py --fold 3\nFold=3, Accuracy=0.861\n")),(0,i.kt)("p",null,"We can use argparse in an even more useful way though, we can change the model with this! We can create a new python script, ",(0,i.kt)("inlineCode",{parentName:"p"},"model_dispatcher.py"),", that has a dictionary containing different models. In this dictionary, the keys are the names of the models and the values are the models themselves."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/model_dispatcher.py"',title:'"/src/model_dispatcher.py"'},'from sklearn import tree, ensemble, linear_model, svm\nmodels = {\n     "decision_tree_gini": tree.DecisionTreeClassifier(\n         criterion="gini"\n     ),\n     "decision_tree_entropy": tree.DecisionTreeClassifier(\n         criterion=\'entropy\'\n     ),\n     "rf": ensemble.RandomForestClassifier(),\n     "log_reg": linear_model.LogisticRegression(),\n     "svc": svm.SVC(C=10, gamma=0.001, kernel="rbf")\n}\n')),(0,i.kt)("p",null,"We can then add the following things to our code."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/train.py"',title:'"/src/train.py"'},"import os, argparse, joblib\nimport pandas as pd\nfrom sklearn import metrics\nimport config\nimport model_dispatcher\ndef run(fold, model):\n    # we first read in data with our folds\n    df = pd.read_csv(config.TRAINING_FILE)\n    # we then split it into our training and validation data\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    # drop target column (label) and turn to np array\n    x_train = df_train.drop('label', axis=1).values\n    y_train = df_train.label.values\n    x_valid = df_valid.drop('label', axis=1).values\n    y_valid = df_valid.label.values\n    # fetch model from model dispatcher\n    clf = model_dispatcher.models[model]\n    # fit model on training data\n    clf.fit(x_train, y_train)\n    # create predictions for validation data\n    preds = clf.predict(x_valid)\n    # calculate and print accuracy\n    score = metrics.accuracy_score(y_valid, preds)\n    print(f\"Model={model}, Fold={fold}, Accuracy={score}\")\n    # save the model\n    joblib.dump(\n        clf,\n        os.path.join(config.OUTPUT_PATH, f\"../models/dt_{fold}.bin\")\n    )\nif __name__ == \"__main__\":\n  # initialize ArgumentParser class of argparse\n  parser = argparse.ArgumentParser()\n\n  # add different arguments and their types\n  parser.add_argument('--fold', type=int)\n  parser.add_argument('--model', type=str)\n\n  # read arguments from command line\n  args = parser.parse_args()\n  # run with the fold and model specified by command line arguments\n  run(\n      fold=args.fold,\n      model=args.model\n  )\n")),(0,i.kt)("p",null,"And that's it, now we can choose the fold and model in the terminal! What's great about this is that to try a new model/tweak hyperparameters, all we need to do is change our model dispatcher."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},"> python train.py --fold 2 --model rf\n  Model=rf, Fold=2, Accuracy=0.9658333333333333\n")),(0,i.kt)("p",null,"We also aren\u2019t limited to just doing this. We can make dispatchers for loads of other things too: categorical encoders, feature selection, hyperparameter optimization, the list goes on!"),(0,i.kt)("p",null,"To go even further, we can create a shell script to try a few different models from our dispatcher at once."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh",metastring:'title="/src/run.sh"',title:'"/src/run.sh"'},"#!/bin/sh\npython train.py --fold 0 --model rf\npython train.py --fold 0 --model decision_tree_gini\npython train.py --fold 0 --model log_reg\npython train.py --fold 0 --model svc\n")),(0,i.kt)("p",null,"Which, when run, gives:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},"> sh run.sh\nModel=rf, Fold=0, Accuracy=0.9661666666666666\nModel=decision_tree_gini, Fold=0, Accuracy=0.8658333333333333\nModel=log_reg, Fold=0, Accuracy=0.917\nModel=svc, Fold=0, Accuracy=0.9671812654676666\n")),(0,i.kt)("p",null,"I first learned how to do all of this in Abhishek Thakur\u2019s (quadruple Kaggle Grand Master) book: Approaching (Almost) Any Machine Learning Problem. It\u2019s a fantastic and pragmatic exploration of data science problems. I would highly recommend it."),(0,i.kt)("h2",{id:"a-word-on-github"},"A word on Github"),(0,i.kt)("p",null,"Once you\u2019ve finished up (or during!) a problem that you find interesting make sure to create a Github repository and upload your datasets, python scripts, models, Jupyter notebooks, R scripts, etc. Creating Github repositories to showcase your work is extremely important! It also means you have some version control and you can access your code at all times."))}u.isMDXComponent=!0}}]);