"use strict";(self.webpackChunkjackmleitch_com_np=self.webpackChunkjackmleitch_com_np||[]).push([[7549],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return u}});var i=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,o=function(e,t){if(null==e)return{};var n,i,o={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=i.createContext({}),c=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},p=function(e){var t=c(e.components);return i.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},m=i.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=c(n),u=o,h=m["".concat(l,".").concat(u)]||m[u]||d[u]||r;return n?i.createElement(h,a(a({ref:t},p),{},{components:n})):i.createElement(h,a({ref:t},p))}));function u(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,a=new Array(r);a[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,a[1]=s;for(var c=2;c<r;c++)a[c]=n[c];return i.createElement.apply(null,a)}return i.createElement.apply(null,n)}m.displayName="MDXCreateElement"},7046:function(e,t,n){n.r(t),n.d(t,{assets:function(){return p},contentTitle:function(){return l},default:function(){return u},frontMatter:function(){return s},metadata:function(){return c},toc:function(){return d}});var i=n(7462),o=n(3366),r=(n(7294),n(3905)),a=["components"],s={slug:"Recipe-Recommendation2",title:"Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku",tags:["Python","NLP","Recommendation System","API"],authors:"jack"},l=void 0,c={permalink:"/blog/Recipe-Recommendation2",source:"@site/blog/2020-12-08-Recipe2.md",title:"Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku",description:"The journey to my first \u2018full-stack\u2019 data science project (part 2) \u2014 building and deployment",date:"2020-12-08T00:00:00.000Z",formattedDate:"December 8, 2020",tags:[{label:"Python",permalink:"/blog/tags/python"},{label:"NLP",permalink:"/blog/tags/nlp"},{label:"Recommendation System",permalink:"/blog/tags/recommendation-system"},{label:"API",permalink:"/blog/tags/api"}],readingTime:14.03,truncated:!0,authors:[{name:"Jack Leitch",title:"Machine Learning Engineer",url:"https://github.com/jackmleitch",imageURL:"https://github.com/jackmleitch.png",key:"jack"}],frontMatter:{slug:"Recipe-Recommendation2",title:"Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku",tags:["Python","NLP","Recommendation System","API"],authors:"jack"},prevItem:{title:"Automating the Setup of my Data Science Projects",permalink:"/blog/Project-Auto"},nextItem:{title:"Organizing Machine Learning Projects",permalink:"/blog/OrganizingML"}},p={authorsImageUrls:[void 0]},d=[{value:"Preprocessing and Parsing of Ingredients",id:"preprocessing-and-parsing-of-ingredients",level:2},{value:"Extracting Features",id:"extracting-features",level:2},{value:"Recommendation System",id:"recommendation-system",level:2},{value:"Create an API to Deploy the Model",id:"create-an-api-to-deploy-the-model",level:2},{value:"Deployment using Flask",id:"deployment-using-flask",level:3},{value:"Deploying Flask API to Heroku \u2014 It\u2019s Time To Go Online",id:"deploying-flask-api-to-heroku--its-time-to-go-online",level:3},{value:"Docker",id:"docker",level:3}],m={toc:d};function u(e){var t=e.components,s=(0,o.Z)(e,a);return(0,r.kt)("wrapper",(0,i.Z)({},m,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"The journey to my first \u2018full-stack\u2019 data science project (part 2) \u2014 building and deployment")),(0,r.kt)("p",null,"So\u2026 the idea: Given a list of ingredients, what are different recipes I can make? That is, what recipes can I make with the food I have in my apartment?"),(0,r.kt)("p",null,"First things first, if you would like to see my API in action (or use it!) then please do so by following (I have updated the app and release a new blog on it soon):"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://share.streamlit.io/jackmleitch/whatscooking-deployment/streamlit.py"},"https://share.streamlit.io/jackmleitch/whatscooking-deployment/streamlit.py")),(0,r.kt)("p",null,"In my first blog post on this project, I walked through how I scraped the data for this project. The data being cooking recipes and the corresponding ingredients. Since then I have added more recipes so we now have a total of 4647. Please feel free to make use of this dataset yourself, you can find it on my ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/jackmleitch/Whatscooking-"},"Github"),"."),(0,r.kt)("p",null,"This post will be focused on preprocessing the data, building the recommendation system, and then finally deploying the model using Flask and Heroku."),(0,r.kt)("p",null,"The process to build the recommendation system is as follows:\n",(0,r.kt)("img",{alt:"flowchart",src:n(3444).Z,width:"1347",height:"925"})),(0,r.kt)("p",null,"We start by cleaning and parsing the dataset, then we extract the numerical features from the data, from here we can apply a similarity function to find the similarity between ",(0,r.kt)("strong",{parentName:"p"},"ingredients for known recipes")," and the ",(0,r.kt)("strong",{parentName:"p"},"ingredients given by the end-user"),". Finally, we can get the top-recommended recipes according to the similarity score."),(0,r.kt)("p",null,"Unlike the first article in this series, this will not be a tutorial of the tools I used, but it will describe how I built the system and why I made the decisions I did along the way. Although, the code comments do a good job of explaining things in my opinion \u2014 so check them out if you\u2019re curious. As with most projects, my aim was to create the simplest model I could to get the job done to the standard I desired."),(0,r.kt)("h2",{id:"preprocessing-and-parsing-of-ingredients"},"Preprocessing and Parsing of Ingredients"),(0,r.kt)("p",null,"To understand the task at hand here, let's look at an example. The delicious \u2018Gennaro\u2019s classic spaghetti carbonara\u2019 recipe found on Jamie Oliver\u2019s ",(0,r.kt)("a",{parentName:"p",href:"https://www.jamieoliver.com/recipes/pasta-recipes/gennaro-s-classic-spaghetti-carbonara/"},"website")," requires the ingredients:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"3 large free-range egg yolks")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"40 g Parmesan cheese, plus extra to serve")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"1 x 150 g piece of higher-welfare pancetta")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"200 g dried spaghetti")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"1 clove of garlic")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"extra virgin olive oil"))),(0,r.kt)("p",null,"There is a lot of redundant information here; for example, weights and measures are not going to add value to the vector encodings of the recipe. If anything, it is going to make distinguishing between recipes more difficult. So we need to get rid of those. A quick google search led me to a ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikibooks.org/wiki/Cookbook:Units_of_measurement"},"Wikipedia page")," containing a list of standard cooking metrics e.g. clove, gram (g), teaspoon, etc. Removing all these words in my ingredient parser worked really well."),(0,r.kt)("p",null,"We also want to remove stop words from our ingredients. In NLP \u2018stop words\u2019 refer to the most common words in a language. For example, the sentence \u2018learning about what stop words are\u2019, becomes, \u2018learning stop words\u2019. NLTK provides us with an easy way to remove (most of) these words."),(0,r.kt)("p",null,"There are also other words in the ingredients that are useless to us \u2014 these are the words that are very common among recipes. Oil, for example, is used in most recipes and will provide little to no distinguishing power between recipes. Also, most people have oil in their homes so having to write oil every time you use the API is cumbersome and pointless. There are advanced NLP techniques, for example using ",(0,r.kt)("a",{parentName:"p",href:"https://open.blogs.nytimes.com/2015/04/09/extracting-structured-data-from-recipes-using-conditional-random-fields/"},"conditional random fields")," (a class of statistical modeling), that can calculate the probability that a word is an ingredient, as opposed to a measure, texture, or another type of word that surrounds the ingredient word. But, simply removing the most common words seemed to be very effective, so I did this. Occam\u2019s razor and all that jazz\u2026 To get the most common words we can do the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"import nltk\nvocabulary = nltk.FreqDist()\n# This was done once I had already preprocessed the ingredients\nfor ingredients in recipe_df['ingredients']:\n    ingredients = ingredients.split()\n    vocabulary.update(ingredients)\nfor word, frequency in vocabulary.most_common(200):\n    print(f'{word};{frequency}')\n")),(0,r.kt)("p",null,"We have one final obstacle to get over, however. When we try to remove these \u2018junk\u2019 words from our ingredient list, what happens when we have different variations of the same word? What happens if we want to remove every occurrence of the word \u2018pound\u2019 but the recipe ingredients say \u2018pounds\u2019? Luckily there is a pretty trivial workaround: lemmatization and stemming. Stemming and lemmatization both generate the root form of inflected words \u2014 the difference is that a stem might not be an actual word whereas, a lemma is an actual language word. Although lemmatization is often slower, I chose to use this technique as I know the ingredients will be actual words which is useful for debugging and visualization (the results turned out to be practically identical using stemming instead). When the user feeds ingredients to the API we also lemmatize those words as the lemmatized words are the words with the embeddings."),(0,r.kt)("p",null,"We can put this all together in a function, ",(0,r.kt)("inlineCode",{parentName:"p"},"ingredient_parser"),", along with some other standard preprocessing: getting rid of punctuation, removing accents, making everything lowercase, getting rid of Unicode."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/parse_ingredients.py"',title:'"/src/parse_ingredients.py"'},"def ingredient_parser(ingredients):\n    # measures and common words (already lemmatized)\n    measures = ['teaspoon', 't', 'tsp.', 'tablespoon', 'T', ...]\n    words_to_remove = ['fresh', 'oil', 'a', 'red', 'bunch', ...]\n    # Turn ingredient list from string into a list\n    if isinstance(ingredients, list):\n       ingredients = ingredients\n    else:\n       ingredients = ast.literal_eval(ingredients)\n    # We first get rid of all the punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    # initialize nltk's lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    ingred_list = []\n    for i in ingredients:\n        i.translate(translator)\n        # We split up with hyphens as well as spaces\n        items = re.split(' |-', i)\n        # Get rid of words containing non alphabet letters\n        items = [word for word in items if word.isalpha()]\n        # Turn everything to lowercase\n        items = [word.lower() for word in items]\n        # remove accents\n        items = [unidecode.unidecode(word) for word in items]\n        # Lemmatize words so we can compare words to measuring words\n        items = [lemmatizer.lemmatize(word) for word in items]\n        # get rid of stop words\n        stop_words = set(corpus.stopwords.words('english'))\n        items = [word for word in items if word not in stop_words]\n        # Gets rid of measuring words/phrases, e.g. heaped teaspoon\n        items = [word for word in items if word not in measures]\n        # Get rid of common easy words\n        items = [word for word in items if word not in words_to_remove]\n        if items:\n           ingred_list.append(' '.join(items))\n           ingred_list = ' '.join(ingred_list)\n    return ingred_list\n")),(0,r.kt)("p",null,"When parsing the ingredients for \u2018Gennaro\u2019s classic spaghetti carbonara\u2019 we get: egg yolk parmesan cheese pancetta spaghetti garlic. Perfect, that works fantastically!"),(0,r.kt)("p",null,"Using lambda functions, it\u2019s easy to parse all of the ingredients."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"recipe_df = pd.read_csv(config.RECIPES_PATH)\nrecipe_df['ingredients_parsed'] = recipe_df['ingredients'].apply(lambda x: ingredient_parser(x))\ndf = recipe_df.dropna()\ndf.to_csv(config.PARSED_PATH, index=False)\n")),(0,r.kt)("h2",{id:"extracting-features"},"Extracting Features"),(0,r.kt)("p",null,"We now need to encode each document (recipe ingredients), and as before, simple models worked really well. One of the most basic models that should always be tried when doing NLP is ",(0,r.kt)("strong",{parentName:"p"},"bag of words"),". This entails creating a huge sparse matrix that stores counts of all the words in our corpus (all of the documents i.e. all of the ingredients for every recipe). Scikit-learn\u2019s CountVectorizer has a nice implementation for this."),(0,r.kt)("p",null,"Bag of words performed ok but ",(0,r.kt)("strong",{parentName:"p"},"TF-IDF")," (term frequencies-inverse document frequency) marginally out-performed it, so we opted for this instead. I\u2019m not going to go into the details of how tf-idf works as it is not relevant to the blog but it basically does what it says on the tin. As per usual, Scikit-learn has a lovely implementation of this: TfidfVectorizer. I then saved the model and encodings using pickle, as retraining the model every time the API is used would make it painfully slow."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/feature_extraction.py"',title:'"/src/feature_extraction.py"'},"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pickle\nimport config\n# load in parsed recipe dataset\ndf_recipes = pd.read_csv(config.PARSED_PATH)\n# Tfidf needs unicode or string types\ndf_recipes['ingredients_parsed'] =       df_recipes.ingredients_parsed.values.astype('U')\n# TF-IDF feature extractor\ntfidf = TfidfVectorizer()\ntfidf.fit(df_recipes['ingredients_parsed'])\ntfidf_recipe = tfidf.transform(df_recipes['ingredients_parsed'])\n# save the tfidf model and encodings\nwith open(config.TFIDF_MODEL_PATH, \"wb\") as f:\n     pickle.dump(tfidf, f)\nwith open(config.TFIDF_ENCODING_PATH, \"wb\") as f:\n     pickle.dump(tfidf_recipe, f)\n")),(0,r.kt)("h2",{id:"recommendation-system"},"Recommendation System"),(0,r.kt)("p",null,"This application simply consists of text data and there is no kind of ratings available, ",(0,r.kt)("strong",{parentName:"p"},"so we can not use")," matrix decomposition methods, such as SVD and correlation coefficient-based methods."),(0,r.kt)("p",null,"We use content-based filtering which enables us to recommend recipes to people based on the attributes (ingredients) the user provides. To measure the similarity between documents I used ",(0,r.kt)("strong",{parentName:"p"},"Cosine similarity"),". I also tried using Spacy and KNN but cosine similarity won in terms of performance (and ease)."),(0,r.kt)("p",null,"Mathematically, cosine similarity measures the cosine of the angle between two vectors. For the mathematically inclined out there, this is the same as the inner product of the same vectors normalized to both have length 1. I chose to use this measure of similarity as even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. For example, if the user inputs a lot of ingredients and only the first half match with a recipe, we should, in theory, still get a good recipe match. In cosine similarity, the smaller the angle, the higher the cosine similarity: so we are trying to maximize this score."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pickle\nimport config\nfrom ingredient_parser import ingredient_parser\n# load in tdidf model and encodings\nwith open(config.TFIDF_ENCODING_PATH, 'rb') as f:\n     tfidf_encodings = pickle.load(f)\nwith open(config.TFIDF_MODEL_PATH, \"rb\") as f:\n     tfidf = pickle.load(f)\n# parse the ingredients using my ingredient_parser\ntry:\n    ingredients_parsed = ingredient_parser(ingredients)\nexcept:\n    ingredients_parsed = ingredient_parser([ingredients])\n# use our pretrained tfidf model to encode our input ingredients\ningredients_tfidf = tfidf.transform([ingredients_parsed])\n# calculate cosine similarity between actual recipe ingreds and test ingreds\ncos_sim = map(lambda x: cosine_similarity(ingredients_tfidf, x), tfidf_encodings)\nscores = list(cos_sim)\n")),(0,r.kt)("p",null,"I then wrote a function, ",(0,r.kt)("inlineCode",{parentName:"p"},"get_recommendations"),", to rank these scores and output a pandas data frame containing all the details of the top ",(0,r.kt)("strong",{parentName:"p"},"N")," recipes."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/recommendation_system.py"',title:'"/src/recommendation_system.py"'},"def get_recommendations(N, scores):\n    # load in recipe dataset\n    df_recipes = pd.read_csv(config.PARSED_PATH)\n    # order the scores with and filter to get the highest N scores\n    top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:N]\n    # create dataframe to load in recommendations\n    recommendation = pd.DataFrame(columns = ['recipe', 'ingredients', 'score', 'url'])\n    count = 0\n    for i in top:\n        recommendation.at[count, 'recipe'] = title_parser(df_recipes['recipe_name'][i])\n\n        recommendation.at[count, 'ingredients'] = ingredient_parser_final(df_recipes['ingredients'][i])\n\n        recommendation.at[count, 'url'] = df_recipes['recipe_urls'][i]\n        recommendation.at[count, 'score'] = \"{:.3f}\".format(float(scores[i]))\n\n        count += 1\n    return recommendation\n")),(0,r.kt)("p",null,"It\u2019s worth noting that there was no concrete way of assessing the performance of the model so I had to evaluate the recommendations manually. To be honest though, this ended up actually being quite fun\u2026 I also discovered a tonne of new recipes! It\u2019s comforting to see that when we put in the ingredients (before they are parsed of course!) for our beloved Gennaro\u2019s classic spaghetti carbonara we get the correct recommendation with a score of 1."),(0,r.kt)("p",null,"As of right now, a few items in my fridge/cupboards are: ground beef, pasta, spaghetti, tomato pasta sauce, bacon, onion, zucchini, and, cheese. Our newly built recommendation system suggests we make:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'{ "ingredients" :\n    "1 (15 ounce) can tomato sauce, 1 (8 ounce) package uncooked pasta shells, 1 large zucchini - peeled and cubed, 1 teaspoon dried basil, 1 teaspoon dried oregano, 1/2 cup white sugar, 1/2 medium onion, finely chopped, 1/4 cup grated Romano cheese, 1/4 cup olive oil, 1/8 teaspoon crushed red pepper flakes, 2 cups water, 3 cloves garlic, minced",\n\n  "recipe" : "Zucchini and Shells",\n\n  "score: "0.760",\n\n  "url":"https://www.allrecipes.com/recipe/88377/zucchini-and-shells/"}\n')),(0,r.kt)("p",null,"Sounds good to me \u2014 best get cooking!"),(0,r.kt)("h2",{id:"create-an-api-to-deploy-the-model"},"Create an API to Deploy the Model"),(0,r.kt)("h3",{id:"deployment-using-flask"},"Deployment using Flask"),(0,r.kt)("p",null,"So how can I serve this model I have built to an end-user? I created an API that can be used to input ingredients and in turn, it outputs the top 5 recipe recommendations based on those ingredients. To build this API I used Flask, which is a micro web service framework."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/app.py"',title:'"/src/app.py"'},"from flask import Flask, jsonify, request\nimport json, requests, pickle\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom ingredient_parser import ingredient_parser\nimport config, rec_sys\napp = Flask(__name__)\n@app.route('/', methods=[\"GET\"])\ndef hello():\n    # This is the homepage of our API.\n    # It can be accessed by http://127.0.0.1:5000/\n    return HELLO_HTML\nHELLO_HTML = \"\"\"\n     <html><body>\n         <h1>Welcome to my api: Whatscooking!</h1>\n         <p>Please add some ingredients to the url to receive recipe recommendations.\n            You can do this by appending \"/recipe?ingredients= Pasta Tomato ...\" to the current url.\n         <br>Click <a href=\"/recipe?ingredients= pasta tomato onion\">here</a> for an example when using the ingredients: pasta, tomato and onion.\n     </body></html>\n     \"\"\"\n@app.route('/recipe', methods=[\"GET\"])\ndef recommend_recipe():\n    # This is our endpoint. It can be accessed by http://127.0.0.1:5000/recipe\n    ingredients = request.args.get('ingredients')\n    recipe = rec_sys.RecSys(ingredients)\n\n    # We need to turn output into JSON.\n    response = {}\n    count = 0\n    for index, row in recipe.iterrows():\n        response[count] = {\n                            'recipe': str(row['recipe']),\n                            'score': str(row['score']),\n                            'ingredients': str(row['ingredients']),\n                            'url': str(row['url'])\n                          }\n        count += 1\n    return jsonify(response)\nif __name__ == \"__main__\":\n   app.run(host=\"0.0.0.0\", debug=True)\n")),(0,r.kt)("p",null,"We can start this API by running the command ",(0,r.kt)("inlineCode",{parentName:"p"},"python src/app.py")," and the API will start on localhost on port 5000. We can access recipe recommendations for the inputs pasta, tomato, and onion by visiting ",(0,r.kt)("a",{parentName:"p",href:"http://192.168.1.51:5000/recipe?ingredients=%20pasta%20tomato%20onion"},"http://192.168.1.51:5000/recipe?ingredients=%20pasta%20tomato%20onion")," in our browser."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"api_call",src:n(6947).Z,width:"1400",height:"306"}),"\n",(0,r.kt)("img",{alt:"api_response",src:n(6924).Z,width:"1400",height:"659"})),(0,r.kt)("h3",{id:"deploying-flask-api-to-heroku--its-time-to-go-online"},"Deploying Flask API to Heroku \u2014 It\u2019s Time To Go Online"),(0,r.kt)("p",null,"Deploying a Flask API to Heroku is extremely easy if you use Github! First, I created a file in my project folder called Procfile, with no extension. All you need to put inside this file is:"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"web: gunicorn app:app")),(0,r.kt)("p",null,"The next step was to create a file called ",(0,r.kt)("inlineCode",{parentName:"p"},"requirements.txt")," which consists of all of the python libraries that I used for this project. If you\u2019re working in a virtual environment (I use conda) then it\u2019s as easy as ",(0,r.kt)("inlineCode",{parentName:"p"},"pip freeze > requirements.txt")," \u2014 make sure you're in the correct working directory otherwise it\u2019ll save the file somewhere else."),(0,r.kt)("p",null,"All I had to do now was commit the changes to ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/jackmleitch/whatscooking-deployment"},"my Github repository")," and follow the deployment steps on ",(0,r.kt)("a",{parentName:"p",href:"https://dashboard.heroku.com/apps"},"https://dashboard.heroku.com/apps"),". If you would like to try out or use my API, please do by visiting:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://whats-cooking-recommendation.herokuapp.com/-"},"https://whats-cooking-recommendation.herokuapp.com/-")," if you are in USA")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://whatscooking-deployment.herokuapp.com/-"},"https://whatscooking-deployment.herokuapp.com/-")," if you are in Europe")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Either should work if you are elsewhere, it\u2019ll just be a tad slower"))),(0,r.kt)("h3",{id:"docker"},"Docker"),(0,r.kt)("p",null,"We have now reached the stage where I am happy with the model I have built, so I want to be able to distribute my model to others so that they can use it too. I have uploaded my whole project to ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/jackmleitch/Whatscooking-"},"Github")," but that's not quite enough. Just because the code works on my computer does not mean it's going to work on someone else's computer, this could be because of many reasons. It would be fantastic if when I distribute my code, I replicate my computer so that I know it is going to work. One of the most popular ways of doing this nowadays is by using ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/jackmleitch/Whatscooking-"},"Docker Containers"),"."),(0,r.kt)("p",null,"The first thing I did was create a ",(0,r.kt)("strong",{parentName:"p"},"docker file")," called Dockerfile (it doesn't have an extension). Simply put, the docker file tells us how to build our environment and contains all the commands a user could call in the command line to assemble the image."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="Dockerfile"',title:'"Dockerfile"'},'# Include where we get the image from (operating system)\nFROM ubuntu:18.04\nMAINTAINER Jack Leitch \'jackmleitch@gmail.com\'\n# We cannot press Y so we do it automatically.\nRUN apt-get update && apt-get install -y \\\n    git \\\n    curl \\\n    ca-certificates \\\n    python3 \\\n    python3-pip \\\n    sudo \\\n    && rm -rf /var/lib/apt/lists/*\n# Set working directory\nWORKDIR /app\n# Copy everything in currect directory into the app directory.\nADD . /app\n# Install all of the requirements\nRUN pip3 install -r requirements.txt\n# Download wordnet as its used to lemmatize\nRUN python3 -c "import nltk; nltk.download(\'wordnet\')"\n# CMD executes once the container is started\nCMD ["python3", "app.py"]\n')),(0,r.kt)("p",null,"Once I created the docker file, I then needed to build my container \u2014 and that's very easy. Side note: if you do this, make sure all your file paths (I keep mine in a ",(0,r.kt)("inlineCode",{parentName:"p"},"config.py")," file) aren\u2019t specific to your computer because docker is like a virtual machine and contains its own file system e.g. you can put ",(0,r.kt)("inlineCode",{parentName:"p"},"./input/df_recipes.csv")," instead."),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"docker build -f Dockerfile -t whatscooking:api")),(0,r.kt)("p",null,"To start our API, on any machine (!), all we have to do now is (assuming you have downloaded the docker container):"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"docker run -p 5000:5000 -d whatscooking:api")),(0,r.kt)("p",null,"If you want to check out the container yourself, here is a link to my ",(0,r.kt)("a",{parentName:"p",href:"https://hub.docker.com/repository/docker/jackmleitch/whatscooking"},"Docker Hub"),". You can pull the image by:"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"docker pull jackmleitch/whatscooking:api")),(0,r.kt)("p",null,"The plan moving forward from here is to build a nicer interface to the API using Streamlit."))}u.isMDXComponent=!0},6947:function(e,t,n){t.Z=n.p+"assets/images/api_call-dc8bd3528db12682f8b17ad04df9c60e.png"},6924:function(e,t,n){t.Z=n.p+"assets/images/api_response-30122221120391d52a884f4f2bcb61a4.png"},3444:function(e,t,n){t.Z=n.p+"assets/images/flowchart-55db802e3605933d0b683dc6d533f488.png"}}]);