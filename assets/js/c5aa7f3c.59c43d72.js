"use strict";(self.webpackChunkjackmleitch_com_np=self.webpackChunkjackmleitch_com_np||[]).push([[4272],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return g}});var i=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,r=function(e,t){if(null==e)return{};var n,i,r={},o=Object.keys(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var d=i.createContext({}),l=function(e){var t=i.useContext(d),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},c=function(e){var t=l(e.components);return i.createElement(d.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},p=i.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,d=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=l(n),g=r,u=p["".concat(d,".").concat(g)]||p[g]||m[g]||o;return n?i.createElement(u,a(a({ref:t},c),{},{components:n})):i.createElement(u,a({ref:t},c))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,a=new Array(o);a[0]=p;var s={};for(var d in t)hasOwnProperty.call(t,d)&&(s[d]=t[d]);s.originalType=e,s.mdxType="string"==typeof e?e:r,a[1]=s;for(var l=2;l<o;l++)a[l]=n[l];return i.createElement.apply(null,a)}return i.createElement.apply(null,n)}p.displayName="MDXCreateElement"},3620:function(e,t,n){n.r(t),n.d(t,{assets:function(){return c},contentTitle:function(){return d},default:function(){return g},frontMatter:function(){return s},metadata:function(){return l},toc:function(){return m}});var i=n(7462),r=n(3366),o=(n(7294),n(3905)),a=["components"],s={slug:"Recipe-Recommendation",title:"Building a Recipe Recommendation System",tags:["Python","NLP","Recommendation System"],authors:"jack"},d=void 0,l={permalink:"/blog/Recipe-Recommendation",source:"@site/blog/2021-07-28-RecipeRecomm.md",title:"Building a Recipe Recommendation System",description:"Using Word2Vec, Scikit-Learn, and Streamlit",date:"2021-07-28T00:00:00.000Z",formattedDate:"July 28, 2021",tags:[{label:"Python",permalink:"/blog/tags/python"},{label:"NLP",permalink:"/blog/tags/nlp"},{label:"Recommendation System",permalink:"/blog/tags/recommendation-system"}],readingTime:12.565,truncated:!0,authors:[{name:"Jack Leitch",title:"Machine Learning Engineer",url:"https://github.com/jackmleitch",imageURL:"https://github.com/jackmleitch.png",key:"jack"}],frontMatter:{slug:"Recipe-Recommendation",title:"Building a Recipe Recommendation System",tags:["Python","NLP","Recommendation System"],authors:"jack"},prevItem:{title:"Predicting Strava Kudos",permalink:"/blog/Strava-Kudos"},nextItem:{title:"Automating Mundane Web-Based Tasks With Selenium and Heroku",permalink:"/blog/Strava-AP"}},c={authorsImageUrls:[void 0]},m=[{value:"Preprocessing and Parsing of Ingredients",id:"preprocessing-and-parsing-of-ingredients",level:2},{value:"Word Embeddings Using Word2Vec",id:"word-embeddings-using-word2vec",level:2},{value:"Document Embeddings",id:"document-embeddings",level:2},{value:"Averaging Word Embeddings",id:"averaging-word-embeddings",level:3},{value:"Using TF-IDF to Aggregate Embeddings",id:"using-tf-idf-to-aggregate-embeddings",level:3},{value:"Recommendation System",id:"recommendation-system",level:2},{value:"Creating an App Using Streamlit",id:"creating-an-app-using-streamlit",level:2}],p={toc:m};function g(e){var t=e.components,s=(0,r.Z)(e,a);return(0,o.kt)("wrapper",(0,i.Z)({},p,s,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Using Word2Vec, Scikit-Learn, and Streamlit")),(0,o.kt)("p",null,"First things first, If you would like to play around with the finished app. You can here: ",(0,o.kt)("a",{parentName:"p",href:"https://share.streamlit.io/jackmleitch/whatscooking-deployment/streamlit.py"},"https://share.streamlit.io/jackmleitch/whatscooking-deployment/streamlit.py"),"."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"alt",src:n(9473).Z,width:"1134",height:"801"})),(0,o.kt)("p",null,"In a previous blog post (Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku) I wrote about how I went about building a recipe recommendation system. To summarize: I first cleaned and parsed the ingredients for each recipe (for example, 1 diced onion becomes onion), next I ",(0,o.kt)("strong",{parentName:"p"},"encoded each recipe ingredient list using TF-IDF"),". From here I applied a similarity function to find the similarity between ",(0,o.kt)("strong",{parentName:"p"},"ingredients for known recipes and the ingredients given by the end-user"),". Finally, we can get the top-recommended recipes according to the similarity score."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"alt",src:n(3444).Z,width:"1347",height:"925"})),(0,o.kt)("p",null,"However, after using my new API for a while I wanted to improve on two things:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Sometimes the ingredients in the recommendations didn't line up well with the input ingredients, so I needed some better embeddings.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"The Flask API was well in need for a good makeover."))),(0,o.kt)("p",null,"Enter Gensim\u2019s ",(0,o.kt)("strong",{parentName:"p"},"Word2Vec")," and ",(0,o.kt)("strong",{parentName:"p"},"Streamlit"),"."),(0,o.kt)("h2",{id:"preprocessing-and-parsing-of-ingredients"},"Preprocessing and Parsing of Ingredients"),(0,o.kt)("p",null,"To understand the task at hand here, let\u2019s look at an example. The delicious \u2018Gennaro\u2019s classic spaghetti carbonara\u2019 recipe found on Jamie Oliver\u2019s ",(0,o.kt)("a",{parentName:"p",href:"https://www.jamieoliver.com/recipes/pasta-recipes/gennaro-s-classic-spaghetti-carbonara/"},"website")," requires the ingredients:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"3 large free-range egg yolks")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"40 g Parmesan cheese, plus extra to serve")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"1 x 150 g piece of higher-welfare pancetta")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"200 g dried spaghetti")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"1 clove of garlic")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"extra virgin olive oil"))),(0,o.kt)("p",null,"There is a lot of redundant information here; for example, weights and measures are not going to add value to the vector encodings of the recipe. If anything, it is going to make distinguishing between recipes more difficult. So we need to get rid of those. A quick google search led me to a ",(0,o.kt)("a",{parentName:"p",href:"https://en.wikibooks.org/wiki/Cookbook:Units_of_measurement"},"Wikipedia page")," containing a list of standard cooking metrics e.g. clove, gram (g), teaspoon, etc. Removing all these words in my ingredient parser worked really well."),(0,o.kt)("p",null,"We also want to remove stop words from our ingredients. In NLP \u2018stop words\u2019 refer to the most common words in a language. For example, the sentence \u2018learning about what stop words are\u2019, becomes, \u2018learning stop words\u2019. NLTK provides us with an easy way to remove (most of) these words."),(0,o.kt)("p",null,"There are also other words in the ingredients that are useless to us \u2014 these are the words that are very common among recipes. Oil, for example, is used in most recipes and will provide little to no distinguishing power between recipes. Also, most people have oil in their homes so having to write oil every time you use the API is cumbersome and pointless. There are advanced NLP techniques, for example using ",(0,o.kt)("a",{parentName:"p",href:"https://open.blogs.nytimes.com/2015/04/09/extracting-structured-data-from-recipes-using-conditional-random-fields/"},"conditional random fields")," (a class of statistical modeling), that can calculate the probability that a word is an ingredient, as opposed to a measure, texture, or another type of word that surrounds the ingredient word. But, simply removing the most common words seemed to be very effective, so I did this. Occam\u2019s razor and all that jazz\u2026 To get the most common words we can do the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/get_word_freq.py"',title:'"/src/get_word_freq.py"'},"import nltk\nvocabulary = nltk.FreqDist()\n# This was done once I had already preprocessed the ingredients\nfor ingredients in recipe_df['ingredients']:\n    ingredients = ingredients.split()\n    vocabulary.update(ingredients)\nfor word, frequency in vocabulary.most_common(200):\n    print(f'{word};{frequency}')\n")),(0,o.kt)("p",null,"We have one final obstacle to get over, however. When we try to remove these \u2018junk\u2019 words from our ingredient list, what happens when we have different variations of the same word? What happens if we want to remove every occurrence of the word \u2018pound\u2019 but the recipe ingredients say \u2018pounds\u2019? Luckily there is a pretty trivial workaround: ",(0,o.kt)("strong",{parentName:"p"},"lemmatization")," and ",(0,o.kt)("strong",{parentName:"p"},"stemming"),". Stemming and lemmatization both generate the root form of inflected words \u2014 the difference is that a stem might not be an actual word whereas, a lemma is an actual language word. Although lemmatization is often slower, I chose to use this technique as I know the ingredients will be actual words which is useful for debugging and visualization (the results turned out to be practically identical using stemming instead). When the user feeds ingredients to the API we also lemmatize those words as the lemmatized words are the words with the corresponding vectorizations."),(0,o.kt)("p",null,"We can put this all together in a function, ",(0,o.kt)("inlineCode",{parentName:"p"},"ingredient_parser"),", along with some other standard preprocessing: getting rid of punctuation, removing accents, making everything lowercase, getting rid of Unicode."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/parse_ingredients.py"',title:'"/src/parse_ingredients.py"'},"def ingredient_parser(ingredients):\n    # measures and common words (already lemmatized)\n    measures = ['teaspoon', 't', 'tsp.', 'tablespoon', 'T', ...]\n    words_to_remove = ['fresh', 'oil', 'a', 'red', 'bunch', ...]\n    # Turn ingredient list from string into a list\n    if isinstance(ingredients, list):\n       ingredients = ingredients\n    else:\n       ingredients = ast.literal_eval(ingredients)\n    # We first get rid of all the punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    # initialize nltk's lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    ingred_list = []\n    for i in ingredients:\n        i.translate(translator)\n        # We split up with hyphens as well as spaces\n        items = re.split(' |-', i)\n        # Get rid of words containing non alphabet letters\n        items = [word for word in items if word.isalpha()]\n        # Turn everything to lowercase\n        items = [word.lower() for word in items]\n        # remove accents\n        items = [unidecode.unidecode(word) for word in items]\n        # Lemmatize words so we can compare words to measuring words\n        items = [lemmatizer.lemmatize(word) for word in items]\n        # get rid of stop words\n        stop_words = set(corpus.stopwords.words('english'))\n        items = [word for word in items if word not in stop_words]\n        # Gets rid of measuring words/phrases, e.g. heaped teaspoon\n        items = [word for word in items if word not in measures]\n        # Get rid of common easy words\n        items = [word for word in items if word not in words_to_remove]\n        if items:\n           ingred_list.append(' '.join(items))\n    return ingred_list\n")),(0,o.kt)("p",null,"When parsing the ingredients for \u2018Gennaro\u2019s classic spaghetti carbonara\u2019 we get: egg yolk, parmesan cheese, pancetta, spaghetti, garlic. Perfect, that works fantastically! Using ",(0,o.kt)("inlineCode",{parentName:"p"},"df.apply")," when can easily parse the ingredients for each recipe."),(0,o.kt)("h2",{id:"word-embeddings-using-word2vec"},"Word Embeddings Using Word2Vec"),(0,o.kt)("p",null,"Word2Vec is a 2-layer neural network that produces word embeddings. It takes in a corpus of text as an input (a collection of individual documents) and maps each word to a vector in Euclidean space. The end goal is to have a text representation that captures ",(0,o.kt)("strong",{parentName:"p"},"distributional similarities")," between words. That is, words that often appear in similar contexts are mapped to vectors separated by a shorter Euclidean distance (the L2 norm)."),(0,o.kt)("p",null,"In the context of recipe ingredients, Word2vec allowed me to capture similarities between recipe ingredients that are commonly used together. For example, mozzarella cheese which is commonly used when making pizza is most similar to other cheeses and pizza-related ingredients."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"print(model_cbow.wv.most_similar(u'mozzarella cheese'))\n>>> [('parmesan cheese', 0.999547004699707), ('cheddar cheese', 0.9995082020759583),\n    ('tomato', 0.9994533061981201), ('mushroom', 0.9994530081748962),\n    ('cheese', 0.9994290471076965), ('mozzarella', 0.999423086643219),\n    ('sausage', 0.9993994832038879), ('pepperoni', 0.9993913173675537),\n    ('onion', 0.9993910193443298), ('chicken breast', 0.9993739724159241)]\n")),(0,o.kt)("p",null,"I chose to use the ",(0,o.kt)("strong",{parentName:"p"},"Continuous bag of words")," (CBOW) variant of Word2Vec. CBOW tries to learn a language model that tries to predict the center word in a sliding window. Due to the fact that Word2Vec tries to predict words based on the word's surroundings, it was vital to sort the ingredients alphabetically. If I did not sort the ingredients with respect to a consistent criterion, the model would interpret ingredients in a different order as having different contexts."),(0,o.kt)("p",null,"The hyperparameters chosen are below:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"size=100"),", this refers to the dimensionality of the embeddings. I settled on 100 by trying 50, 100, 150 and seeing which performed best.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"sg = 0"),", this creates a CBOW model as opposed to a SkipGram model.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"workers = 8"),", the number of CPUs I have.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"window = 6"),", this is the size of the sliding window in which the center word is predicted. 6 was chosen as it is the average length of each document (ingredient list).")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"min_count = 1"),", words below the min_count frequency are dropped before training occurs. 1 was chosen as I want all ingredients to have an embedding."))),(0,o.kt)("p",null,"The Word2Vec training code is below. I used the popular python library Gensim."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/train_word2vec.py"',title:'"/src/train_word2vec.py"'},"from gensim.models import Word2Vec\n\n# get corpus with the documents sorted in alphabetical order\ndef get_and_sort_corpus(data):\n    corpus_sorted = []\n    for doc in data.parsed.values:\n        doc.sort()\n        corpus_sorted.append(doc)\n    return corpus_sorted\n\n# calculate average length of each document\ndef get_window(corpus):\n    lengths = [len(doc) for doc in corpus]\n    avg_len = float(sum(lengths)) / len(lengths)\n    return round(avg_len)\n\nif __name__ == \"__main__:\n    # load in data\n    data = pd.read_csv('input/df_recipe.csv')\n    # parse the ingredients for each recipe\n    data['parsed'] = data.ingredients.apply(ingredient_parser)\n    # get corpus\n    corpus = get_and_sort_corpus(data)\n    print(f\"Length of corpus: {len(corpus)}\")\n    # train and save CBOW Word2Vec model\n    model_cbow = Word2Vec(\n      corpus, sg=0, workers=8, window=get_window(corpus), min_count=1, vector_size=100\n    )\n    model_cbow.save('models/model_cbow.bin')\n    print(\"Word2Vec model successfully trained\")\n")),(0,o.kt)("h2",{id:"document-embeddings"},"Document Embeddings"),(0,o.kt)("p",null,"In order to build the recipe recommendation, I needed to represent each document (each ingredient list) as a single embedding. This then allowed me to calculate corresponding similarities. So how can we use our word embeddings to compute a representative vector for the whole ingredient list? I tried out two different methods: one simple and one slightly more complicated. For this, I followed this excellent ",(0,o.kt)("a",{parentName:"p",href:"http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"},"tutorial"),"."),(0,o.kt)("h3",{id:"averaging-word-embeddings"},"Averaging Word Embeddings"),(0,o.kt)("p",null,"This does what it says on the tin, it directly averages all word embeddings in each document. The code is adapted from ",(0,o.kt)("a",{parentName:"p",href:"http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"},"here")," and ",(0,o.kt)("a",{parentName:"p",href:"https://towardsdatascience.com/nlp-performance-of-different-word-embeddings-on-text-classification-de648c6262b"},"here"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/mean_word_embeddings.py"',title:'"/src/mean_word_embeddings.py"'},"class MeanEmbeddingVectorizer(object):\n    def __init__(self, model_cbow):\n        self.model_cbow = model_cbow\n        self.vector_size = model_cbow.wv.vector_size\n\n    def fit(self):\n        return self\n\n    def transform(self, docs):\n        doc_vector = self.doc_average_list(docs)\n        return doc_word_vector\n\n    def doc_average(self, doc):\n        mean = []\n        for word in doc:\n            if word in self.model_cbow.wv.index_to_key:\n                mean.append(self.model_cbow.wv.get_vector(word))\n\n        if not mean:\n            return np.zeros(self.vector_size)\n        else:\n            mean = np.array(mean).mean(axis=0)\n            return mean\n\n    def doc_average_list(self, docs):\n        return np.vstack([self.doc_average(doc) for doc in docs])\n")),(0,o.kt)("h3",{id:"using-tf-idf-to-aggregate-embeddings"},"Using TF-IDF to Aggregate Embeddings"),(0,o.kt)("p",null,"Here we do the same as above but instead, we do a weighted mean and scale each vector using its inverse document frequency (IDF). The IDF measures the importance of a term across the whole corpus. IDF will weigh down terms that are very common across a corpus (in our case words like olive oil, garlic, butter, etc.) and weighs up rare terms. This is useful for us as it will give us better distinguishing power between recipes as the ingredients that are scaled-down will be ingredients that the user will tend to not give as an input to the recommendation system."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/tfidf_word_embeddings.py"',title:'"/src/tfidf_word_embeddings.py"'},'class TfidfEmbeddingVectorizer(object):\n    def __init__(self, model_cbow):\n\n        self.model_cbow = model_cbow\n        self.word_idf_weight = None\n        self.vector_size = model_cbow.wv.vector_size\n\n    def fit(self, docs):\n        """\n    Build a tfidf model to compute each word\'s idf as its weight.\n    """\n\n        text_docs = []\n        for doc in docs:\n            text_docs.append(" ".join(doc))\n\n        tfidf = TfidfVectorizer()\n        tfidf.fit(text_docs)\n        # if a word was never seen it is given idf of the max of known idf value\n        max_idf = max(tfidf.idf_)\n        self.word_idf_weight = defaultdict(\n            lambda: max_idf,\n            [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()],\n        )\n        return self\n\n    def transform(self, docs):\n        doc_word_vector = self.doc_average_list(docs)\n        return doc_word_vector\n\n    def doc_average(self, doc):\n    """\n    Compute weighted mean of documents word embeddings\n    """\n\n        mean = []\n        for word in doc:\n            if word in self.model_cbow.wv.index_to_key:\n                mean.append(\n                    self.model_cbow.wv.get_vector(word) * self.word_idf_weight[word]\n                )\n\n        if not mean:\n            return np.zeros(self.vector_size)\n        else:\n            mean = np.array(mean).mean(axis=0)\n            return mean\n\n    def doc_average_list(self, docs):\n        return np.vstack([self.doc_average(doc) for doc in docs])\n')),(0,o.kt)("h2",{id:"recommendation-system"},"Recommendation System"),(0,o.kt)("p",null,"This application simply consists of text data and there is no kind of ratings available, ",(0,o.kt)("strong",{parentName:"p"},"so we can not use matrix decomposition")," methods, such as SVD and correlation coefficient-based methods.\nWe use content-based filtering which enables us to recommend recipes to people based on the attributes (ingredients) the user provides. To measure the similarity between documents I used Cosine similarity. I also tried using Spacy and KNN but cosine similarity won in terms of performance (and ease)."),(0,o.kt)("p",null,"Mathematically, ",(0,o.kt)("strong",{parentName:"p"},"cosine similarity")," measures the cosine of the angle between two vectors. For the mathematically inclined out there, this is the same as the inner product of the same vectors normalized to both have length 1. With cosine similarity, the smaller the angle, the higher the cosine similarity: so we are trying to maximize this score."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="/src/recommendation_system.py"',title:'"/src/recommendation_system.py"'},'from gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import defaultdict\n\nimport config\nfrom ingredient_parser import ingredient_parser\n\ndef get_recommendations(N, scores):\n    """\n    Rank scores and output a pandas data frame containing all the details of the top N recipes.\n    :param scores: list of cosine similarities\n    """\n    # load in recipe dataset\n    df_recipes = pd.read_csv(config.PARSED_PATH)\n    # order the scores with and filter to get the highest N scores\n    top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:N]\n    # create dataframe to load in recommendations\n    recommendation = pd.DataFrame(columns=["recipe", "ingredients", "score", "url"])\n    count = 0\n    for i in top:\n        recommendation.at[count, "recipe"] = title_parser(df_recipes["recipe_name"][i])\n        recommendation.at[count, "ingredients"] = ingredient_parser_final(\n            df_recipes["ingredients"][i]\n        )\n        recommendation.at[count, "url"] = df_recipes["recipe_urls"][i]\n        recommendation.at[count, "score"] = f"{scores[i]}"\n        count += 1\n    return recommendation\n\ndef get_recs(ingredients, N=5, mean=False):\n    """\n    Get the top N recipe recomendations.\n    :param ingredients: comma seperated string listing ingredients\n    :param N: number of recommendations\n    :param mean: False if using tfidf weighted embeddings, True if using simple mean\n    """\n    # load in word2vec model\n    model = Word2Vec.load("models/model_cbow.bin")\n    # normalize embeddings\n    model.init_sims(replace=True)\n    if model:\n        print("Successfully loaded model")\n    # load in data\n    data = pd.read_csv("input/df_recipes.csv")\n    # parse ingredients\n    data["parsed"] = data.ingredients.apply(ingredient_parser)\n    # create corpus\n    corpus = get_and_sort_corpus(data)\n\n    if mean:\n        # get average embdeddings for each document\n        mean_vec_tr = MeanEmbeddingVectorizer(model)\n        doc_vec = mean_vec_tr.transform(corpus)\n        doc_vec = [doc.reshape(1, -1) for doc in doc_vec]\n        assert len(doc_vec) == len(corpus)\n    else:\n        # use TF-IDF as weights for each word embedding\n        tfidf_vec_tr = TfidfEmbeddingVectorizer(model)\n        tfidf_vec_tr.fit(corpus)\n        doc_vec = tfidf_vec_tr.transform(corpus)\n        doc_vec = [doc.reshape(1, -1) for doc in doc_vec]\n        assert len(doc_vec) == len(corpus)\n\n    # create embeddings for input text\n    input = ingredients\n    # create tokens with elements\n    input = input.split(",")\n    # parse ingredient list\n    input = ingredient_parser(input)\n    # get embeddings for ingredient doc\n    if mean:\n        input_embedding = mean_vec_tr.transform([input])[0].reshape(1, -1)\n    else:\n        input_embedding = tfidf_vec_tr.transform([input])[0].reshape(1, -1)\n\n    # get cosine similarity between input embedding and all the document embeddings\n    cos_sim = map(lambda x: cosine_similarity(input_embedding, x)[0][0], doc_vec)\n    scores = list(cos_sim)\n    # Filter top N recommendations\n    recommendations = get_recommendations(N, scores)\n    return recommendations\n\nif __name__ == "__main__":\n    # test\n    input = "chicken thigh, onion, rice noodle, seaweed nori sheet, sesame, shallot, soy, spinach, star, tofu"\n    rec = get_recs(input)\n    print(rec)\n')),(0,o.kt)("p",null,"It\u2019s worth noting that there was no concrete way of assessing the performance of the model so I had to evaluate the recommendations manually. To be honest, this ended up actually being quite fun\u2026 I also discovered a tonne of new recipes! It\u2019s comforting to see that when we put in the ingredients (before they are parsed of course!) for our beloved Gennaro\u2019s classic spaghetti carbonara we get the correct recommendation with a score of 1."),(0,o.kt)("p",null,"As of right now, a few items in my fridge/cupboards are: tomato sauce, pasta, zucchini, onion, cheese Our newly built recommendation system suggests we make:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"alt",src:n(7056).Z,width:"759",height:"487"})),(0,o.kt)("p",null,"Sounds good to me \u2014 best get cooking! As we can see, all the ingredients in this recipe that I did not enter are all very common household items."),(0,o.kt)("h2",{id:"creating-an-app-using-streamlit"},"Creating an App Using Streamlit"),(0,o.kt)("p",null,"Finally, I created and deployed an app using Streamlit. It was fantastically easy to make and the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.streamlit.io/en/stable/api.html"},"documentation")," was very easy to follow. Code for this app can be found on my ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/jackmleitch/whatscooking-deployment"},"Github"),"."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"alt",src:n(9473).Z,width:"1134",height:"801"})),(0,o.kt)("p",null,"Thanks for reading and I hope you enjoyed it!"))}g.isMDXComponent=!0},9473:function(e,t,n){t.Z=n.p+"assets/images/app-b4b868e72b5712f9ec9ec9e0ee050eb6.png"},3444:function(e,t,n){t.Z=n.p+"assets/images/flowchart-55db802e3605933d0b683dc6d533f488.png"},7056:function(e,t,n){t.Z=n.p+"assets/images/test-b918c541a96847fd2391dab985ad0953.png"}}]);